{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670a73bb",
   "metadata": {},
   "source": [
    "ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib koreanize-matplotlib konlpy wheel JPype1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f30bf21",
   "metadata": {},
   "source": [
    "konlpyë¥¼ ìœ„í•œ Java ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import sys\n",
    "\n",
    "def update_gitignore():\n",
    "    \"\"\"gitignoreì— Java ê´€ë ¨ íŒŒì¼ë“¤ ì¶”ê°€ (ê¶Œí•œ ë¬¸ì œì‹œ ê±´ë„ˆë›°ê¸°)\"\"\"\n",
    "    gitignore_entries = [\n",
    "        \"# Java Runtime for KoNLPy\",\n",
    "        \"java/\",\n",
    "        \"openjdk-*.zip\", \n",
    "        \"*.jdk\",\n",
    "        \"jdk*/\",\n",
    "        \"# Java ì‹¤í–‰ íŒŒì¼\",\n",
    "        \"java.exe\",\n",
    "        \"javac.exe\",\n",
    "        \"\",\n",
    "        \"# Python\",\n",
    "        \"__pycache__/\",\n",
    "        \"*.pyc\",\n",
    "        \"*.pyo\", \n",
    "        \"*.pyd\",\n",
    "        \".Python\",\n",
    "        \"*.so\",\n",
    "        \"\",\n",
    "        \"# Jupyter Notebook\",\n",
    "        \".ipynb_checkpoints\",\n",
    "        \"\",\n",
    "        \"# IDE\",\n",
    "        \".vscode/\",\n",
    "        \".idea/\",\n",
    "        \"*.swp\",\n",
    "        \"*.swo\",\n",
    "        \"*~\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # ê¸°ì¡´ .gitignore ë‚´ìš© ì½ê¸°\n",
    "        existing_content = \"\"\n",
    "        gitignore_exists = os.path.exists(\".gitignore\")\n",
    "        \n",
    "        if gitignore_exists:\n",
    "            try:\n",
    "                with open(\".gitignore\", \"r\", encoding=\"utf-8\") as f:\n",
    "                    existing_content = f.read()\n",
    "            except PermissionError:\n",
    "                print(\"âš ï¸ .gitignore ì½ê¸° ê¶Œí•œ ì—†ìŒ - ê±´ë„ˆëœ€\")\n",
    "                print(\"ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ .gitignoreì— 'java/' ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”\")\n",
    "                return False\n",
    "            except:\n",
    "                try:\n",
    "                    with open(\".gitignore\", \"r\", encoding=\"cp949\") as f:\n",
    "                        existing_content = f.read()\n",
    "                except PermissionError:\n",
    "                    print(\"âš ï¸ .gitignore ì½ê¸° ê¶Œí•œ ì—†ìŒ - ê±´ë„ˆëœ€\")\n",
    "                    print(\"ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ .gitignoreì— 'java/' ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”\")\n",
    "                    return False\n",
    "                except:\n",
    "                    existing_content = \"\"\n",
    "        else:\n",
    "            print(\"ğŸ“ .gitignore íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ì´ë¯¸ Java ê´€ë ¨ í•­ëª©ì´ ìˆëŠ”ì§€ í™•ì¸\n",
    "        if \"java/\" in existing_content:\n",
    "            print(\"âœ… .gitignoreì— ì´ë¯¸ Java ê´€ë ¨ í•­ëª©ì´ ìˆìŠµë‹ˆë‹¤\")\n",
    "            return True\n",
    "        \n",
    "        # .gitignore ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸\n",
    "        try:\n",
    "            mode = \"a\" if gitignore_exists else \"w\"\n",
    "            with open(\".gitignore\", mode, encoding=\"utf-8\") as f:\n",
    "                if gitignore_exists and existing_content and not existing_content.endswith('\\n'):\n",
    "                    f.write('\\n')\n",
    "                for entry in gitignore_entries:\n",
    "                    f.write(entry + \"\\n\")\n",
    "            \n",
    "            if gitignore_exists:\n",
    "                print(\"âœ… .gitignoreì— Java ê´€ë ¨ í•­ëª© ì¶”ê°€ ì™„ë£Œ\")\n",
    "            else:\n",
    "                print(\"âœ… .gitignore íŒŒì¼ ìƒì„± ë° Java ê´€ë ¨ í•­ëª© ì¶”ê°€ ì™„ë£Œ\")\n",
    "            return True\n",
    "            \n",
    "        except PermissionError:\n",
    "            print(\"âš ï¸ .gitignore ì“°ê¸° ê¶Œí•œ ì—†ìŒ - ê±´ë„ˆëœ€\")\n",
    "            print(\"ğŸ’¡ Java ì„¤ì¹˜ëŠ” ê³„ì† ì§„í–‰í•˜ë˜, ìˆ˜ë™ìœ¼ë¡œ .gitignoreì— ë‹¤ìŒì„ ì¶”ê°€í•´ì£¼ì„¸ìš”:\")\n",
    "            print(\"   java/\")\n",
    "            print(\"   openjdk-*.zip\")\n",
    "            print(\"   *.jdk\")\n",
    "            print(\"   .ipynb_checkpoints\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            # ë‹¤ë¥¸ ì¸ì½”ë”©ìœ¼ë¡œ ì‹œë„\n",
    "            try:\n",
    "                mode = \"a\" if gitignore_exists else \"w\"\n",
    "                with open(\".gitignore\", mode, encoding=\"cp949\") as f:\n",
    "                    if gitignore_exists and existing_content and not existing_content.endswith('\\n'):\n",
    "                        f.write('\\n')\n",
    "                    for entry in gitignore_entries:\n",
    "                        f.write(entry + \"\\n\")\n",
    "                \n",
    "                if gitignore_exists:\n",
    "                    print(\"âœ… .gitignoreì— Java ê´€ë ¨ í•­ëª© ì¶”ê°€ ì™„ë£Œ\")\n",
    "                else:\n",
    "                    print(\"âœ… .gitignore íŒŒì¼ ìƒì„± ë° Java ê´€ë ¨ í•­ëª© ì¶”ê°€ ì™„ë£Œ\")\n",
    "                return True\n",
    "                \n",
    "            except PermissionError:\n",
    "                print(\"âš ï¸ .gitignore ì“°ê¸° ê¶Œí•œ ì—†ìŒ - ê±´ë„ˆëœ€\")\n",
    "                print(\"ğŸ’¡ Java ì„¤ì¹˜ëŠ” ê³„ì† ì§„í–‰í•˜ë˜, ìˆ˜ë™ìœ¼ë¡œ .gitignoreì— ë‹¤ìŒì„ ì¶”ê°€í•´ì£¼ì„¸ìš”:\")\n",
    "                print(\"   java/\")\n",
    "                print(\"   openjdk-*.zip\")\n",
    "                print(\"   *.jdk\")\n",
    "                print(\"   .ipynb_checkpoints\")\n",
    "                return False\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"âš ï¸ .gitignore ì²˜ë¦¬ ì‹¤íŒ¨: {e2}\")\n",
    "                print(\"ğŸ’¡ Java ì„¤ì¹˜ëŠ” ê³„ì† ì§„í–‰í•˜ë˜, ìˆ˜ë™ìœ¼ë¡œ .gitignoreì— ë‹¤ìŒì„ ì¶”ê°€í•´ì£¼ì„¸ìš”:\")\n",
    "                print(\"   java/\")\n",
    "                print(\"   openjdk-*.zip\")\n",
    "                print(\"   *.jdk\")\n",
    "                print(\"   .ipynb_checkpoints\")\n",
    "                return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ .gitignore ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {e}\")\n",
    "        print(\"ğŸ’¡ Java ì„¤ì¹˜ëŠ” ê³„ì† ì§„í–‰í•˜ë˜, ìˆ˜ë™ìœ¼ë¡œ .gitignore íŒŒì¼ì„ ìƒì„±í•˜ê³  ë‹¤ìŒ ë‚´ìš©ì„ ì¶”ê°€í•˜ì„¸ìš”:\")\n",
    "        print(\"   java/\")\n",
    "        print(\"   openjdk-*.zip\")\n",
    "        print(\"   *.jdk\")\n",
    "        print(\"   .ipynb_checkpoints\")\n",
    "        return False\n",
    "\n",
    "def install_java_and_konlpy():\n",
    "    \"\"\"Java ì„¤ì¹˜ ë° KoNLPy ì„¤ì •\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”„ Java ë° KoNLPy ì„¤ì¹˜ ì‹œì‘...\")\n",
    "    \n",
    "    # ë¨¼ì € .gitignore ì—…ë°ì´íŠ¸ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)\n",
    "    print(\"ğŸ“ .gitignore ì—…ë°ì´íŠ¸ ì‹œë„...\")\n",
    "    gitignore_success = update_gitignore()\n",
    "    if not gitignore_success:\n",
    "        print(\"âš ï¸ .gitignore ì„¤ì • ì‹¤íŒ¨í–ˆì§€ë§Œ Java ì„¤ì¹˜ëŠ” ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤\")\n",
    "        print(\"ğŸ“Œ ë‚˜ì¤‘ì— ìˆ˜ë™ìœ¼ë¡œ .gitignoreì— 'java/' ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”\")\n",
    "    \n",
    "    # 1ë‹¨ê³„: Java ì„¤ì¹˜ ì—¬ë¶€ í™•ì¸\n",
    "    def check_java():\n",
    "        try:\n",
    "            result = subprocess.run(['java', '-version'], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(\"âœ… Javaê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤\")\n",
    "                return True\n",
    "        except FileNotFoundError:\n",
    "            print(\"âŒ Javaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤\")\n",
    "        return False\n",
    "    \n",
    "    # 2ë‹¨ê³„: Java ìë™ ë‹¤ìš´ë¡œë“œ (Portable)\n",
    "    def download_portable_java():\n",
    "        print(\"ğŸ“¥ Portable Java ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "        \n",
    "        # OpenJDK ë‹¤ìš´ë¡œë“œ URL (Windows x64)\n",
    "        java_url = \"https://download.java.net/java/GA/jdk17.0.2/dfd4a8d0985749f896bed50d7138ee7f/8/GPL/openjdk-17.0.2_windows-x64_bin.zip\"\n",
    "        java_zip = \"openjdk-17.0.2.zip\"\n",
    "        java_dir = \"java\"\n",
    "        \n",
    "        try:\n",
    "            # ì§„í–‰ë¥  í‘œì‹œ í•¨ìˆ˜\n",
    "            def show_progress(block_num, block_size, total_size):\n",
    "                downloaded = block_num * block_size\n",
    "                if total_size > 0:\n",
    "                    percent = min(100, (downloaded * 100) // total_size)\n",
    "                    print(f\"\\rë‹¤ìš´ë¡œë“œ ì§„í–‰ë¥ : {percent}% ({downloaded // (1024*1024)}MB / {total_size // (1024*1024)}MB)\", end=\"\")\n",
    "            \n",
    "            # ë‹¤ìš´ë¡œë“œ\n",
    "            urllib.request.urlretrieve(java_url, java_zip, show_progress)\n",
    "            print(\"\\nâœ… Java ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\")\n",
    "            \n",
    "            # ì••ì¶• í•´ì œ\n",
    "            print(\"ğŸ“‚ Java ì••ì¶• í•´ì œ ì¤‘...\")\n",
    "            with zipfile.ZipFile(java_zip, 'r') as zip_ref:\n",
    "                zip_ref.extractall(java_dir)\n",
    "            \n",
    "            # ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì‚­ì œ\n",
    "            os.remove(java_zip)\n",
    "            print(\"âœ… ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ\")\n",
    "            \n",
    "            # Java ê²½ë¡œ ì°¾ê¸°\n",
    "            for root, dirs, files in os.walk(java_dir):\n",
    "                if 'java.exe' in files:\n",
    "                    java_home = os.path.dirname(root)\n",
    "                    print(f\"âœ… Java ì„¤ì¹˜ ê²½ë¡œ: {java_home}\")\n",
    "                    return java_home\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Java ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # 3ë‹¨ê³„: Java ê²½ë¡œ ìë™ ì°¾ê¸°\n",
    "    def find_java_home():\n",
    "        # ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ Java ì°¾ê¸°\n",
    "        possible_paths = [\n",
    "            \"C:/Program Files/Java/*/\",\n",
    "            \"C:/Program Files (x86)/Java/*/\",\n",
    "            \"C:/Users/*/AppData/Local/Programs/Java/*/\",\n",
    "        ]\n",
    "        \n",
    "        import glob\n",
    "        for pattern in possible_paths:\n",
    "            paths = glob.glob(pattern)\n",
    "            for path in paths:\n",
    "                java_exe = os.path.join(path, \"bin\", \"java.exe\")\n",
    "                if os.path.exists(java_exe):\n",
    "                    return path\n",
    "        return None\n",
    "    \n",
    "    # Java ì„¤ì¹˜ í”„ë¡œì„¸ìŠ¤\n",
    "    java_home = None\n",
    "    \n",
    "    if not check_java():\n",
    "        print(\"ğŸ”§ ìë™ìœ¼ë¡œ Portable Javaë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...\")\n",
    "        java_home = download_portable_java()\n",
    "        \n",
    "        if not java_home:\n",
    "            print(\"\\nğŸ“‹ ìë™ ì„¤ì¹˜ ì‹¤íŒ¨. ìˆ˜ë™ ì„¤ì¹˜ ë°©ë²•:\")\n",
    "            print(\"1. https://www.oracle.com/java/technologies/downloads/ ë°©ë¬¸\")\n",
    "            print(\"2. Windows x64 Installer ë‹¤ìš´ë¡œë“œ\")\n",
    "            print(\"3. ì„¤ì¹˜ í›„ ì•„ë˜ ì½”ë“œ ë‹¤ì‹œ ì‹¤í–‰\")\n",
    "            return False\n",
    "    else:\n",
    "        java_home = find_java_home()\n",
    "    \n",
    "    # 4ë‹¨ê³„: JAVA_HOME ì„¤ì •\n",
    "    if java_home:\n",
    "        os.environ['JAVA_HOME'] = java_home\n",
    "        java_bin = os.path.join(java_home, \"bin\")\n",
    "        current_path = os.environ.get('PATH', '')\n",
    "        if java_bin not in current_path:\n",
    "            os.environ['PATH'] = java_bin + os.pathsep + current_path\n",
    "        print(f\"âœ… JAVA_HOME ì„¤ì •: {java_home}\")\n",
    "        \n",
    "        # 5ë‹¨ê³„: KoNLPy ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸\n",
    "        try:\n",
    "            print(\"ğŸ“¦ KoNLPy ì„¤ì¹˜ ì¤‘...\")\n",
    "            subprocess.run([sys.executable, '-m', 'pip', 'install', 'konlpy', 'JPype1'], check=True)\n",
    "            \n",
    "            print(\"ğŸ§ª KoNLPy í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "            from konlpy.tag import Okt\n",
    "            okt = Okt()\n",
    "            result = okt.morphs(\"í…ŒìŠ¤íŠ¸ ì„±ê³µ\")\n",
    "            print(f\"âœ… KoNLPy ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸ ì„±ê³µ: {result}\")\n",
    "            \n",
    "            print(\"\\nğŸ‰ ì„¤ì¹˜ ì™„ë£Œ!\")\n",
    "            print(\"=\" * 50)\n",
    "            print(\"ğŸ“ ì„¤ì¹˜ëœ íŒŒì¼ë“¤:\")\n",
    "            print(f\"   - Java Runtime: {java_home}\")\n",
    "            print(\"   - KoNLPy: ì„¤ì¹˜ë¨\")\n",
    "            print(\"   - .gitignore: ì—…ë°ì´íŠ¸ë¨\")\n",
    "            print(\"\\nğŸ’¡ ì´ì œ í˜•íƒœì†Œ ë¶„ì„ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n",
    "            print(\"   ì»¤ë„ì„ ì¬ì‹œì‘í•œ í›„ ì›ë˜ ë¶„ì„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ KoNLPy ì„¤ì¹˜ ì‹¤íŒ¨: {e}\")\n",
    "            print(\"ğŸ’¡ ì»¤ë„ì„ ì¬ì‹œì‘í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"âŒ Java ì„¤ì¹˜ ì‹¤íŒ¨\")\n",
    "        return False\n",
    "\n",
    "# ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    if install_java_and_konlpy():\n",
    "        print(\"ğŸ‰ ëª¨ë“  ì„¤ì¹˜ ì™„ë£Œ! í˜•íƒœì†Œ ë¶„ì„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "        KONLPY_AVAILABLE = True\n",
    "    else:\n",
    "        print(\"âš ï¸ Java ì„¤ì¹˜ ì‹¤íŒ¨. ì •ê·œì‹ ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
    "        KONLPY_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e11dc8",
   "metadata": {},
   "source": [
    "ã€ì‹œê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„ã€‘\n",
    "\n",
    "1ï¸âƒ£  ì—°ë„ë³„ íŠ¸ë Œë“œ ë¶„ì„ (ê²Œì‹œê¸€ìˆ˜, ì¡°íšŒìˆ˜, ëŒ“ê¸€ìˆ˜)\n",
    "\n",
    "2ï¸âƒ£  ì›”ë³„ íŠ¸ë Œë“œ ë¶„ì„ (ê²Œì‹œê¸€ìˆ˜, ì¡°íšŒìˆ˜, ëŒ“ê¸€ìˆ˜)\n",
    "\n",
    "3ï¸âƒ£  ì£¼ê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„ (ê²Œì‹œê¸€ìˆ˜, ì¡°íšŒìˆ˜, ëŒ“ê¸€ìˆ˜)\n",
    "\n",
    "ã€ì‹œê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ã€‘\n",
    "\n",
    "4ï¸âƒ£  ì—°ë„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\n",
    "\n",
    "5ï¸âƒ£  ì›”ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\n",
    "\n",
    "6ï¸âƒ£  ì£¼ê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ì „ì²´ ì£¼ì°¨)\n",
    "\n",
    "ã€ì „ì²´ ë°ì´í„° ë¶„ì„ã€‘\n",
    "\n",
    "7ï¸âƒ£  ì „ì²´ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\n",
    "\n",
    "8ï¸âƒ£  ì •ê·œì‹ ê¸°ë°˜ ë‹¨ì–´ ë¹ˆë„ (í˜•íƒœì†Œ ë¶„ì„ ì—†ìŒ)\n",
    "\n",
    "ã€ê²°ê³¼ ì €ì¥ ë° ë¦¬í¬íŠ¸ã€‘\n",
    "\n",
    "9ï¸âƒ£  ëª¨ë“  ë¶„ì„ ê²°ê³¼ Excel ì €ì¥\n",
    "\n",
    "ğŸ”Ÿ  ì¢…í•© ë¦¬í¬íŠ¸ ì¶œë ¥\n",
    "\n",
    "ã€ê·¸ë˜í”„ ì‹œê°í™”ã€‘\n",
    "\n",
    "1ï¸âƒ£1ï¸âƒ£ ì—°ë„ë³„ íŠ¸ë Œë“œ ê·¸ë˜í”„\n",
    "\n",
    "1ï¸âƒ£2ï¸âƒ£ ì›”ë³„ íŠ¸ë Œë“œ ê·¸ë˜í”„\n",
    "\n",
    "1ï¸âƒ£3ï¸âƒ£ ì£¼ê°„ë³„ íŠ¸ë Œë“œ ê·¸ë˜í”„\n",
    "\n",
    "1ï¸âƒ£4ï¸âƒ£ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ ê·¸ë˜í”„\n",
    "\n",
    "ã€í”„ë¡œê·¸ë¨ ì¢…ë£Œã€‘\n",
    "0ï¸âƒ£  í”„ë¡œê·¸ë¨ ì¢…ë£Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78587a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import koreanize_matplotlib\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„¤ì¹˜ í•„ìš”)\n",
    "try:\n",
    "    from konlpy.tag import Okt, Mecab, Kkma\n",
    "    KONLPY_AVAILABLE = True\n",
    "    print(\"âœ… KoNLPy ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "except ImportError:\n",
    "    KONLPY_AVAILABLE = False\n",
    "    print(\"âš ï¸ KoNLPy ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    print(\"ì„¤ì¹˜ ë°©ë²•: !pip install konlpy\")\n",
    "\n",
    "class AdvancedCommunityDataAnalyzer:\n",
    "    def __init__(self, csv_file_path):\n",
    "        \"\"\"\n",
    "        ê°œì„ ëœ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            csv_file_path (str): CSV íŒŒì¼ ê²½ë¡œ\n",
    "        \"\"\"\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.df = None\n",
    "        self.processed_df = None\n",
    "        self.filtered_df = None  # ë„ë°° í•„í„°ë§ëœ ë°ì´í„°\n",
    "        \n",
    "        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "        if KONLPY_AVAILABLE:\n",
    "            try:\n",
    "                self.okt = Okt()\n",
    "                self.morphology_analyzer = self.okt\n",
    "                self.use_morphology = True\n",
    "                print(\"âœ… Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "            except:\n",
    "                self.use_morphology = False\n",
    "                print(\"âš ï¸ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨. ê¸°ë³¸ ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            self.use_morphology = False\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê¸°ë³¸ ì •ë³´ë¥¼ ì¶œë ¥\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.csv_file_path, encoding='utf-8')\n",
    "            print(\"=== ë°ì´í„° ë¡œë“œ ì™„ë£Œ ===\")\n",
    "            print(f\"ì´ ë°ì´í„° ìˆ˜: {len(self.df):,}ê°œ\")\n",
    "            print(f\"ì»¬ëŸ¼: {list(self.df.columns)}\")\n",
    "            print(\"\\n=== ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ===\")\n",
    "            print(self.df.head())\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
    "        if self.df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.\")\n",
    "            return False\n",
    "        \n",
    "        # ë°ì´í„° ë³µì‚¬\n",
    "        self.processed_df = self.df.copy()\n",
    "        \n",
    "        # ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜\n",
    "        def convert_date_pandas(date_str):\n",
    "            if pd.isna(date_str):\n",
    "                return pd.NaT\n",
    "            \n",
    "            date_str = str(date_str)\n",
    "            try:\n",
    "                if '.' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%y.%m.%d')\n",
    "                elif '/' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%y/%m/%d')\n",
    "                else:\n",
    "                    return pd.NaT\n",
    "            except:\n",
    "                return pd.NaT\n",
    "        \n",
    "        # ë‚ ì§œ ë³€í™˜\n",
    "        self.processed_df['ë‚ ì§œ_ë³€í™˜'] = self.processed_df['ë‚ ì§œ'].apply(convert_date_pandas)\n",
    "        self.processed_df = self.processed_df.dropna(subset=['ë‚ ì§œ_ë³€í™˜'])\n",
    "        \n",
    "        # ì—°ë„, ì—°ì›”, ì£¼ì°¨ ì»¬ëŸ¼ ì¶”ê°€\n",
    "        self.processed_df['ì—°ë„'] = self.processed_df['ë‚ ì§œ_ë³€í™˜'].dt.year\n",
    "        self.processed_df['ì—°ì›”'] = self.processed_df['ë‚ ì§œ_ë³€í™˜'].dt.to_period('M')\n",
    "        self.processed_df['ì£¼ì°¨'] = self.processed_df['ë‚ ì§œ_ë³€í™˜'].dt.to_period('W')\n",
    "        \n",
    "        # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜\n",
    "        def safe_convert_to_numeric(x):\n",
    "            try:\n",
    "                if pd.isna(x) or x == '' or x == 'NaN':\n",
    "                    return 0\n",
    "                if isinstance(x, str):\n",
    "                    clean_num = ''.join(filter(str.isdigit, str(x)))\n",
    "                    return int(clean_num) if clean_num else 0\n",
    "                return int(x)\n",
    "            except:\n",
    "                return 0\n",
    "        \n",
    "        self.processed_df['ì¡°íšŒìˆ˜'] = self.processed_df['ì¡°íšŒìˆ˜'].apply(safe_convert_to_numeric)\n",
    "        self.processed_df['ëŒ“ê¸€ê°¯ìˆ˜'] = self.processed_df['ëŒ“ê¸€ê°¯ìˆ˜'].apply(safe_convert_to_numeric)\n",
    "        \n",
    "        # ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "        self.processed_df['ì œëª©'] = self.processed_df['ì œëª©'].fillna(\"\")\n",
    "        self.processed_df['ë‚´ìš©'] = self.processed_df['ë‚´ìš©'].fillna(\"\")\n",
    "        \n",
    "        print(\"=== ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ ===\")\n",
    "        print(f\"ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜: {len(self.processed_df):,}ê°œ\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def calculate_text_similarity(self, text1, text2):\n",
    "        \"\"\"ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1)\"\"\"\n",
    "        return SequenceMatcher(None, text1, text2).ratio()\n",
    "    \n",
    "    def detect_spam_posts_exact_only(self, use_hashing=True):\n",
    "        \"\"\"\n",
    "        ì™„ì „íˆ ë˜‘ê°™ì€ ê¸€ë§Œ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬\n",
    "        \n",
    "        Args:\n",
    "            use_hashing (bool): í•´ì‹œ ê¸°ë°˜ ë¹ ë¥¸ í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=== ì™„ì „ ë™ì¼í•œ ì¤‘ë³µ ê²Œì‹œê¸€ë§Œ íƒì§€ ===\")\n",
    "        print(f\"ğŸ“Š ì´ {len(self.processed_df):,}ê°œ ê²Œì‹œê¸€ ë¶„ì„\")\n",
    "        print(\"ğŸ” ê¸€ì í•˜ë‚˜ë¼ë„ ë‹¤ë¥´ë©´ ë‹¤ë¥¸ ê¸€ë¡œ ì¸ì •í•©ë‹ˆë‹¤\")\n",
    "        \n",
    "        spam_indices = set()\n",
    "        \n",
    "        if use_hashing:\n",
    "            # í•´ì‹œ ê¸°ë°˜ ì™„ì „ ì¤‘ë³µ íƒì§€ë§Œ ì‚¬ìš©\n",
    "            print(\"ğŸš€ í•´ì‹œ ê¸°ë°˜ ì™„ì „ ì¤‘ë³µ íƒì§€...\")\n",
    "            title_hashes = {}\n",
    "            content_hashes = {}\n",
    "            combined_hashes = {}  # ì œëª©+ë‚´ìš© í†µí•© í•´ì‹œ\n",
    "            \n",
    "            for idx, row in self.processed_df.iterrows():\n",
    "                # ê³µë°± ì •ê·œí™” (ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ)\n",
    "                title_clean = re.sub(r'\\s+', ' ', str(row['ì œëª©']).strip())\n",
    "                content_clean = re.sub(r'\\s+', ' ', str(row['ë‚´ìš©']).strip())\n",
    "                combined_clean = title_clean + \" | \" + content_clean\n",
    "                \n",
    "                # 1. ì œëª© ì™„ì „ ì¤‘ë³µ ì²´í¬\n",
    "                title_hash = hash(title_clean.lower())\n",
    "                if title_hash in title_hashes:\n",
    "                    spam_indices.add(idx)\n",
    "                else:\n",
    "                    title_hashes[title_hash] = idx\n",
    "                \n",
    "                # 2. ë‚´ìš© ì™„ì „ ì¤‘ë³µ ì²´í¬ (3ê¸€ì ì´ìƒì¸ ê²½ìš°ë§Œ)\n",
    "                if len(content_clean) >= 3:\n",
    "                    content_hash = hash(content_clean.lower())\n",
    "                    if content_hash in content_hashes:\n",
    "                        spam_indices.add(idx)\n",
    "                    else:\n",
    "                        content_hashes[content_hash] = idx\n",
    "                \n",
    "                # 3. ì œëª©+ë‚´ìš© í†µí•© ì™„ì „ ì¤‘ë³µ ì²´í¬\n",
    "                combined_hash = hash(combined_clean.lower())\n",
    "                if combined_hash in combined_hashes:\n",
    "                    spam_indices.add(idx)\n",
    "                else:\n",
    "                    combined_hashes[combined_hash] = idx\n",
    "            \n",
    "            print(f\"   ì´ ì¤‘ë³µ ë°œê²¬: {len(spam_indices):,}ê°œ\")\n",
    "        \n",
    "        # ì¤‘ë³µì´ ì•„ë‹Œ ê²Œì‹œê¸€ë§Œ í•„í„°ë§\n",
    "        self.filtered_df = self.processed_df[~self.processed_df.index.isin(spam_indices)].copy()\n",
    "        \n",
    "        print(f\"\\n=== ì™„ì „ ë™ì¼ ì¤‘ë³µ í•„í„°ë§ ê²°ê³¼ ===\")\n",
    "        print(f\"ì›ë³¸ ê²Œì‹œê¸€ ìˆ˜: {len(self.processed_df):,}ê°œ\")\n",
    "        print(f\"ì™„ì „ ë™ì¼ ì¤‘ë³µ ìˆ˜: {len(spam_indices):,}ê°œ ({len(spam_indices)/len(self.processed_df)*100:.1f}%)\")\n",
    "        print(f\"í•„í„°ë§ í›„ ê²Œì‹œê¸€ ìˆ˜: {len(self.filtered_df):,}ê°œ\")\n",
    "        print(f\"âœ… ê¸€ì í•˜ë‚˜ë¼ë„ ë‹¤ë¥´ë©´ ë³„ê°œ ê²Œì‹œê¸€ë¡œ ìœ ì§€ë©ë‹ˆë‹¤!\")\n",
    "        \n",
    "        return spam_indices\n",
    "    \n",
    "    def analyze_yearly_trends(self, save_to_csv=True):\n",
    "        \"\"\"ì—°ë„ë³„ íŠ¸ë Œë“œ ë¶„ì„ (í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥, CSV ì €ì¥ ì˜µì…˜)\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        # ì—°ë„ë³„ í†µê³„ ê³„ì‚°\n",
    "        yearly_stats = self.processed_df.groupby('ì—°ë„').agg({\n",
    "            'ë²ˆí˜¸': 'count',\n",
    "            'ì¡°íšŒìˆ˜': ['sum', 'mean', 'max'],\n",
    "            'ëŒ“ê¸€ê°¯ìˆ˜': ['sum', 'mean']\n",
    "        }).round(2)\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… ì •ë¦¬\n",
    "        yearly_stats.columns = ['ê²Œì‹œê¸€ìˆ˜', 'ì´ì¡°íšŒìˆ˜', 'í‰ê· ì¡°íšŒìˆ˜', 'ìµœëŒ€ì¡°íšŒìˆ˜', 'ì´ëŒ“ê¸€ìˆ˜', 'í‰ê· ëŒ“ê¸€ìˆ˜']\n",
    "        yearly_stats = yearly_stats.reset_index()\n",
    "        \n",
    "        print(\"=== ì—°ë„ë³„ í†µê³„ (í…ìŠ¤íŠ¸ ì¶œë ¥) ===\")\n",
    "        for _, row in yearly_stats.iterrows():\n",
    "            print(f\"{int(row['ì—°ë„'])}: ê²Œì‹œê¸€ {row['ê²Œì‹œê¸€ìˆ˜']:,}ê°œ, \"\n",
    "                  f\"ì´ì¡°íšŒìˆ˜ {row['ì´ì¡°íšŒìˆ˜']:,.0f}, í‰ê· ì¡°íšŒìˆ˜ {row['í‰ê· ì¡°íšŒìˆ˜']:.1f}, \"\n",
    "                  f\"ìµœëŒ€ì¡°íšŒìˆ˜ {row['ìµœëŒ€ì¡°íšŒìˆ˜']:,.0f}, ì´ëŒ“ê¸€ìˆ˜ {row['ì´ëŒ“ê¸€ìˆ˜']:,.0f}, \"\n",
    "                  f\"í‰ê· ëŒ“ê¸€ìˆ˜ {row['í‰ê· ëŒ“ê¸€ìˆ˜']:.1f}\")\n",
    "        \n",
    "        # CSV ì €ì¥\n",
    "        if save_to_csv:\n",
    "            filename = 'yearly_trends_analysis.csv'\n",
    "            yearly_stats.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\nâœ… ì—°ë„ë³„ ë¶„ì„ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return yearly_stats\n",
    "    \n",
    "    def analyze_monthly_trends(self):\n",
    "        \"\"\"ì›”ë³„ íŠ¸ë Œë“œ ë¶„ì„\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        # ì›”ë³„ í†µê³„ ê³„ì‚°\n",
    "        monthly_stats = self.processed_df.groupby('ì—°ì›”').agg({\n",
    "            'ë²ˆí˜¸': 'count',\n",
    "            'ì¡°íšŒìˆ˜': ['sum', 'mean', 'max'],\n",
    "            'ëŒ“ê¸€ê°¯ìˆ˜': ['sum', 'mean']\n",
    "        }).round(2)\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… ì •ë¦¬\n",
    "        monthly_stats.columns = ['ê²Œì‹œê¸€ìˆ˜', 'ì´ì¡°íšŒìˆ˜', 'í‰ê· ì¡°íšŒìˆ˜', 'ìµœëŒ€ì¡°íšŒìˆ˜', 'ì´ëŒ“ê¸€ìˆ˜', 'í‰ê· ëŒ“ê¸€ìˆ˜']\n",
    "        monthly_stats = monthly_stats.reset_index()\n",
    "        monthly_stats['ì—°ì›”_str'] = monthly_stats['ì—°ì›”'].astype(str)\n",
    "        \n",
    "        print(\"=== ì›”ë³„ í†µê³„ ===\")\n",
    "        for _, row in monthly_stats.iterrows():\n",
    "            print(f\"{row['ì—°ì›”_str']}: ê²Œì‹œê¸€ {row['ê²Œì‹œê¸€ìˆ˜']:,}ê°œ, \"\n",
    "                  f\"ì´ì¡°íšŒìˆ˜ {row['ì´ì¡°íšŒìˆ˜']:,.0f}, í‰ê· ì¡°íšŒìˆ˜ {row['í‰ê· ì¡°íšŒìˆ˜']:.1f}\")\n",
    "        \n",
    "        return monthly_stats\n",
    "    \n",
    "    def _contains_profanity_pattern(self, word):\n",
    "        \"\"\"\n",
    "        ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ìš•ì„¤ íƒì§€\n",
    "        \"\"\"\n",
    "        profanity_patterns = [\n",
    "            r'ì‹œ.*ë°œ',      # ì‹œë°œ, ì‹œì´ë°œ, ì‹œ1ë°œ ë“±\n",
    "            r'ê°œ.*ìƒˆë¼',    # ê°œìƒˆë¼, ê°œìŒ”ë¼ ë“±\n",
    "            r'ë³‘.*ì‹ ',      # ë³‘ì‹ , ë³‘1ì‹  ë“±\n",
    "            r'ë¯¸.*ì¹œ',      # ë¯¸ì¹œ, ë¯¸1ì¹œ ë“±\n",
    "            r'[ã……ã…†][ã…‚ã…]',  # ã……ã…‚, ã…†ã…‚ ë“±\n",
    "            r'[ì§€ã…ˆ][ë„ã„¹]',  # ì§€ë„, ã…ˆã„¹ ë“±\n",
    "            r'êº¼.*ì ¸',      # êº¼ì ¸, êº¼1ì ¸ ë“±\n",
    "            r'ì£½.*ì–´',      # ì£½ì–´, ë’ˆì ¸ ë“±\n",
    "            r'ëŠ.*ê¸ˆ',      # ëŠê¸ˆë§ˆ ë“±\n",
    "            r'.*ì¢†.*',      # ì¢†ì´ í¬í•¨ëœ ëª¨ë“  ë‹¨ì–´\n",
    "            r'.*ì”¨.*ë°œ.*',  # ì”¨ë°œ ë³€í˜•\n",
    "            r'.*ê°œ.*ë†ˆ',    # ê°œë†ˆ ë³€í˜•\n",
    "        ]\n",
    "        \n",
    "        for pattern in profanity_patterns:\n",
    "            if re.search(pattern, word, re.IGNORECASE):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def extract_korean_morphemes(self, text, extract_nouns=True, extract_verbs=False, extract_adjectives=False):\n",
    "        \"\"\"\n",
    "        í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ë‹¨ì–´ ì¶”ì¶œ\n",
    "        \n",
    "        Args:\n",
    "            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸\n",
    "            extract_nouns (bool): ëª…ì‚¬ ì¶”ì¶œ ì—¬ë¶€\n",
    "            extract_verbs (bool): ë™ì‚¬ ì¶”ì¶œ ì—¬ë¶€  \n",
    "            extract_adjectives (bool): í˜•ìš©ì‚¬ ì¶”ì¶œ ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        if not self.use_morphology:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # í˜•íƒœì†Œ ë¶„ì„\n",
    "            morphemes = self.morphology_analyzer.pos(text, stem=True)\n",
    "            \n",
    "            extracted_words = []\n",
    "            for word, pos in morphemes:\n",
    "                # 2ê¸€ì ì´ìƒë§Œ ì¶”ì¶œ\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                    \n",
    "                # í’ˆì‚¬ë³„ ì¶”ì¶œ\n",
    "                if extract_nouns and pos.startswith('N'):  # ëª…ì‚¬\n",
    "                    extracted_words.append(word)\n",
    "                elif extract_verbs and pos.startswith('V'):  # ë™ì‚¬\n",
    "                    extracted_words.append(word)\n",
    "                elif extract_adjectives and pos.startswith('A'):  # í˜•ìš©ì‚¬\n",
    "                    extracted_words.append(word)\n",
    "            \n",
    "            return extracted_words\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"í˜•íƒœì†Œ ë¶„ì„ ì˜¤ë¥˜: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _get_filtered_words_advanced(self, text, use_morphology=True):\n",
    "        \"\"\"\n",
    "        ê°œì„ ëœ ë‹¨ì–´ í•„í„°ë§ (í˜•íƒœì†Œ ë¶„ì„ + ê¸°ì¡´ ë°©ì‹ + ìš•ì„¤ í•„í„°ë§)\n",
    "        \"\"\"\n",
    "        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ì‚¬ìš©\n",
    "        if use_morphology and self.use_morphology:\n",
    "            korean_words = self.extract_korean_morphemes(\n",
    "                text, \n",
    "                extract_nouns=True, \n",
    "                extract_verbs=False,  # ë™ì‚¬ëŠ” ì œì™¸ (ì˜ë¯¸ê°€ ëª¨í˜¸í•  ìˆ˜ ìˆìŒ)\n",
    "                extract_adjectives=False  # í˜•ìš©ì‚¬ë„ ì œì™¸\n",
    "            )\n",
    "        else:\n",
    "            # ê¸°ì¡´ ë°©ì‹: ì •ê·œì‹ìœ¼ë¡œ í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ\n",
    "            korean_words = re.findall(r'[ê°€-í£]{2,}', text)\n",
    "        \n",
    "        # ì˜ì–´/ìˆ«ì ë‹¨ì–´ ì¶”ì¶œ\n",
    "        english_words = re.findall(r'[a-zA-Z0-9]{2,}', text.lower())\n",
    "        \n",
    "        # ëª¨ë“  ë‹¨ì–´ í•©ì¹˜ê¸°\n",
    "        all_words = korean_words + english_words\n",
    "        \n",
    "        # í™•ì¥ëœ ë¶ˆìš©ì–´ ëª©ë¡ (ìš•ì„¤ í¬í•¨)\n",
    "        stop_words = {\n",
    "            # ê¸°ì¡´ ë¶ˆìš©ì–´ë“¤\n",
    "            'ì´ê±°', 'ì´ê±´', 'ì €ê±°', 'ì €ê±´', 'ê·¸ê±°', 'ê·¸ê±´', 'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°',\n",
    "            'ì´ê²Œ', 'ì €ê²Œ', 'ê·¸ê²Œ', 'ì´ì•¼', 'ì €ì•¼', 'ê·¸ì•¼', 'ì´ëŸ°', 'ì €ëŸ°', 'ê·¸ëŸ°',\n",
    "            'ë­ì•¼', 'ë­”ê°€', 'ì§„ì§œ', 'ì •ë§', 'ì™„ì „', 'ì•„ë‹ˆ', 'ê·¸ëƒ¥', 'ì¢€', 'ë”', \n",
    "            'ë„ˆë¬´', 'ë˜ê²Œ', 'ì—„ì²­', 'ì™„ì „íˆ', 'ì •ë§ë¡œ', 'ì§„ì§œë¡œ', 'ë˜ë©´', 'í•˜ì§€ë§Œ',\n",
    "            'ê·¼ë°', 'ê·¸ëŸ°ë°', 'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë‚˜', \n",
    "            'ê·¸ë ‡ì§€ë§Œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë§ê³ ', 'í•´ì„œ', 'ë˜ê³ ', 'í•˜ê³ ', 'ìˆê³ ', 'ì—†ê³ ',\n",
    "            'ì´ì œ', 'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ìš”ì¦˜', 'ìµœê·¼', 'ì–¸ì œ', 'ë°”ë¡œ',\n",
    "            'ë‚´ìš©', 'ì—†ìŒ', 'ê²½ìš°', 'ë•Œë¬¸', 'ë˜ëŠ”', 'í•˜ëŠ”', 'ìˆëŠ”', 'ì—†ëŠ”',\n",
    "            'ì´ë ‡ê²Œ', 'ì €ë ‡ê²Œ', 'ê·¸ë ‡ê²Œ', 'ì–´ë–»ê²Œ', 'ì™œëƒ', 'ë•Œë¬¸ì—', 'ì´ë¼ê³ ',\n",
    "            'ë‚´ê°€', 'ë‚˜ëŠ”', 'ë„ˆëŠ”', 'ë„ˆê°€', 'ê±”ëŠ”', 'ê±”ê°€', 'ìŸ¤ëŠ”', 'ìŸ¤ê°€',\n",
    "            'ìš°ë¦¬ëŠ”', 'ìš°ë¦¬ê°€', 'ì €ëŠ”', 'ì €ê°€', 'ê·¸ê°€', 'ê·¸ëŠ”', 'ê·¸ë…€ëŠ”', 'ê·¸ë…€ê°€',\n",
    "            \n",
    "            # ì»¤ë®¤ë‹ˆí‹° íŠ¹í™” ë¶ˆìš©ì–´\n",
    "            'ê²Œì‹œê¸€', 'ëŒ“ê¸€', 'ì¡°íšŒìˆ˜', 'ì¶”ì²œ', 'ë¹„ì¶”ì²œ', 'ì‹ ê³ ', 'ìˆ˜ì •', 'ì‚­ì œ',\n",
    "            'ì‘ì„±ì', 'ë‹‰ë„¤ì„', 'ì•„ì´ë””', 'íšŒì›', 'ë“±ê¸‰', 'í¬ì¸íŠ¸', 'ê²Œì‹œíŒ',\n",
    "            'dc', 'official', 'app', 'ë‹¤ì‹œ', 'ê³„ì†', 'ì—¬ê¸°ì„œ', 'ë§ì´', 'ì œë°œ', 'name',\n",
    "            'txt','ìœ¼í›„ë£¨ê¾¸ê¾¸ë£¨í›„ìœ¼','ë£¨ê¾¸ê¾¸ë£¨','ìš´ì§€','ë…¸ë¬´í˜„','ì¼ë² ','https',\n",
    "            \n",
    "            # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ ìì£¼ ë‚˜ì˜¤ëŠ” ë¶ˆìš©ì–´\n",
    "            'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ê³³', 'ì ', 'ë²ˆ', 'ê°œ', 'ëª…', 'ë…„', 'ì›”', 'ì¼',\n",
    "            'ì‹œê°„', 'ë¶„', 'ì´ˆ', 'ì •ë„', 'ë§Œí¼', 'ì´ìƒ', 'ì´í•˜', 'ì‚¬ì´', 'ì¤‘',\n",
    "            'ì•ˆ', 'ë°–', 'ìœ„', 'ì•„ë˜', 'ì•', 'ë’¤', 'ì˜†', 'ë‹¤ìŒ', 'ì´ì „', 'ë§ˆì§€ë§‰',\n",
    "            'ì²˜ìŒ', 'ë', 'ì‹œì‘', 'ì¢…ë£Œ', 'ì™„ë£Œ', 'ì‹œë„', 'ë…¸ë ¥', 'ìƒê°', 'ì˜ê²¬',\n",
    "            'ë¬¸ì œ', 'í•´ê²°', 'ìƒí™©', 'ìƒíƒœ', 'ê²°ê³¼', 'ê³¼ì •', 'ë°©ë²•', 'ë°©ì‹',\n",
    "            \n",
    "            # ê°ì • í‘œí˜„ (ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²ƒë“¤)\n",
    "            'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ì‹«ë‹¤', 'ì¢‹ì•„', 'ì‹«ì–´', 'ì¬ë¯¸', 'ì¬ë°Œ', 'boring',\n",
    "            'ì›ƒìŒ', 'ìŠ¬í””', 'ê¸°ì¨', 'í™”ë‚¨', 'ë†€ëŒ', 'ê±±ì •', 'ë¶ˆì•ˆ', 'ì•ˆì‹¬',\n",
    "            \n",
    "            # === ìš•ì„¤ ë° ë¹„ì†ì–´ í•„í„°ë§ ===\n",
    "            # ì¼ë°˜ì ì¸ ìš•ì„¤\n",
    "            'ì‹œë°œ', 'ì”¨ë°œ', 'ã……ã…‚', 'ã…†ã…‚', 'ì‹œíŒ”', 'ì”¨íŒ”', 'ì‹œë°”', 'ì”¨ë°”',\n",
    "            'ê°œìƒˆë¼', 'ê°œìƒ‰ë¼', 'ê°œìƒˆí‚¤', 'ê°œìƒ‰í‚¤', 'ê°œë†ˆ', 'ê°œë…„', 'ê°œì…', \n",
    "            'ê°œì“°ë ˆê¸°', 'ê°œë¼ì§€', 'ê°œë³‘ì‹ ', 'ê°œë°”ë³´', 'ê°œë©ì²­ì´',\n",
    "            'ë³‘ì‹ ', 'ë¸…ì‹ ', 'ã…‚ã……', 'ë°”ë³´', 'ë©ì²­ì´', 'ë“±ì‹ ', 'ì²œì¹˜',\n",
    "            'ë¯¸ì¹œë†ˆ', 'ë¯¸ì¹œë…„', 'ë¯¸ì¹œìƒˆë¼', 'ë¯¸ì¹œê²ƒ', 'ë¯¸ì¹œê°œ', 'ë¯¸ì³¤ë‚˜',\n",
    "            'ë˜ë¼ì´', 'ë˜ë¼ì‡', 'ëŒì•„ì´', 'ëŒì•˜ë‚˜', 'ì •ì‹ ë³‘ì', 'ì •ì‹ ë‚˜ê°„',\n",
    "            'ì£½ì–´', 'ë’ˆì ¸', 'ë’¤ì ¸', 'ì£½ì–´ë¼', 'ë””ì ¸ë¼', 'ë””ì ¸', 'ë””ì§„ë‹¤',\n",
    "            'êº¼ì ¸', 'êº¼ì§€ë¼', 'êº¼ì €', 'êº¼ìª„', 'ì‚¬ë¼ì ¸', 'ì—†ì–´ì ¸',\n",
    "            'ì§€ë„', 'ì§€ëŸ´', 'ã…ˆã„¹', 'í—›ì†Œë¦¬', 'ê°œì†Œë¦¬', 'ë˜¥ì‹¸ë‹¤', 'ë˜¥',\n",
    "            'ì—¿ë¨¹ì–´', 'ì—¿ì´ë‚˜', 'ì¢†', 'ã…ˆ', 'ìì§€', 'ì¢†ê¹Œ', 'ì¢†ë‚˜',\n",
    "            'ë‹ˆë¯¸', 'ë‹ˆì• ë¯¸', 'ëŠê¸ˆë§ˆ', 'ëŠê¸ˆ', 'ë‹ˆì—„ë§ˆ', 'ë„ˆí¬ì—„ë§ˆ',\n",
    "            'í˜¸ë¡œ', 'ì°½ë…€', 'ê±¸ë ˆ', 'ì…ë…„', 'ì…', 'ìŒë…„', 'ìŒë†ˆ',\n",
    "            'ë¹¡ëŒ€ê°€ë¦¬', 'ë¹¡ì¢…', 'ë¹¡ì³', 'í™”ë‚˜', 'ê°œë¹¡', 'ì—´ë°›ì•„',\n",
    "            'íŒ¨ê³ ì‹¶ë‹¤', 'íŒ¨ë²„ë¦°ë‹¤', 'ë•Œë¦¬ê³ ì‹¶ë‹¤', 'ì£½ì´ê³ ì‹¶ë‹¤', 'ì¡°ì§€ê³ ì‹¶ë‹¤',\n",
    "            'ìƒˆë¼','ì¡´ë‚˜',\n",
    "            \n",
    "            # ì¤„ì„ë§/ì€ì–´ ìš•ì„¤\n",
    "            'ã…„', 'ã…‚ã……', 'ã……ã…‚', 'ã…†ã…‚', 'ã…ˆã„¹', 'ã…†ã„¹', 'ã…‚ã„±',\n",
    "            'ã……ã„²', 'ã„±ã……ã„²', 'ã„·ã…Š', 'ã…ã…Œã…Š', 'ã…—ã…œã…‘', 'ã…‚ã……ã„´',\n",
    "        }\n",
    "        \n",
    "        # ë¶ˆìš©ì–´ ì œê±° ë° ì¶”ê°€ í•„í„°ë§\n",
    "        filtered_words = []\n",
    "        for word in all_words:\n",
    "            # ë¶ˆìš©ì–´ ì œê±° (ìš•ì„¤ í¬í•¨)\n",
    "            if word.lower() in stop_words:\n",
    "                continue\n",
    "            \n",
    "            # ìˆ«ìë§Œ ìˆëŠ” ë‹¨ì–´ ì œê±°\n",
    "            if word.isdigit():\n",
    "                continue\n",
    "            \n",
    "            # ë°˜ë³µ ë¬¸ì ì œê±° (ã…‹ã…‹ã…‹, ã…ã…ã… ë“±)\n",
    "            if len(set(word)) == 1 and len(word) > 2:\n",
    "                continue\n",
    "            \n",
    "            # ì˜ì„±ì–´/ì˜íƒœì–´ íŒ¨í„´ ì œê±°\n",
    "            if re.match(r'^(.{1,2})\\1+$', word):\n",
    "                continue\n",
    "    \n",
    "            # íŠ¹ìˆ˜ íŒ¨í„´ ì œê±° (URL ì¡°ê°, ì´ë©”ì¼ ì¡°ê° ë“±)\n",
    "            if re.match(r'^(www|http|com|net|org)$', word):\n",
    "                continue\n",
    "            \n",
    "            # ìš•ì„¤ íŒ¨í„´ ì¶”ê°€ ê²€ì‚¬ (ì •ê·œì‹ ê¸°ë°˜)\n",
    "            if self._contains_profanity_pattern(word):\n",
    "                continue\n",
    "                \n",
    "            filtered_words.append(word)\n",
    "        \n",
    "        return filtered_words\n",
    "    \n",
    "    def analyze_text_frequency_advanced(self, top_n=30, use_filtered_data=True, use_morphology=True, save_to_csv=True):\n",
    "        \"\"\"\n",
    "        ê°œì„ ëœ í…ìŠ¤íŠ¸ ë¹ˆë„ ë¶„ì„ (CSV ì €ì¥ ì˜µì…˜)\n",
    "        \n",
    "        Args:\n",
    "            top_n (int): ìƒìœ„ Nê°œ ë‹¨ì–´\n",
    "            use_filtered_data (bool): ë„ë°° í•„í„°ë§ëœ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€\n",
    "            use_morphology (bool): í˜•íƒœì†Œ ë¶„ì„ ì‚¬ìš© ì—¬ë¶€\n",
    "            save_to_csv (bool): CSV íŒŒì¼ ì €ì¥ ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        # ì‚¬ìš©í•  ë°ì´í„° ì„ íƒ\n",
    "        if use_filtered_data and self.filtered_df is not None:\n",
    "            data_to_use = self.filtered_df\n",
    "            data_desc = \"ë„ë°° í•„í„°ë§ í›„\"\n",
    "        else:\n",
    "            data_to_use = self.processed_df\n",
    "            data_desc = \"ì „ì²´\"\n",
    "        \n",
    "        if data_to_use is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        # ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°\n",
    "        all_text = (data_to_use['ì œëª©'] + ' ' + data_to_use['ë‚´ìš©']).str.cat(sep=' ')\n",
    "        \n",
    "        # ê°œì„ ëœ ë‹¨ì–´ ì¶”ì¶œ\n",
    "        final_words = self._get_filtered_words_advanced(all_text, use_morphology)\n",
    "        \n",
    "        # ë¹ˆë„ ê³„ì‚°\n",
    "        word_freq = Counter(final_words)\n",
    "        top_words = word_freq.most_common(top_n)\n",
    "        \n",
    "        print(f\"=== {data_desc} ë°ì´í„° ë‹¨ì–´ ë¹ˆë„ TOP {top_n} ===\")\n",
    "        for i, (word, count) in enumerate(top_words, 1):\n",
    "            print(f\"{i:2d}. {word}: {count:,}íšŒ\")\n",
    "        \n",
    "        # CSV ì €ì¥\n",
    "        if save_to_csv and top_words:\n",
    "            import pandas as pd\n",
    "            word_freq_data = []\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                word_freq_data.append({\n",
    "                    'ìˆœìœ„': i,\n",
    "                    'ë‹¨ì–´': word,\n",
    "                    'ë¹ˆë„ìˆ˜': count,\n",
    "                    'ë°ì´í„°íƒ€ì…': data_desc\n",
    "                })\n",
    "            \n",
    "            word_freq_df = pd.DataFrame(word_freq_data)\n",
    "            filename = f'word_frequency_{data_desc}.csv'\n",
    "            filename = filename.replace(' ', '_').replace('(', '').replace(')', '')\n",
    "            word_freq_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\nâœ… ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return top_words\n",
    "    \n",
    "    def analyze_weekly_trends(self):\n",
    "        \"\"\"ì£¼ê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„ (ì „ì²´ ê¸°ê°„)\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        # ì£¼ê°„ë³„ í†µê³„ ê³„ì‚° (ì „ì²´ ê¸°ê°„)\n",
    "        weekly_stats = self.processed_df.groupby('ì£¼ì°¨').agg({\n",
    "            'ë²ˆí˜¸': 'count',\n",
    "            'ì¡°íšŒìˆ˜': ['sum', 'mean'],\n",
    "            'ëŒ“ê¸€ê°¯ìˆ˜': ['sum', 'mean']\n",
    "        }).round(2)\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… ì •ë¦¬\n",
    "        weekly_stats.columns = ['ê²Œì‹œê¸€ìˆ˜', 'ì´ì¡°íšŒìˆ˜', 'í‰ê· ì¡°íšŒìˆ˜', 'ì´ëŒ“ê¸€ìˆ˜', 'í‰ê· ëŒ“ê¸€ìˆ˜']\n",
    "        weekly_stats = weekly_stats.reset_index()\n",
    "        weekly_stats['ì£¼ì°¨_str'] = weekly_stats['ì£¼ì°¨'].astype(str)\n",
    "        \n",
    "        print(f\"=== ì£¼ê°„ë³„ í†µê³„ (ì „ì²´ {len(weekly_stats)}ì£¼) ===\")\n",
    "        print(f\"ì²« ì£¼ì°¨: {weekly_stats['ì£¼ì°¨_str'].iloc[0]}\")\n",
    "        print(f\"ë§ˆì§€ë§‰ ì£¼ì°¨: {weekly_stats['ì£¼ì°¨_str'].iloc[-1]}\")\n",
    "        print(f\"ì£¼ê°„ í‰ê·  ê²Œì‹œê¸€ìˆ˜: {weekly_stats['ê²Œì‹œê¸€ìˆ˜'].mean():.1f}ê°œ\")\n",
    "        print(f\"ì£¼ê°„ ìµœëŒ€ ê²Œì‹œê¸€ìˆ˜: {weekly_stats['ê²Œì‹œê¸€ìˆ˜'].max()}ê°œ\")\n",
    "        print(f\"ì£¼ê°„ í‰ê·  ì´ì¡°íšŒìˆ˜: {weekly_stats['ì´ì¡°íšŒìˆ˜'].mean():.0f}\")\n",
    "        \n",
    "        # CSV ì €ì¥\n",
    "        filename = 'weekly_trends_analysis.csv'\n",
    "        weekly_stats.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nâœ… ì£¼ê°„ë³„ ë¶„ì„ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return weekly_stats\n",
    "    \n",
    "    def analyze_monthly_word_frequency(self, top_n=20):\n",
    "        \"\"\"ì›”ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        monthly_word_freq = {}\n",
    "        all_monthly_data = []  # CSV ì €ì¥ìš© ë°ì´í„°\n",
    "        \n",
    "        for month in sorted(self.processed_df['ì—°ì›”'].unique()):\n",
    "            month_data = self.processed_df[self.processed_df['ì—°ì›”'] == month]\n",
    "            \n",
    "            # í•´ë‹¹ ì›”ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°\n",
    "            month_text = (month_data['ì œëª©'] + ' ' + month_data['ë‚´ìš©']).str.cat(sep=' ')\n",
    "            \n",
    "            # í•„í„°ë§ëœ ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸°\n",
    "            month_words = self._get_filtered_words_advanced(month_text, use_morphology=self.use_morphology)\n",
    "            \n",
    "            # ë¹ˆë„ ê³„ì‚°\n",
    "            word_freq = Counter(month_words)\n",
    "            top_words = word_freq.most_common(top_n)\n",
    "            monthly_word_freq[month] = top_words\n",
    "            \n",
    "            print(f\"=== {month} ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë‹¨ì–´ TOP {top_n} ===\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"{i:2d}. {word}: {count:,}íšŒ\")\n",
    "                # CSV ì €ì¥ìš© ë°ì´í„° ì¶”ê°€\n",
    "                all_monthly_data.append({\n",
    "                    'ì›”': str(month),\n",
    "                    'ê²Œì‹œê¸€ìˆ˜': len(month_data),\n",
    "                    'ìˆœìœ„': i,\n",
    "                    'ë‹¨ì–´': word,\n",
    "                    'ë¹ˆë„ìˆ˜': count\n",
    "                })\n",
    "            print()\n",
    "        \n",
    "        # CSV ì €ì¥\n",
    "        if all_monthly_data:\n",
    "            import pandas as pd\n",
    "            monthly_word_df = pd.DataFrame(all_monthly_data)\n",
    "            filename = 'monthly_word_frequency.csv'\n",
    "            monthly_word_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"âœ… ì›”ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return monthly_word_freq\n",
    "    \n",
    "    def analyze_weekly_word_frequency(self, top_n=20, save_to_csv=True):\n",
    "        \"\"\"ì£¼ê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ì „ì²´ ì£¼ì°¨)\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        weekly_word_freq = {}\n",
    "        all_weekly_data = []  # CSV ì €ì¥ìš© ë°ì´í„°\n",
    "        \n",
    "        # ì „ì²´ ì£¼ì°¨ ë¶„ì„\n",
    "        all_weeks = sorted(self.processed_df['ì£¼ì°¨'].unique())\n",
    "        \n",
    "        print(f\"=== ì£¼ê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ì „ì²´ {len(all_weeks)}ì£¼) ===\")\n",
    "        print(f\"ë¶„ì„ ê¸°ê°„: {all_weeks[0]} ~ {all_weeks[-1]}\")\n",
    "        print()\n",
    "        \n",
    "        for week in all_weeks:\n",
    "            week_data = self.processed_df[self.processed_df['ì£¼ì°¨'] == week]\n",
    "            \n",
    "            if len(week_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            # í•´ë‹¹ ì£¼ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°\n",
    "            week_text = (week_data['ì œëª©'] + ' ' + week_data['ë‚´ìš©']).str.cat(sep=' ')\n",
    "            \n",
    "            # í•„í„°ë§ëœ ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸°\n",
    "            week_words = self._get_filtered_words_advanced(week_text, use_morphology=self.use_morphology)\n",
    "            \n",
    "            # ë¹ˆë„ ê³„ì‚°\n",
    "            word_freq = Counter(week_words)\n",
    "            top_words = word_freq.most_common(top_n)\n",
    "            weekly_word_freq[week] = top_words\n",
    "            \n",
    "            print(f\"=== {week} (ê²Œì‹œê¸€ {len(week_data)}ê°œ) - TOP {top_n} ===\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"{i:2d}. {word}: {count:,}íšŒ\")\n",
    "            \n",
    "            # CSV ì €ì¥ìš© ë°ì´í„° ì¶”ê°€\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                all_weekly_data.append({\n",
    "                    'ì£¼ì°¨': str(week),\n",
    "                    'ê²Œì‹œê¸€ìˆ˜': len(week_data),\n",
    "                    'ìˆœìœ„': i,\n",
    "                    'ë‹¨ì–´': word,\n",
    "                    'ë¹ˆë„ìˆ˜': count\n",
    "                })\n",
    "            print()\n",
    "        \n",
    "        # CSV ì €ì¥\n",
    "        if save_to_csv and all_weekly_data:\n",
    "            import pandas as pd\n",
    "            weekly_word_df = pd.DataFrame(all_weekly_data)\n",
    "            filename = 'weekly_word_frequency_all.csv'\n",
    "            weekly_word_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"âœ… ì£¼ê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return weekly_word_freq\n",
    "    \n",
    "    def plot_yearly_trends(self, yearly_stats):\n",
    "        \"\"\"ì—°ë„ë³„ íŠ¸ë Œë“œ ì°¨íŠ¸ ê·¸ë¦¬ê¸°\"\"\"\n",
    "        if yearly_stats is None:\n",
    "            print(\"ì—°ë„ë³„ í†µê³„ë¥¼ ë¨¼ì € ê³„ì‚°í•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        \n",
    "        # ì—°ë„ë³„ ê²Œì‹œê¸€ ìˆ˜\n",
    "        ax1.bar(yearly_stats['ì—°ë„'], yearly_stats['ê²Œì‹œê¸€ìˆ˜'], \n",
    "                color='#58D68D', alpha=0.8, width=0.6)\n",
    "        ax1.set_title('ì—°ë„ë³„ ê²Œì‹œê¸€ ìˆ˜', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('ì—°ë„', fontsize=12)\n",
    "        ax1.set_ylabel('ê²Œì‹œê¸€ ìˆ˜', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # ê°’ í‘œì‹œ\n",
    "        for i, v in enumerate(yearly_stats['ê²Œì‹œê¸€ìˆ˜']):\n",
    "            ax1.annotate(f'{v:,.0f}', (yearly_stats['ì—°ë„'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # ì—°ë„ë³„ ì´ ì¡°íšŒìˆ˜\n",
    "        ax2.plot(yearly_stats['ì—°ë„'], yearly_stats['ì´ì¡°íšŒìˆ˜'], \n",
    "                marker='o', linewidth=3, markersize=8, color='#2E86C1')\n",
    "        ax2.set_title('ì—°ë„ë³„ ì´ ì¡°íšŒìˆ˜', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('ì—°ë„', fontsize=12)\n",
    "        ax2.set_ylabel('ì´ ì¡°íšŒìˆ˜', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ê°’ í‘œì‹œ\n",
    "        for i, v in enumerate(yearly_stats['ì´ì¡°íšŒìˆ˜']):\n",
    "            ax2.annotate(f'{v:,.0f}', (yearly_stats['ì—°ë„'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # ì—°ë„ë³„ í‰ê·  ì¡°íšŒìˆ˜\n",
    "        ax3.plot(yearly_stats['ì—°ë„'], yearly_stats['í‰ê· ì¡°íšŒìˆ˜'], \n",
    "                marker='s', linewidth=3, markersize=8, color='#E74C3C')\n",
    "        ax3.set_title('ì—°ë„ë³„ í‰ê·  ì¡°íšŒìˆ˜', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('ì—°ë„', fontsize=12)\n",
    "        ax3.set_ylabel('í‰ê·  ì¡°íšŒìˆ˜', fontsize=12)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ê°’ í‘œì‹œ\n",
    "        for i, v in enumerate(yearly_stats['í‰ê· ì¡°íšŒìˆ˜']):\n",
    "            ax3.annotate(f'{v:.1f}', (yearly_stats['ì—°ë„'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # ì—°ë„ë³„ ëŒ“ê¸€ ìˆ˜\n",
    "        ax4.bar(yearly_stats['ì—°ë„'], yearly_stats['ì´ëŒ“ê¸€ìˆ˜'], \n",
    "                color='#F39C12', alpha=0.8, width=0.6)\n",
    "        ax4.set_title('ì—°ë„ë³„ ì´ ëŒ“ê¸€ ìˆ˜', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('ì—°ë„', fontsize=12)\n",
    "        ax4.set_ylabel('ì´ ëŒ“ê¸€ ìˆ˜', fontsize=12)\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # ê°’ í‘œì‹œ\n",
    "        for i, v in enumerate(yearly_stats['ì´ëŒ“ê¸€ìˆ˜']):\n",
    "            ax4.annotate(f'{v:,.0f}', (yearly_stats['ì—°ë„'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_monthly_views(self, monthly_stats):\n",
    "        \"\"\"ì›”ë³„ ì¡°íšŒìˆ˜ ì°¨íŠ¸ ê·¸ë¦¬ê¸° - ê°œì„ ëœ ë²„ì „\"\"\"\n",
    "        if monthly_stats is None:\n",
    "            print(\"ì›”ë³„ í†µê³„ë¥¼ ë¨¼ì € ê³„ì‚°í•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "        \n",
    "        # ê·¸ë˜í”„ í¬ê¸°ë¥¼ ë” í¬ê²Œ ì„¤ì •\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12))\n",
    "        \n",
    "        # ì´ ì¡°íšŒìˆ˜ ì¶”ì´\n",
    "        ax1.plot(monthly_stats['ì—°ì›”_str'], monthly_stats['ì´ì¡°íšŒìˆ˜'], \n",
    "                marker='o', linewidth=3, markersize=8, color='#2E86C1')\n",
    "        ax1.set_title('ì›”ë³„ ì´ ì¡°íšŒìˆ˜ ì¶”ì´', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax1.set_xlabel('ì›”', fontsize=12)\n",
    "        ax1.set_ylabel('ì´ ì¡°íšŒìˆ˜', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Xì¶• ë¼ë²¨ íšŒì „ê° ì¡°ì •í•˜ê³  ê°„ê²© ëŠ˜ë¦¬ê¸°\n",
    "        ax1.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax1.tick_params(axis='y', labelsize=10)\n",
    "        \n",
    "        # í‰ê·  ì¡°íšŒìˆ˜ ì¶”ì´\n",
    "        ax2.plot(monthly_stats['ì—°ì›”_str'], monthly_stats['í‰ê· ì¡°íšŒìˆ˜'], \n",
    "                marker='s', linewidth=3, markersize=8, color='#E74C3C')\n",
    "        ax2.set_title('ì›”ë³„ í‰ê·  ì¡°íšŒìˆ˜ ì¶”ì´', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax2.set_xlabel('ì›”', fontsize=12)\n",
    "        ax2.set_ylabel('í‰ê·  ì¡°íšŒìˆ˜', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Xì¶• ë¼ë²¨ íšŒì „ê° ì¡°ì •\n",
    "        ax2.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax2.tick_params(axis='y', labelsize=10)\n",
    "        \n",
    "        # ì—¬ë°± ì¡°ì •\n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "        \n",
    "        # ê²Œì‹œê¸€ ìˆ˜ ì°¨íŠ¸ - ë” í° í¬ê¸°ë¡œ\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        bars = plt.bar(monthly_stats['ì—°ì›”_str'], monthly_stats['ê²Œì‹œê¸€ìˆ˜'], \n",
    "                      color='#58D68D', alpha=0.8, width=0.6)\n",
    "        plt.title('ì›”ë³„ ê²Œì‹œê¸€ ìˆ˜', fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel('ì›”', fontsize=12)\n",
    "        plt.ylabel('ê²Œì‹œê¸€ ìˆ˜', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # ê°’ í‘œì‹œ - ë§‰ëŒ€ ìœ„ì— í‘œì‹œ\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.annotate(f'{height:,.0f}', \n",
    "                        xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                        xytext=(0, 8), \n",
    "                        textcoords=\"offset points\", \n",
    "                        ha='center', \n",
    "                        va='bottom',\n",
    "                        fontsize=11)\n",
    "        \n",
    "        # Xì¶• ë¼ë²¨ ì¡°ì •\n",
    "        plt.xticks(rotation=45, fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_weekly_trends(self, weekly_stats):\n",
    "        \"\"\"ì£¼ê°„ë³„ íŠ¸ë Œë“œ ì°¨íŠ¸ ê·¸ë¦¬ê¸° (ì „ì²´ ê¸°ê°„)\"\"\"\n",
    "        if weekly_stats is None:\n",
    "            print(\"ì£¼ê°„ë³„ í†µê³„ë¥¼ ë¨¼ì € ê³„ì‚°í•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "        \n",
    "        # ì „ì²´ ì£¼ì°¨ê°€ ë§ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ xì¶• ë¼ë²¨ ê°„ê²© ì¡°ì •\n",
    "        total_weeks = len(weekly_stats)\n",
    "        label_step = max(1, total_weeks // 20)  # ìµœëŒ€ 20ê°œ ë¼ë²¨ë§Œ í‘œì‹œ\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20, 15))\n",
    "        \n",
    "        # ì£¼ê°„ë³„ ê²Œì‹œê¸€ ìˆ˜\n",
    "        ax1.plot(range(len(weekly_stats)), weekly_stats['ê²Œì‹œê¸€ìˆ˜'], \n",
    "                linewidth=1, color='#58D68D', alpha=0.8)\n",
    "        ax1.fill_between(range(len(weekly_stats)), weekly_stats['ê²Œì‹œê¸€ìˆ˜'], \n",
    "                        alpha=0.3, color='#58D68D')\n",
    "        ax1.set_title(f'ì£¼ê°„ë³„ ê²Œì‹œê¸€ ìˆ˜ (ì „ì²´ {total_weeks}ì£¼)', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('ì£¼ì°¨', fontsize=12)\n",
    "        ax1.set_ylabel('ê²Œì‹œê¸€ ìˆ˜', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # xì¶• ë¼ë²¨ ê°„ê²© ì¡°ì •\n",
    "        xtick_positions = range(0, len(weekly_stats), label_step)\n",
    "        xtick_labels = [weekly_stats['ì£¼ì°¨_str'].iloc[i] for i in xtick_positions]\n",
    "        ax1.set_xticks(xtick_positions)\n",
    "        ax1.set_xticklabels(xtick_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        # ì£¼ê°„ë³„ ì´ ì¡°íšŒìˆ˜\n",
    "        ax2.plot(range(len(weekly_stats)), weekly_stats['ì´ì¡°íšŒìˆ˜'], \n",
    "                linewidth=1, color='#2E86C1', alpha=0.8)\n",
    "        ax2.fill_between(range(len(weekly_stats)), weekly_stats['ì´ì¡°íšŒìˆ˜'], \n",
    "                        alpha=0.3, color='#2E86C1')\n",
    "        ax2.set_title(f'ì£¼ê°„ë³„ ì´ ì¡°íšŒìˆ˜ (ì „ì²´ {total_weeks}ì£¼)', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('ì£¼ì°¨', fontsize=12)\n",
    "        ax2.set_ylabel('ì´ ì¡°íšŒìˆ˜', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xticks(xtick_positions)\n",
    "        ax2.set_xticklabels(xtick_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        # ì£¼ê°„ë³„ í‰ê·  ì¡°íšŒìˆ˜\n",
    "        ax3.plot(range(len(weekly_stats)), weekly_stats['í‰ê· ì¡°íšŒìˆ˜'], \n",
    "                linewidth=1, color='#E74C3C', alpha=0.8)\n",
    "        ax3.fill_between(range(len(weekly_stats)), weekly_stats['í‰ê· ì¡°íšŒìˆ˜'], \n",
    "                        alpha=0.3, color='#E74C3C')\n",
    "        ax3.set_title(f'ì£¼ê°„ë³„ í‰ê·  ì¡°íšŒìˆ˜ (ì „ì²´ {total_weeks}ì£¼)', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('ì£¼ì°¨', fontsize=12)\n",
    "        ax3.set_ylabel('í‰ê·  ì¡°íšŒìˆ˜', fontsize=12)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_xticks(xtick_positions)\n",
    "        ax3.set_xticklabels(xtick_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "        \n",
    "        # ì£¼ê°„ë³„ í†µê³„ ìš”ì•½ ì¶œë ¥\n",
    "        print(f\"\\n=== ì£¼ê°„ë³„ ë°ì´í„° ìš”ì•½ ===\")\n",
    "        print(f\"ì´ ì£¼ì°¨ ìˆ˜: {total_weeks}ì£¼\")\n",
    "        print(f\"ì£¼ê°„ ê²Œì‹œê¸€ ìˆ˜ - í‰ê· : {weekly_stats['ê²Œì‹œê¸€ìˆ˜'].mean():.1f}, ìµœì†Œ: {weekly_stats['ê²Œì‹œê¸€ìˆ˜'].min()}, ìµœëŒ€: {weekly_stats['ê²Œì‹œê¸€ìˆ˜'].max()}\")\n",
    "        print(f\"ì£¼ê°„ ì´ì¡°íšŒìˆ˜ - í‰ê· : {weekly_stats['ì´ì¡°íšŒìˆ˜'].mean():.0f}, ìµœì†Œ: {weekly_stats['ì´ì¡°íšŒìˆ˜'].min():.0f}, ìµœëŒ€: {weekly_stats['ì´ì¡°íšŒìˆ˜'].max():.0f}\")\n",
    "        print(f\"ì£¼ê°„ í‰ê· ì¡°íšŒìˆ˜ - í‰ê· : {weekly_stats['í‰ê· ì¡°íšŒìˆ˜'].mean():.1f}, ìµœì†Œ: {weekly_stats['í‰ê· ì¡°íšŒìˆ˜'].min():.1f}, ìµœëŒ€: {weekly_stats['í‰ê· ì¡°íšŒìˆ˜'].max():.1f}\")\n",
    "    \n",
    "    def plot_word_frequency(self, top_words, data_desc=\"ì „ì²´\", use_morphology=True):\n",
    "        \"\"\"ë‹¨ì–´ ë¹ˆë„ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\"\"\"\n",
    "        if not top_words:\n",
    "            print(\"ë‹¨ì–´ ë¹ˆë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        # ìƒìœ„ 25ê°œ ë‹¨ì–´ë§Œ í‘œì‹œ (ê·¸ë˜í”„ê°€ ë„ˆë¬´ ë³µì¡í•´ì§€ì§€ ì•Šë„ë¡)\n",
    "        display_words = top_words[:25]\n",
    "        words, counts = zip(*display_words)\n",
    "        \n",
    "        # ê·¸ë˜í”„ í¬ê¸° ì„¤ì •\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„\n",
    "        y_pos = np.arange(len(words))\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(words)))\n",
    "        \n",
    "        bars = plt.barh(y_pos, counts, color=colors, alpha=0.8, height=0.7)\n",
    "        \n",
    "        # ê·¸ë˜í”„ ì„¤ì •\n",
    "        analysis_method = \"í˜•íƒœì†Œ ë¶„ì„\" if use_morphology else \"ì •ê·œì‹\"\n",
    "        plt.title(f'{data_desc} ë°ì´í„° ë‹¨ì–´ ë¹ˆë„ TOP {len(display_words)} ({analysis_method})', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel('ë¹ˆë„ìˆ˜', fontsize=14)\n",
    "        plt.ylabel('ë‹¨ì–´', fontsize=14)\n",
    "        \n",
    "        # Yì¶• ë ˆì´ë¸” ì„¤ì •\n",
    "        plt.yticks(y_pos, words, fontsize=12)\n",
    "        plt.gca().invert_yaxis()  # ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ê°€ ìœ„ì— ì˜¤ë„ë¡\n",
    "        \n",
    "        # ê²©ì í‘œì‹œ\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # ê°’ í‘œì‹œ\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            plt.annotate(f'{width:,}', \n",
    "                        xy=(width, bar.get_y() + bar.get_height()/2),\n",
    "                        xytext=(5, 0), \n",
    "                        textcoords=\"offset points\", \n",
    "                        ha='left', \n",
    "                        va='center', \n",
    "                        fontsize=11,\n",
    "                        fontweight='bold')\n",
    "        \n",
    "        # ë ˆì´ì•„ì›ƒ ì¡°ì •\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        \n",
    "        # í†µê³„ ì •ë³´ í‘œì‹œ\n",
    "        total_unique_words = len(set([word for word, count in top_words]))\n",
    "        total_word_count = sum([count for word, count in top_words])\n",
    "        \n",
    "        plt.figtext(0.02, 0.02, \n",
    "                   f'ì´ ê³ ìœ  ë‹¨ì–´ ìˆ˜: {total_unique_words:,}ê°œ | ì´ ë‹¨ì–´ ì¶œí˜„ íšŸìˆ˜: {total_word_count:,}íšŒ', \n",
    "                   fontsize=10, ha='left')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥\n",
    "        print(f\"\\n=== ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ìš”ì•½ ({analysis_method}) ===\")\n",
    "        print(f\"ì´ ê³ ìœ  ë‹¨ì–´ ìˆ˜: {total_unique_words:,}ê°œ\")\n",
    "        print(f\"ì´ ë‹¨ì–´ ì¶œí˜„ íšŸìˆ˜: {total_word_count:,}íšŒ\")\n",
    "        print(f\"ê°€ì¥ ë¹ˆë²ˆí•œ ë‹¨ì–´: '{words[0]}' ({counts[0]:,}íšŒ)\")\n",
    "        print(f\"í‰ê·  ë¹ˆë„: {total_word_count/total_unique_words:.1f}íšŒ\")\n",
    "        \"\"\"ì—°ë„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (CSV ì €ì¥ ì˜µì…˜)\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        yearly_word_freq = {}\n",
    "        all_yearly_data = []  # CSV ì €ì¥ìš© ë°ì´í„°\n",
    "        \n",
    "        for year in sorted(self.processed_df['ì—°ë„'].unique()):\n",
    "            year_data = self.processed_df[self.processed_df['ì—°ë„'] == year]\n",
    "            \n",
    "            # í•´ë‹¹ ì—°ë„ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°\n",
    "            year_text = (year_data['ì œëª©'] + ' ' + year_data['ë‚´ìš©']).str.cat(sep=' ')\n",
    "            \n",
    "            # í•„í„°ë§ëœ ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸°\n",
    "            year_words = self._get_filtered_words_advanced(year_text, use_morphology=self.use_morphology)\n",
    "            \n",
    "            # ë¹ˆë„ ê³„ì‚°\n",
    "            word_freq = Counter(year_words)\n",
    "            top_words = word_freq.most_common(top_n)\n",
    "            yearly_word_freq[year] = top_words\n",
    "            \n",
    "            print(f\"=== {year}ë…„ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë‹¨ì–´ TOP {top_n} ===\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"{i:2d}. {word}: {count:,}íšŒ\")\n",
    "                # CSV ì €ì¥ìš© ë°ì´í„° ì¶”ê°€\n",
    "                all_yearly_data.append({\n",
    "                    'ì—°ë„': year,\n",
    "                    'ìˆœìœ„': i,\n",
    "                    'ë‹¨ì–´': word,\n",
    "                    'ë¹ˆë„ìˆ˜': count\n",
    "                })\n",
    "            print()\n",
    "        \n",
    "        # CSV ì €ì¥\n",
    "        if save_to_csv and all_yearly_data:\n",
    "            import pandas as pd\n",
    "            yearly_word_df = pd.DataFrame(all_yearly_data)\n",
    "            filename = 'yearly_word_frequency.csv'\n",
    "            yearly_word_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"âœ… ì—°ë„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return yearly_word_freq\n",
    "\n",
    "\n",
    "def main_menu():\n",
    "    \"\"\"ë©”ì¸ ë©”ë‰´ í‘œì‹œ ë° ì‚¬ìš©ì ì„ íƒ ì²˜ë¦¬\"\"\"\n",
    "    # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "    csv_file_path = 'community/ChartAnalysis.csv'  # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”\n",
    "    analyzer = AdvancedCommunityDataAnalyzer(csv_file_path)\n",
    "    \n",
    "    # ìë™ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ìˆ˜í–‰\n",
    "    print(\"ğŸ”„ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    if not analyzer.load_data():\n",
    "        print(\"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    if not analyzer.preprocess_data():\n",
    "        print(\"âŒ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        return\n",
    "    \n",
    "    print(\"âœ… ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ìë™ìœ¼ë¡œ ì¤‘ë³µ í•„í„°ë§ ìˆ˜í–‰\n",
    "    print(\"\\nğŸ§¹ ì¤‘ë³µ ê²Œì‹œê¸€ í•„í„°ë§ ì‹œì‘...\")\n",
    "    spam_indices = analyzer.detect_spam_posts_exact_only(use_hashing=True)\n",
    "    \n",
    "    # í•„í„°ë§ëœ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥\n",
    "    if analyzer.filtered_df is not None:\n",
    "        output_filename = 'filtered_community_data.csv'\n",
    "        analyzer.filtered_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "        print(f\"âœ… í•„í„°ë§ëœ ë°ì´í„°ê°€ '{output_filename}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸš€ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ë¶„ì„ê¸° - ë¶„ì„ ë©”ë‰´\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"ã€ì‹œê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„ã€‘\")\n",
    "        print(\"1ï¸âƒ£  ì—°ë„ë³„ íŠ¸ë Œë“œ ë¶„ì„ (ê²Œì‹œê¸€ìˆ˜, ì¡°íšŒìˆ˜, ëŒ“ê¸€ìˆ˜)\")\n",
    "        print(\"2ï¸âƒ£  ì›”ë³„ íŠ¸ë Œë“œ ë¶„ì„ (ê²Œì‹œê¸€ìˆ˜, ì¡°íšŒìˆ˜, ëŒ“ê¸€ìˆ˜)\")\n",
    "        print(\"3ï¸âƒ£  ì£¼ê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„ (ê²Œì‹œê¸€ìˆ˜, ì¡°íšŒìˆ˜, ëŒ“ê¸€ìˆ˜)\")\n",
    "        print()\n",
    "        print(\"ã€ì‹œê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ã€‘\")\n",
    "        print(\"4ï¸âƒ£  ì—°ë„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\")\n",
    "        print(\"5ï¸âƒ£  ì›”ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\")\n",
    "        print(\"6ï¸âƒ£  ì£¼ê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ì „ì²´ ì£¼ì°¨)\")\n",
    "        print()\n",
    "        print(\"ã€ì „ì²´ ë°ì´í„° ë¶„ì„ã€‘\")\n",
    "        print(\"7ï¸âƒ£  ì „ì²´ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\")\n",
    "        print(\"8ï¸âƒ£  ì •ê·œì‹ ê¸°ë°˜ ë‹¨ì–´ ë¹ˆë„ (í˜•íƒœì†Œ ë¶„ì„ ì—†ìŒ)\")\n",
    "        print()\n",
    "        print(\"ã€ê·¸ë˜í”„ ì‹œê°í™”ã€‘\")\n",
    "        print(\"11  ì—°ë„ë³„ íŠ¸ë Œë“œ ê·¸ë˜í”„\")\n",
    "        print(\"12  ì›”ë³„ íŠ¸ë Œë“œ ê·¸ë˜í”„\")\n",
    "        print(\"13  ì£¼ê°„ë³„ íŠ¸ë Œë“œ ê·¸ë˜í”„\")\n",
    "        print(\"14  ì „ì²´ ë‹¨ì–´ ë¹ˆë„ ê·¸ë˜í”„\")\n",
    "        print()\n",
    "        print(\"ã€ê²°ê³¼ ì €ì¥ ë° ë¦¬í¬íŠ¸ã€‘\")\n",
    "        print(\"9ï¸âƒ£  ëª¨ë“  ë¶„ì„ ê²°ê³¼ Excel ì €ì¥\")\n",
    "        print(\"ğŸ”Ÿ  ì¢…í•© ë¦¬í¬íŠ¸ ì¶œë ¥\")\n",
    "        print(\"0ï¸âƒ£  í”„ë¡œê·¸ë¨ ì¢…ë£Œ\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            choice = input(\"\\nğŸ“Œ ì›í•˜ëŠ” ë¶„ì„ì˜ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (0-14): \").strip()\n",
    "            \n",
    "            if choice == '0':\n",
    "                print(\"ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "                break\n",
    "                \n",
    "            elif choice == '1':\n",
    "                print(\"\\nğŸ“… ì—°ë„ë³„ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...\")\n",
    "                yearly_stats = analyzer.analyze_yearly_trends(save_to_csv=True)\n",
    "                \n",
    "            elif choice == '2':\n",
    "                print(\"\\nğŸ“Š ì›”ë³„ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...\")\n",
    "                monthly_stats = analyzer.analyze_monthly_trends()\n",
    "                \n",
    "            elif choice == '3':\n",
    "                print(\"\\nğŸ“ˆ ì£¼ê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘...\")\n",
    "                weekly_stats = analyzer.analyze_weekly_trends()\n",
    "                \n",
    "            elif choice == '4':\n",
    "                print(\"\\nğŸ“… ì—°ë„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì‹œì‘...\")\n",
    "                if not analyzer.use_morphology:\n",
    "                    print(\"âš ï¸ í˜•íƒœì†Œ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ì„œ ì •ê·œì‹ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.\")\n",
    "                else:\n",
    "                    print(\"ğŸ”¤ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.\")\n",
    "                yearly_word_freq = analyzer.analyze_yearly_word_frequency(top_n=20, save_to_csv=True)\n",
    "                \n",
    "            elif choice == '5':\n",
    "                print(\"\\nğŸ“Š ì›”ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì‹œì‘...\")\n",
    "                if not analyzer.use_morphology:\n",
    "                    print(\"âš ï¸ í˜•íƒœì†Œ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ì„œ ì •ê·œì‹ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.\")\n",
    "                else:\n",
    "                    print(\"ğŸ”¤ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.\")\n",
    "                monthly_word_freq = analyzer.analyze_monthly_word_frequency(top_n=20)\n",
    "                \n",
    "            elif choice == '6':\n",
    "                print(\"\\nğŸ“ˆ ì£¼ê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì‹œì‘...\")\n",
    "                if not analyzer.use_morphology:\n",
    "                    print(\"âš ï¸ í˜•íƒœì†Œ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ì„œ ì •ê·œì‹ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.\")\n",
    "                else:\n",
    "                    print(\"ğŸ”¤ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.\")\n",
    "                weekly_word_freq = analyzer.analyze_weekly_word_frequency(top_n=15, save_to_csv=True)\n",
    "                \n",
    "            elif choice == '7':\n",
    "                print(\"\\nğŸ”¤ ì „ì²´ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì‹œì‘...\")\n",
    "                \n",
    "                if not analyzer.use_morphology:\n",
    "                    print(\"âš ï¸ í˜•íƒœì†Œ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ì„œ ì •ê·œì‹ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.\")\n",
    "                else:\n",
    "                    print(\"ğŸ”¤ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.\")\n",
    "                \n",
    "                # í•„í„°ë§ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©\n",
    "                use_filtered = analyzer.filtered_df is not None\n",
    "                if use_filtered:\n",
    "                    print(\"ğŸ§¹ í•„í„°ë§ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "                else:\n",
    "                    print(\"ğŸ“Š ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "                    \n",
    "                word_freq = analyzer.analyze_text_frequency_advanced(\n",
    "                    top_n=30, \n",
    "                    use_filtered_data=use_filtered, \n",
    "                    use_morphology=analyzer.use_morphology,  # í˜•íƒœì†Œ ë¶„ì„ ìš°ì„  ì‚¬ìš©\n",
    "                    save_to_csv=True\n",
    "                )\n",
    "                \n",
    "            elif choice == '8':\n",
    "                print(\"\\nğŸ“ ì •ê·œì‹ ê¸°ë°˜ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ì‹œì‘...\")\n",
    "                print(\"ğŸ” ì •ê·œì‹ìœ¼ë¡œë§Œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (í˜•íƒœì†Œ ë¶„ì„ ì—†ìŒ)\")\n",
    "                \n",
    "                # í•„í„°ë§ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©\n",
    "                use_filtered = analyzer.filtered_df is not None\n",
    "                if use_filtered:\n",
    "                    print(\"ğŸ§¹ í•„í„°ë§ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "                else:\n",
    "                    print(\"ğŸ“Š ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "                    \n",
    "                word_freq = analyzer.analyze_text_frequency_advanced(\n",
    "                    top_n=30, \n",
    "                    use_filtered_data=use_filtered, \n",
    "                    use_morphology=False,  # ì •ê·œì‹ë§Œ ì‚¬ìš©\n",
    "                    save_to_csv=True\n",
    "                )\n",
    "                \n",
    "            elif choice == '9':\n",
    "                print(\"\\nğŸ’¾ ëª¨ë“  ë¶„ì„ ê²°ê³¼ Excel ì €ì¥ ì‹œì‘...\")\n",
    "                analyzer.save_all_analysis_to_excel()\n",
    "                \n",
    "            elif choice == '10':\n",
    "                print(\"\\nğŸ“‹ ì¢…í•© ë¦¬í¬íŠ¸ ì¶œë ¥...\")\n",
    "                analyzer.generate_advanced_summary_report()\n",
    "                \n",
    "            elif choice == '11':\n",
    "                print(\"\\nğŸ“Š ì—°ë„ë³„ íŠ¸ë Œë“œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°...\")\n",
    "                # ì—°ë„ë³„ í†µê³„ ë‹¤ì‹œ ê³„ì‚°\n",
    "                yearly_stats = analyzer.processed_df.groupby('ì—°ë„').agg({\n",
    "                    'ë²ˆí˜¸': 'count',\n",
    "                    'ì¡°íšŒìˆ˜': ['sum', 'mean', 'max'],\n",
    "                    'ëŒ“ê¸€ê°¯ìˆ˜': ['sum', 'mean']\n",
    "                }).round(2)\n",
    "                yearly_stats.columns = ['ê²Œì‹œê¸€ìˆ˜', 'ì´ì¡°íšŒìˆ˜', 'í‰ê· ì¡°íšŒìˆ˜', 'ìµœëŒ€ì¡°íšŒìˆ˜', 'ì´ëŒ“ê¸€ìˆ˜', 'í‰ê· ëŒ“ê¸€ìˆ˜']\n",
    "                yearly_stats = yearly_stats.reset_index()\n",
    "                analyzer.plot_yearly_trends(yearly_stats)\n",
    "                \n",
    "            elif choice == '12':\n",
    "                print(\"\\nğŸ“Š ì›”ë³„ íŠ¸ë Œë“œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°...\")\n",
    "                # ì›”ë³„ í†µê³„ ë‹¤ì‹œ ê³„ì‚°\n",
    "                monthly_stats = analyzer.processed_df.groupby('ì—°ì›”').agg({\n",
    "                    'ë²ˆí˜¸': 'count',\n",
    "                    'ì¡°íšŒìˆ˜': ['sum', 'mean', 'max'],\n",
    "                    'ëŒ“ê¸€ê°¯ìˆ˜': ['sum', 'mean']\n",
    "                }).round(2)\n",
    "                monthly_stats.columns = ['ê²Œì‹œê¸€ìˆ˜', 'ì´ì¡°íšŒìˆ˜', 'í‰ê· ì¡°íšŒìˆ˜', 'ìµœëŒ€ì¡°íšŒìˆ˜', 'ì´ëŒ“ê¸€ìˆ˜', 'í‰ê· ëŒ“ê¸€ìˆ˜']\n",
    "                monthly_stats = monthly_stats.reset_index()\n",
    "                monthly_stats['ì—°ì›”_str'] = monthly_stats['ì—°ì›”'].astype(str)\n",
    "                analyzer.plot_monthly_views(monthly_stats)\n",
    "                \n",
    "            elif choice == '13':\n",
    "                print(\"\\nğŸ“Š ì£¼ê°„ë³„ íŠ¸ë Œë“œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°...\")\n",
    "                # ì£¼ê°„ë³„ í†µê³„ ë‹¤ì‹œ ê³„ì‚°\n",
    "                weekly_stats = analyzer.processed_df.groupby('ì£¼ì°¨').agg({\n",
    "                    'ë²ˆí˜¸': 'count',\n",
    "                    'ì¡°íšŒìˆ˜': ['sum', 'mean'],\n",
    "                    'ëŒ“ê¸€ê°¯ìˆ˜': ['sum', 'mean']\n",
    "                }).round(2)\n",
    "                weekly_stats.columns = ['ê²Œì‹œê¸€ìˆ˜', 'ì´ì¡°íšŒìˆ˜', 'í‰ê· ì¡°íšŒìˆ˜', 'ì´ëŒ“ê¸€ìˆ˜', 'í‰ê· ëŒ“ê¸€ìˆ˜']\n",
    "                weekly_stats = weekly_stats.reset_index()\n",
    "                weekly_stats['ì£¼ì°¨_str'] = weekly_stats['ì£¼ì°¨'].astype(str)\n",
    "                analyzer.plot_weekly_trends(weekly_stats)\n",
    "                \n",
    "            elif choice == '14':\n",
    "                print(\"\\nğŸ“Š ì „ì²´ ë‹¨ì–´ ë¹ˆë„ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°...\")\n",
    "                \n",
    "                # í•„í„°ë§ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì „ì²´ ë°ì´í„° ì‚¬ìš©\n",
    "                use_filtered = analyzer.filtered_df is not None\n",
    "                data_to_use = analyzer.filtered_df if use_filtered else analyzer.processed_df\n",
    "                data_desc = \"í•„í„°ë§ í›„\" if use_filtered else \"ì „ì²´\"\n",
    "                \n",
    "                if not analyzer.use_morphology:\n",
    "                    print(\"âš ï¸ í˜•íƒœì†Œ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ì„œ ì •ê·œì‹ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.\")\n",
    "                    use_morphology = False\n",
    "                else:\n",
    "                    print(\"ğŸ”¤ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.\")\n",
    "                    use_morphology = True\n",
    "                \n",
    "                print(f\"ğŸ“Š {data_desc} ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "                \n",
    "                # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°\n",
    "                all_text = (data_to_use['ì œëª©'] + ' ' + data_to_use['ë‚´ìš©']).str.cat(sep=' ')\n",
    "                final_words = analyzer._get_filtered_words_advanced(all_text, use_morphology)\n",
    "                word_freq = Counter(final_words)\n",
    "                top_words = word_freq.most_common(30)\n",
    "                \n",
    "                analyzer.plot_word_frequency(top_words, data_desc, use_morphology)\n",
    "                \n",
    "            else:\n",
    "                print(\"âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 0-14 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nğŸ‘‹ ì‚¬ìš©ìê°€ í”„ë¡œê·¸ë¨ì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "\n",
    "\n",
    "def save_all_analysis_to_excel(self):\n",
    "    \"\"\"ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ Excel íŒŒì¼ì— ì €ì¥\"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        from openpyxl import Workbook\n",
    "        from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "        from openpyxl.styles import Font, PatternFill, Alignment\n",
    "        \n",
    "        print(\"ğŸ“Š ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ Excel íŒŒì¼ë¡œ ì €ì¥ ì¤‘...\")\n",
    "        \n",
    "        # Excel íŒŒì¼ ìƒì„±\n",
    "        wb = Workbook()\n",
    "        wb.remove(wb.active)  # ê¸°ë³¸ ì‹œíŠ¸ ì œê±°\n",
    "        \n",
    "        # 1. ì—°ë„ë³„ íŠ¸ë Œë“œ ë¶„ì„\n",
    "        if self.processed_df is not None:\n",
    "            yearly_stats = self.processed_df.groupby('ì—°ë„').agg({\n",
    "                'ë²ˆí˜¸': 'count',\n",
    "                'ì¡°íšŒìˆ˜': ['sum', 'mean', 'max'],\n",
    "                'ëŒ“ê¸€ê°¯ìˆ˜': ['sum', 'mean']\n",
    "            }).round(2)\n",
    "            yearly_stats.columns = ['ê²Œì‹œê¸€ìˆ˜', 'ì´ì¡°íšŒìˆ˜', 'í‰ê· ì¡°íšŒìˆ˜', 'ìµœëŒ€ì¡°íšŒìˆ˜', 'ì´ëŒ“ê¸€ìˆ˜', 'í‰ê· ëŒ“ê¸€ìˆ˜']\n",
    "            yearly_stats = yearly_stats.reset_index()\n",
    "            \n",
    "            ws1 = wb.create_sheet(\"ì—°ë„ë³„_íŠ¸ë Œë“œ\")\n",
    "            for row in dataframe_to_rows(yearly_stats, index=False, header=True):\n",
    "                ws1.append(row)\n",
    "            \n",
    "            # í—¤ë” ìŠ¤íƒ€ì¼ë§\n",
    "            for cell in ws1[1]:\n",
    "                cell.font = Font(bold=True)\n",
    "                cell.fill = PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n",
    "        \n",
    "        # 2. ì „ì²´ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\n",
    "        all_text = (self.processed_df['ì œëª©'] + ' ' + self.processed_df['ë‚´ìš©']).str.cat(sep=' ')\n",
    "        final_words = self._get_filtered_words_advanced(all_text, use_morphology=self.use_morphology)\n",
    "        word_freq = Counter(final_words)\n",
    "        top_words = word_freq.most_common(50)\n",
    "        \n",
    "        word_freq_data = []\n",
    "        for i, (word, count) in enumerate(top_words, 1):\n",
    "            word_freq_data.append({'ìˆœìœ„': i, 'ë‹¨ì–´': word, 'ë¹ˆë„ìˆ˜': count})\n",
    "        \n",
    "        word_freq_df = pd.DataFrame(word_freq_data)\n",
    "        ws2 = wb.create_sheet(\"ì „ì²´_ë‹¨ì–´ë¹ˆë„\")\n",
    "        for row in dataframe_to_rows(word_freq_df, index=False, header=True):\n",
    "            ws2.append(row)\n",
    "        \n",
    "        # í—¤ë” ìŠ¤íƒ€ì¼ë§\n",
    "        for cell in ws2[1]:\n",
    "            cell.font = Font(bold=True)\n",
    "            cell.fill = PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n",
    "        \n",
    "        # 3. ì—°ë„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\n",
    "        yearly_word_data = []\n",
    "        for year in sorted(self.processed_df['ì—°ë„'].unique()):\n",
    "            year_data = self.processed_df[self.processed_df['ì—°ë„'] == year]\n",
    "            year_text = (year_data['ì œëª©'] + ' ' + year_data['ë‚´ìš©']).str.cat(sep=' ')\n",
    "            year_words = self._get_filtered_words_advanced(year_text, use_morphology=self.use_morphology)\n",
    "            year_word_freq = Counter(year_words)\n",
    "            top_year_words = year_word_freq.most_common(20)\n",
    "            \n",
    "            for i, (word, count) in enumerate(top_year_words, 1):\n",
    "                yearly_word_data.append({\n",
    "                    'ì—°ë„': year,\n",
    "                    'ìˆœìœ„': i,\n",
    "                    'ë‹¨ì–´': word,\n",
    "                    'ë¹ˆë„ìˆ˜': count\n",
    "                })\n",
    "        \n",
    "        yearly_word_df = pd.DataFrame(yearly_word_data)\n",
    "        ws3 = wb.create_sheet(\"ì—°ë„ë³„_ë‹¨ì–´ë¹ˆë„\")\n",
    "        for row in dataframe_to_rows(yearly_word_df, index=False, header=True):\n",
    "            ws3.append(row)\n",
    "        \n",
    "        # í—¤ë” ìŠ¤íƒ€ì¼ë§\n",
    "        for cell in ws3[1]:\n",
    "            cell.font = Font(bold=True)\n",
    "            cell.fill = PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n",
    "        \n",
    "        # 4. ì¤‘ë³µ ì œê±° ê²°ê³¼ (ìˆë‹¤ë©´)\n",
    "        if self.filtered_df is not None:\n",
    "            duplicate_stats = pd.DataFrame({\n",
    "                'í•­ëª©': ['ì›ë³¸ ê²Œì‹œê¸€ ìˆ˜', 'ì¤‘ë³µ ì œê±° í›„', 'ì œê±°ëœ ì¤‘ë³µ ìˆ˜', 'ì¤‘ë³µ ë¹„ìœ¨(%)'],\n",
    "                'ê°’': [\n",
    "                    len(self.processed_df),\n",
    "                    len(self.filtered_df),\n",
    "                    len(self.processed_df) - len(self.filtered_df),\n",
    "                    round((len(self.processed_df) - len(self.filtered_df)) / len(self.processed_df) * 100, 2)\n",
    "                ]\n",
    "            })\n",
    "            \n",
    "            ws4 = wb.create_sheet(\"ì¤‘ë³µì œê±°_í†µê³„\")\n",
    "            for row in dataframe_to_rows(duplicate_stats, index=False, header=True):\n",
    "                ws4.append(row)\n",
    "            \n",
    "            # í—¤ë” ìŠ¤íƒ€ì¼ë§\n",
    "            for cell in ws4[1]:\n",
    "                cell.font = Font(bold=True)\n",
    "                cell.fill = PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n",
    "        \n",
    "        # íŒŒì¼ ì €ì¥\n",
    "        filename = 'community_analysis_complete.xlsx'\n",
    "        wb.save(filename)\n",
    "        print(f\"âœ… ëª¨ë“  ë¶„ì„ ê²°ê³¼ê°€ '{filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "        print(f\"   ğŸ“‹ í¬í•¨ëœ ì‹œíŠ¸: {', '.join([ws.title for ws in wb.worksheets])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Excel ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        print(\"ğŸ“¥ openpyxl ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install openpyxl\")\n",
    "\n",
    "\n",
    "def generate_advanced_summary_report(self):\n",
    "    \"\"\"ê°œì„ ëœ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"     ê°œì„ ëœ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸ (ìš•ì„¤ í•„í„°ë§ í¬í•¨)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if self.processed_df is not None:\n",
    "        print(\"ğŸ“Š ì›ë³¸ ë°ì´í„° í†µê³„\")\n",
    "        total_posts = len(self.processed_df)\n",
    "        print(f\"   ì´ ê²Œì‹œê¸€ ìˆ˜: {total_posts:,}ê°œ\")\n",
    "        print(f\"   ì´ ì¡°íšŒìˆ˜: {self.processed_df['ì¡°íšŒìˆ˜'].sum():,}íšŒ\")\n",
    "        print(f\"   í‰ê·  ì¡°íšŒìˆ˜: {self.processed_df['ì¡°íšŒìˆ˜'].mean():.1f}íšŒ\")\n",
    "    \n",
    "    if self.filtered_df is not None:\n",
    "        print(\"\\nğŸ§¹ í•„í„°ë§ í›„ ë°ì´í„° í†µê³„\")\n",
    "        filtered_posts = len(self.filtered_df)\n",
    "        removed_posts = total_posts - filtered_posts\n",
    "        print(f\"   í•„í„°ë§ í›„ ê²Œì‹œê¸€ ìˆ˜: {filtered_posts:,}ê°œ\")\n",
    "        print(f\"   ì œê±°ëœ ê²Œì‹œê¸€ ìˆ˜: {removed_posts:,}ê°œ ({removed_posts/total_posts*100:.1f}%)\")\n",
    "        print(f\"   í•„í„°ë§ í›„ ì´ ì¡°íšŒìˆ˜: {self.filtered_df['ì¡°íšŒìˆ˜'].sum():,}íšŒ\")\n",
    "        print(f\"   í•„í„°ë§ í›„ í‰ê·  ì¡°íšŒìˆ˜: {self.filtered_df['ì¡°íšŒìˆ˜'].mean():.1f}íšŒ\")\n",
    "    \n",
    "    if self.use_morphology:\n",
    "        print(f\"\\nğŸ”¤ í˜•íƒœì†Œ ë¶„ì„: âœ… ì‚¬ìš© ê°€ëŠ¥ (KoNLPy)\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ”¤ í˜•íƒœì†Œ ë¶„ì„: âŒ ì‚¬ìš© ë¶ˆê°€ (ì •ê·œì‹ ì‚¬ìš©)\")\n",
    "    \n",
    "    print(f\"\\nğŸš« ìš•ì„¤ í•„í„°ë§: âœ… ì ìš©ë¨ (íŒ¨í„´ ë§¤ì¹­ + ì‚¬ì „ ê¸°ë°˜)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# í´ë˜ìŠ¤ì— ë©”ì„œë“œ ì¶”ê°€\n",
    "AdvancedCommunityDataAnalyzer.save_all_analysis_to_excel = save_all_analysis_to_excel\n",
    "AdvancedCommunityDataAnalyzer.generate_advanced_summary_report = generate_advanced_summary_report\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸš€ ê°œì„ ëœ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ë¶„ì„ê¸°\")\n",
    "    print(\"ğŸ’¡ íŒ: ë¨¼ì € ë©”ë‰´ 1ë²ˆìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•œ í›„ ë‹¤ë¥¸ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì„¸ìš”!\")\n",
    "    main_menu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
