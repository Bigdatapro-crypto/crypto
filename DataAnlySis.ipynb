{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670a73bb",
   "metadata": {},
   "source": [
    "주요 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715b9f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib koreanize-matplotlib konlpy wheel JPype1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f30bf21",
   "metadata": {},
   "source": [
    "konlpy를 위한 Java 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import sys\n",
    "\n",
    "def update_gitignore():\n",
    "    \"\"\"gitignore에 Java 관련 파일들 추가 (권한 문제시 건너뛰기)\"\"\"\n",
    "    gitignore_entries = [\n",
    "        \"# Java Runtime for KoNLPy\",\n",
    "        \"java/\",\n",
    "        \"openjdk-*.zip\", \n",
    "        \"*.jdk\",\n",
    "        \"jdk*/\",\n",
    "        \"# Java 실행 파일\",\n",
    "        \"java.exe\",\n",
    "        \"javac.exe\",\n",
    "        \"\",\n",
    "        \"# Python\",\n",
    "        \"__pycache__/\",\n",
    "        \"*.pyc\",\n",
    "        \"*.pyo\", \n",
    "        \"*.pyd\",\n",
    "        \".Python\",\n",
    "        \"*.so\",\n",
    "        \"\",\n",
    "        \"# Jupyter Notebook\",\n",
    "        \".ipynb_checkpoints\",\n",
    "        \"\",\n",
    "        \"# IDE\",\n",
    "        \".vscode/\",\n",
    "        \".idea/\",\n",
    "        \"*.swp\",\n",
    "        \"*.swo\",\n",
    "        \"*~\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # 기존 .gitignore 내용 읽기\n",
    "        existing_content = \"\"\n",
    "        gitignore_exists = os.path.exists(\".gitignore\")\n",
    "        \n",
    "        if gitignore_exists:\n",
    "            try:\n",
    "                with open(\".gitignore\", \"r\", encoding=\"utf-8\") as f:\n",
    "                    existing_content = f.read()\n",
    "            except PermissionError:\n",
    "                print(\"⚠️ .gitignore 읽기 권한 없음 - 건너뜀\")\n",
    "                print(\"💡 수동으로 .gitignore에 'java/' 를 추가해주세요\")\n",
    "                return False\n",
    "            except:\n",
    "                try:\n",
    "                    with open(\".gitignore\", \"r\", encoding=\"cp949\") as f:\n",
    "                        existing_content = f.read()\n",
    "                except PermissionError:\n",
    "                    print(\"⚠️ .gitignore 읽기 권한 없음 - 건너뜀\")\n",
    "                    print(\"💡 수동으로 .gitignore에 'java/' 를 추가해주세요\")\n",
    "                    return False\n",
    "                except:\n",
    "                    existing_content = \"\"\n",
    "        else:\n",
    "            print(\"📝 .gitignore 파일이 없습니다. 생성을 시도합니다.\")\n",
    "        \n",
    "        # 이미 Java 관련 항목이 있는지 확인\n",
    "        if \"java/\" in existing_content:\n",
    "            print(\"✅ .gitignore에 이미 Java 관련 항목이 있습니다\")\n",
    "            return True\n",
    "        \n",
    "        # .gitignore 생성 또는 업데이트\n",
    "        try:\n",
    "            mode = \"a\" if gitignore_exists else \"w\"\n",
    "            with open(\".gitignore\", mode, encoding=\"utf-8\") as f:\n",
    "                if gitignore_exists and existing_content and not existing_content.endswith('\\n'):\n",
    "                    f.write('\\n')\n",
    "                for entry in gitignore_entries:\n",
    "                    f.write(entry + \"\\n\")\n",
    "            \n",
    "            if gitignore_exists:\n",
    "                print(\"✅ .gitignore에 Java 관련 항목 추가 완료\")\n",
    "            else:\n",
    "                print(\"✅ .gitignore 파일 생성 및 Java 관련 항목 추가 완료\")\n",
    "            return True\n",
    "            \n",
    "        except PermissionError:\n",
    "            print(\"⚠️ .gitignore 쓰기 권한 없음 - 건너뜀\")\n",
    "            print(\"💡 Java 설치는 계속 진행하되, 수동으로 .gitignore에 다음을 추가해주세요:\")\n",
    "            print(\"   java/\")\n",
    "            print(\"   openjdk-*.zip\")\n",
    "            print(\"   *.jdk\")\n",
    "            print(\"   .ipynb_checkpoints\")\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            # 다른 인코딩으로 시도\n",
    "            try:\n",
    "                mode = \"a\" if gitignore_exists else \"w\"\n",
    "                with open(\".gitignore\", mode, encoding=\"cp949\") as f:\n",
    "                    if gitignore_exists and existing_content and not existing_content.endswith('\\n'):\n",
    "                        f.write('\\n')\n",
    "                    for entry in gitignore_entries:\n",
    "                        f.write(entry + \"\\n\")\n",
    "                \n",
    "                if gitignore_exists:\n",
    "                    print(\"✅ .gitignore에 Java 관련 항목 추가 완료\")\n",
    "                else:\n",
    "                    print(\"✅ .gitignore 파일 생성 및 Java 관련 항목 추가 완료\")\n",
    "                return True\n",
    "                \n",
    "            except PermissionError:\n",
    "                print(\"⚠️ .gitignore 쓰기 권한 없음 - 건너뜀\")\n",
    "                print(\"💡 Java 설치는 계속 진행하되, 수동으로 .gitignore에 다음을 추가해주세요:\")\n",
    "                print(\"   java/\")\n",
    "                print(\"   openjdk-*.zip\")\n",
    "                print(\"   *.jdk\")\n",
    "                print(\"   .ipynb_checkpoints\")\n",
    "                return False\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"⚠️ .gitignore 처리 실패: {e2}\")\n",
    "                print(\"💡 Java 설치는 계속 진행하되, 수동으로 .gitignore에 다음을 추가해주세요:\")\n",
    "                print(\"   java/\")\n",
    "                print(\"   openjdk-*.zip\")\n",
    "                print(\"   *.jdk\")\n",
    "                print(\"   .ipynb_checkpoints\")\n",
    "                return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ .gitignore 처리 중 예외: {e}\")\n",
    "        print(\"💡 Java 설치는 계속 진행하되, 수동으로 .gitignore 파일을 생성하고 다음 내용을 추가하세요:\")\n",
    "        print(\"   java/\")\n",
    "        print(\"   openjdk-*.zip\")\n",
    "        print(\"   *.jdk\")\n",
    "        print(\"   .ipynb_checkpoints\")\n",
    "        return False\n",
    "\n",
    "def install_java_and_konlpy():\n",
    "    \"\"\"Java 설치 및 KoNLPy 설정\"\"\"\n",
    "    \n",
    "    print(\"🔄 Java 및 KoNLPy 설치 시작...\")\n",
    "    \n",
    "    # 먼저 .gitignore 업데이트 (실패해도 계속 진행)\n",
    "    print(\"📝 .gitignore 업데이트 시도...\")\n",
    "    gitignore_success = update_gitignore()\n",
    "    if not gitignore_success:\n",
    "        print(\"⚠️ .gitignore 설정 실패했지만 Java 설치는 계속 진행합니다\")\n",
    "        print(\"📌 나중에 수동으로 .gitignore에 'java/' 를 추가해주세요\")\n",
    "    \n",
    "    # 1단계: Java 설치 여부 확인\n",
    "    def check_java():\n",
    "        try:\n",
    "            result = subprocess.run(['java', '-version'], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(\"✅ Java가 이미 설치되어 있습니다\")\n",
    "                return True\n",
    "        except FileNotFoundError:\n",
    "            print(\"❌ Java가 설치되지 않았습니다\")\n",
    "        return False\n",
    "    \n",
    "    # 2단계: Java 자동 다운로드 (Portable)\n",
    "    def download_portable_java():\n",
    "        print(\"📥 Portable Java 다운로드 중...\")\n",
    "        \n",
    "        # OpenJDK 다운로드 URL (Windows x64)\n",
    "        java_url = \"https://download.java.net/java/GA/jdk17.0.2/dfd4a8d0985749f896bed50d7138ee7f/8/GPL/openjdk-17.0.2_windows-x64_bin.zip\"\n",
    "        java_zip = \"openjdk-17.0.2.zip\"\n",
    "        java_dir = \"java\"\n",
    "        \n",
    "        try:\n",
    "            # 진행률 표시 함수\n",
    "            def show_progress(block_num, block_size, total_size):\n",
    "                downloaded = block_num * block_size\n",
    "                if total_size > 0:\n",
    "                    percent = min(100, (downloaded * 100) // total_size)\n",
    "                    print(f\"\\r다운로드 진행률: {percent}% ({downloaded // (1024*1024)}MB / {total_size // (1024*1024)}MB)\", end=\"\")\n",
    "            \n",
    "            # 다운로드\n",
    "            urllib.request.urlretrieve(java_url, java_zip, show_progress)\n",
    "            print(\"\\n✅ Java 다운로드 완료\")\n",
    "            \n",
    "            # 압축 해제\n",
    "            print(\"📂 Java 압축 해제 중...\")\n",
    "            with zipfile.ZipFile(java_zip, 'r') as zip_ref:\n",
    "                zip_ref.extractall(java_dir)\n",
    "            \n",
    "            # 다운로드 파일 삭제\n",
    "            os.remove(java_zip)\n",
    "            print(\"✅ 임시 파일 정리 완료\")\n",
    "            \n",
    "            # Java 경로 찾기\n",
    "            for root, dirs, files in os.walk(java_dir):\n",
    "                if 'java.exe' in files:\n",
    "                    java_home = os.path.dirname(root)\n",
    "                    print(f\"✅ Java 설치 경로: {java_home}\")\n",
    "                    return java_home\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Java 다운로드 실패: {e}\")\n",
    "            return None\n",
    "    \n",
    "    # 3단계: Java 경로 자동 찾기\n",
    "    def find_java_home():\n",
    "        # 시스템에 설치된 Java 찾기\n",
    "        possible_paths = [\n",
    "            \"C:/Program Files/Java/*/\",\n",
    "            \"C:/Program Files (x86)/Java/*/\",\n",
    "            \"C:/Users/*/AppData/Local/Programs/Java/*/\",\n",
    "        ]\n",
    "        \n",
    "        import glob\n",
    "        for pattern in possible_paths:\n",
    "            paths = glob.glob(pattern)\n",
    "            for path in paths:\n",
    "                java_exe = os.path.join(path, \"bin\", \"java.exe\")\n",
    "                if os.path.exists(java_exe):\n",
    "                    return path\n",
    "        return None\n",
    "    \n",
    "    # Java 설치 프로세스\n",
    "    java_home = None\n",
    "    \n",
    "    if not check_java():\n",
    "        print(\"🔧 자동으로 Portable Java를 설치합니다...\")\n",
    "        java_home = download_portable_java()\n",
    "        \n",
    "        if not java_home:\n",
    "            print(\"\\n📋 자동 설치 실패. 수동 설치 방법:\")\n",
    "            print(\"1. https://www.oracle.com/java/technologies/downloads/ 방문\")\n",
    "            print(\"2. Windows x64 Installer 다운로드\")\n",
    "            print(\"3. 설치 후 아래 코드 다시 실행\")\n",
    "            return False\n",
    "    else:\n",
    "        java_home = find_java_home()\n",
    "    \n",
    "    # 4단계: JAVA_HOME 설정\n",
    "    if java_home:\n",
    "        os.environ['JAVA_HOME'] = java_home\n",
    "        java_bin = os.path.join(java_home, \"bin\")\n",
    "        current_path = os.environ.get('PATH', '')\n",
    "        if java_bin not in current_path:\n",
    "            os.environ['PATH'] = java_bin + os.pathsep + current_path\n",
    "        print(f\"✅ JAVA_HOME 설정: {java_home}\")\n",
    "        \n",
    "        # 5단계: KoNLPy 설치 및 테스트\n",
    "        try:\n",
    "            print(\"📦 KoNLPy 설치 중...\")\n",
    "            subprocess.run([sys.executable, '-m', 'pip', 'install', 'konlpy', 'JPype1'], check=True)\n",
    "            \n",
    "            print(\"🧪 KoNLPy 테스트 중...\")\n",
    "            from konlpy.tag import Okt\n",
    "            okt = Okt()\n",
    "            result = okt.morphs(\"테스트 성공\")\n",
    "            print(f\"✅ KoNLPy 설치 및 테스트 성공: {result}\")\n",
    "            \n",
    "            print(\"\\n🎉 설치 완료!\")\n",
    "            print(\"=\" * 50)\n",
    "            print(\"📁 설치된 파일들:\")\n",
    "            print(f\"   - Java Runtime: {java_home}\")\n",
    "            print(\"   - KoNLPy: 설치됨\")\n",
    "            print(\"   - .gitignore: 업데이트됨\")\n",
    "            print(\"\\n💡 이제 형태소 분석을 사용할 수 있습니다!\")\n",
    "            print(\"   커널을 재시작한 후 원래 분석 코드를 실행하세요.\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ KoNLPy 설치 실패: {e}\")\n",
    "            print(\"💡 커널을 재시작한 후 다시 시도해보세요.\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"❌ Java 설치 실패\")\n",
    "        return False\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    if install_java_and_konlpy():\n",
    "        print(\"🎉 모든 설치 완료! 형태소 분석 사용 가능합니다.\")\n",
    "        KONLPY_AVAILABLE = True\n",
    "    else:\n",
    "        print(\"⚠️ Java 설치 실패. 정규식 방식으로 진행합니다.\")\n",
    "        KONLPY_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e11dc8",
   "metadata": {},
   "source": [
    "【시간별 트렌드 분석】\n",
    "\n",
    "1️⃣  연도별 트렌드 분석 (게시글수, 조회수, 댓글수)\n",
    "\n",
    "2️⃣  월별 트렌드 분석 (게시글수, 조회수, 댓글수)\n",
    "\n",
    "3️⃣  주간별 트렌드 분석 (게시글수, 조회수, 댓글수)\n",
    "\n",
    "【시간별 단어 빈도 분석】\n",
    "\n",
    "4️⃣  연도별 단어 빈도 분석\n",
    "\n",
    "5️⃣  월별 단어 빈도 분석\n",
    "\n",
    "6️⃣  주간별 단어 빈도 분석 (전체 주차)\n",
    "\n",
    "【전체 데이터 분석】\n",
    "\n",
    "7️⃣  전체 단어 빈도 분석\n",
    "\n",
    "8️⃣  정규식 기반 단어 빈도 (형태소 분석 없음)\n",
    "\n",
    "【결과 저장 및 리포트】\n",
    "\n",
    "9️⃣  모든 분석 결과 Excel 저장\n",
    "\n",
    "🔟  종합 리포트 출력\n",
    "\n",
    "【그래프 시각화】\n",
    "\n",
    "1️⃣1️⃣ 연도별 트렌드 그래프\n",
    "\n",
    "1️⃣2️⃣ 월별 트렌드 그래프\n",
    "\n",
    "1️⃣3️⃣ 주간별 트렌드 그래프\n",
    "\n",
    "1️⃣4️⃣ 전체 단어 빈도 그래프\n",
    "\n",
    "【프로그램 종료】\n",
    "0️⃣  프로그램 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78587a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import koreanize_matplotlib\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# 한국어 형태소 분석을 위한 라이브러리 (설치 필요)\n",
    "try:\n",
    "    from konlpy.tag import Okt, Mecab, Kkma\n",
    "    KONLPY_AVAILABLE = True\n",
    "    print(\"✅ KoNLPy 라이브러리가 사용 가능합니다.\")\n",
    "except ImportError:\n",
    "    KONLPY_AVAILABLE = False\n",
    "    print(\"⚠️ KoNLPy 라이브러리가 설치되지 않았습니다. 기본 분석을 사용합니다.\")\n",
    "    print(\"설치 방법: !pip install konlpy\")\n",
    "\n",
    "class AdvancedCommunityDataAnalyzer:\n",
    "    def __init__(self, csv_file_path):\n",
    "        \"\"\"\n",
    "        개선된 커뮤니티 데이터 분석기 초기화\n",
    "        \n",
    "        Args:\n",
    "            csv_file_path (str): CSV 파일 경로\n",
    "        \"\"\"\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.df = None\n",
    "        self.processed_df = None\n",
    "        self.filtered_df = None  # 도배 필터링된 데이터\n",
    "        \n",
    "        # 한국어 형태소 분석기 초기화\n",
    "        if KONLPY_AVAILABLE:\n",
    "            try:\n",
    "                self.okt = Okt()\n",
    "                self.morphology_analyzer = self.okt\n",
    "                self.use_morphology = True\n",
    "                print(\"✅ Okt 형태소 분석기를 사용합니다.\")\n",
    "            except:\n",
    "                self.use_morphology = False\n",
    "                print(\"⚠️ 형태소 분석기 초기화 실패. 기본 분석을 사용합니다.\")\n",
    "        else:\n",
    "            self.use_morphology = False\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"CSV 파일을 로드하고 기본 정보를 출력\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.csv_file_path, encoding='utf-8')\n",
    "            print(\"=== 데이터 로드 완료 ===\")\n",
    "            print(f\"총 데이터 수: {len(self.df):,}개\")\n",
    "            print(f\"컬럼: {list(self.df.columns)}\")\n",
    "            print(\"\\n=== 데이터 미리보기 ===\")\n",
    "            print(self.df.head())\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"데이터 로드 오류: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"데이터 전처리\"\"\"\n",
    "        if self.df is None:\n",
    "            print(\"데이터를 먼저 로드해주세요.\")\n",
    "            return False\n",
    "        \n",
    "        # 데이터 복사\n",
    "        self.processed_df = self.df.copy()\n",
    "        \n",
    "        # 날짜 변환 함수\n",
    "        def convert_date_pandas(date_str):\n",
    "            if pd.isna(date_str):\n",
    "                return pd.NaT\n",
    "            \n",
    "            date_str = str(date_str)\n",
    "            try:\n",
    "                if '.' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%y.%m.%d')\n",
    "                elif '/' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%y/%m/%d')\n",
    "                else:\n",
    "                    return pd.NaT\n",
    "            except:\n",
    "                return pd.NaT\n",
    "        \n",
    "        # 날짜 변환\n",
    "        self.processed_df['날짜_변환'] = self.processed_df['날짜'].apply(convert_date_pandas)\n",
    "        self.processed_df = self.processed_df.dropna(subset=['날짜_변환'])\n",
    "        \n",
    "        # 연도, 연월, 주차 컬럼 추가\n",
    "        self.processed_df['연도'] = self.processed_df['날짜_변환'].dt.year\n",
    "        self.processed_df['연월'] = self.processed_df['날짜_변환'].dt.to_period('M')\n",
    "        self.processed_df['주차'] = self.processed_df['날짜_변환'].dt.to_period('W')\n",
    "        \n",
    "        # 숫자 컬럼 변환\n",
    "        def safe_convert_to_numeric(x):\n",
    "            try:\n",
    "                if pd.isna(x) or x == '' or x == 'NaN':\n",
    "                    return 0\n",
    "                if isinstance(x, str):\n",
    "                    clean_num = ''.join(filter(str.isdigit, str(x)))\n",
    "                    return int(clean_num) if clean_num else 0\n",
    "                return int(x)\n",
    "            except:\n",
    "                return 0\n",
    "        \n",
    "        self.processed_df['조회수'] = self.processed_df['조회수'].apply(safe_convert_to_numeric)\n",
    "        self.processed_df['댓글갯수'] = self.processed_df['댓글갯수'].apply(safe_convert_to_numeric)\n",
    "        \n",
    "        # 결측값 처리\n",
    "        self.processed_df['제목'] = self.processed_df['제목'].fillna(\"\")\n",
    "        self.processed_df['내용'] = self.processed_df['내용'].fillna(\"\")\n",
    "        \n",
    "        print(\"=== 데이터 전처리 완료 ===\")\n",
    "        print(f\"처리된 데이터 수: {len(self.processed_df):,}개\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def calculate_text_similarity(self, text1, text2):\n",
    "        \"\"\"두 텍스트 간의 유사도 계산 (0~1)\"\"\"\n",
    "        return SequenceMatcher(None, text1, text2).ratio()\n",
    "    \n",
    "    def detect_spam_posts_exact_only(self, use_hashing=True):\n",
    "        \"\"\"\n",
    "        완전히 똑같은 글만 중복으로 처리\n",
    "        \n",
    "        Args:\n",
    "            use_hashing (bool): 해시 기반 빠른 필터링 사용 여부\n",
    "        \"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=== 완전 동일한 중복 게시글만 탐지 ===\")\n",
    "        print(f\"📊 총 {len(self.processed_df):,}개 게시글 분석\")\n",
    "        print(\"🔍 글자 하나라도 다르면 다른 글로 인정합니다\")\n",
    "        \n",
    "        spam_indices = set()\n",
    "        \n",
    "        if use_hashing:\n",
    "            # 해시 기반 완전 중복 탐지만 사용\n",
    "            print(\"🚀 해시 기반 완전 중복 탐지...\")\n",
    "            title_hashes = {}\n",
    "            content_hashes = {}\n",
    "            combined_hashes = {}  # 제목+내용 통합 해시\n",
    "            \n",
    "            for idx, row in self.processed_df.iterrows():\n",
    "                # 공백 정규화 (연속된 공백을 하나로)\n",
    "                title_clean = re.sub(r'\\s+', ' ', str(row['제목']).strip())\n",
    "                content_clean = re.sub(r'\\s+', ' ', str(row['내용']).strip())\n",
    "                combined_clean = title_clean + \" | \" + content_clean\n",
    "                \n",
    "                # 1. 제목 완전 중복 체크\n",
    "                title_hash = hash(title_clean.lower())\n",
    "                if title_hash in title_hashes:\n",
    "                    spam_indices.add(idx)\n",
    "                else:\n",
    "                    title_hashes[title_hash] = idx\n",
    "                \n",
    "                # 2. 내용 완전 중복 체크 (3글자 이상인 경우만)\n",
    "                if len(content_clean) >= 3:\n",
    "                    content_hash = hash(content_clean.lower())\n",
    "                    if content_hash in content_hashes:\n",
    "                        spam_indices.add(idx)\n",
    "                    else:\n",
    "                        content_hashes[content_hash] = idx\n",
    "                \n",
    "                # 3. 제목+내용 통합 완전 중복 체크\n",
    "                combined_hash = hash(combined_clean.lower())\n",
    "                if combined_hash in combined_hashes:\n",
    "                    spam_indices.add(idx)\n",
    "                else:\n",
    "                    combined_hashes[combined_hash] = idx\n",
    "            \n",
    "            print(f\"   총 중복 발견: {len(spam_indices):,}개\")\n",
    "        \n",
    "        # 중복이 아닌 게시글만 필터링\n",
    "        self.filtered_df = self.processed_df[~self.processed_df.index.isin(spam_indices)].copy()\n",
    "        \n",
    "        print(f\"\\n=== 완전 동일 중복 필터링 결과 ===\")\n",
    "        print(f\"원본 게시글 수: {len(self.processed_df):,}개\")\n",
    "        print(f\"완전 동일 중복 수: {len(spam_indices):,}개 ({len(spam_indices)/len(self.processed_df)*100:.1f}%)\")\n",
    "        print(f\"필터링 후 게시글 수: {len(self.filtered_df):,}개\")\n",
    "        print(f\"✅ 글자 하나라도 다르면 별개 게시글로 유지됩니다!\")\n",
    "        \n",
    "        return spam_indices\n",
    "    \n",
    "    def analyze_yearly_trends(self, save_to_csv=True):\n",
    "        \"\"\"연도별 트렌드 분석 (텍스트만 출력, CSV 저장 옵션)\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        # 연도별 통계 계산\n",
    "        yearly_stats = self.processed_df.groupby('연도').agg({\n",
    "            '번호': 'count',\n",
    "            '조회수': ['sum', 'mean', 'max'],\n",
    "            '댓글갯수': ['sum', 'mean']\n",
    "        }).round(2)\n",
    "        \n",
    "        # 컬럼명 정리\n",
    "        yearly_stats.columns = ['게시글수', '총조회수', '평균조회수', '최대조회수', '총댓글수', '평균댓글수']\n",
    "        yearly_stats = yearly_stats.reset_index()\n",
    "        \n",
    "        print(\"=== 연도별 통계 (텍스트 출력) ===\")\n",
    "        for _, row in yearly_stats.iterrows():\n",
    "            print(f\"{int(row['연도'])}: 게시글 {row['게시글수']:,}개, \"\n",
    "                  f\"총조회수 {row['총조회수']:,.0f}, 평균조회수 {row['평균조회수']:.1f}, \"\n",
    "                  f\"최대조회수 {row['최대조회수']:,.0f}, 총댓글수 {row['총댓글수']:,.0f}, \"\n",
    "                  f\"평균댓글수 {row['평균댓글수']:.1f}\")\n",
    "        \n",
    "        # CSV 저장\n",
    "        if save_to_csv:\n",
    "            filename = 'yearly_trends_analysis.csv'\n",
    "            yearly_stats.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\n✅ 연도별 분석 결과가 '{filename}'에 저장되었습니다.\")\n",
    "        \n",
    "        return yearly_stats\n",
    "    \n",
    "    def analyze_monthly_trends(self):\n",
    "        \"\"\"월별 트렌드 분석\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        # 월별 통계 계산\n",
    "        monthly_stats = self.processed_df.groupby('연월').agg({\n",
    "            '번호': 'count',\n",
    "            '조회수': ['sum', 'mean', 'max'],\n",
    "            '댓글갯수': ['sum', 'mean']\n",
    "        }).round(2)\n",
    "        \n",
    "        # 컬럼명 정리\n",
    "        monthly_stats.columns = ['게시글수', '총조회수', '평균조회수', '최대조회수', '총댓글수', '평균댓글수']\n",
    "        monthly_stats = monthly_stats.reset_index()\n",
    "        monthly_stats['연월_str'] = monthly_stats['연월'].astype(str)\n",
    "        \n",
    "        print(\"=== 월별 통계 ===\")\n",
    "        for _, row in monthly_stats.iterrows():\n",
    "            print(f\"{row['연월_str']}: 게시글 {row['게시글수']:,}개, \"\n",
    "                  f\"총조회수 {row['총조회수']:,.0f}, 평균조회수 {row['평균조회수']:.1f}\")\n",
    "        \n",
    "        return monthly_stats\n",
    "    \n",
    "    def _contains_profanity_pattern(self, word):\n",
    "        \"\"\"\n",
    "        정규식 패턴으로 욕설 탐지\n",
    "        \"\"\"\n",
    "        profanity_patterns = [\n",
    "            r'시.*발',      # 시발, 시이발, 시1발 등\n",
    "            r'개.*새끼',    # 개새끼, 개쌔끼 등\n",
    "            r'병.*신',      # 병신, 병1신 등\n",
    "            r'미.*친',      # 미친, 미1친 등\n",
    "            r'[ㅅㅆ][ㅂㅍ]',  # ㅅㅂ, ㅆㅂ 등\n",
    "            r'[지ㅈ][랄ㄹ]',  # 지랄, ㅈㄹ 등\n",
    "            r'꺼.*져',      # 꺼져, 꺼1져 등\n",
    "            r'죽.*어',      # 죽어, 뒈져 등\n",
    "            r'느.*금',      # 느금마 등\n",
    "            r'.*좆.*',      # 좆이 포함된 모든 단어\n",
    "            r'.*씨.*발.*',  # 씨발 변형\n",
    "            r'.*개.*놈',    # 개놈 변형\n",
    "        ]\n",
    "        \n",
    "        for pattern in profanity_patterns:\n",
    "            if re.search(pattern, word, re.IGNORECASE):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def extract_korean_morphemes(self, text, extract_nouns=True, extract_verbs=False, extract_adjectives=False):\n",
    "        \"\"\"\n",
    "        한국어 형태소 분석을 통한 단어 추출\n",
    "        \n",
    "        Args:\n",
    "            text (str): 분석할 텍스트\n",
    "            extract_nouns (bool): 명사 추출 여부\n",
    "            extract_verbs (bool): 동사 추출 여부  \n",
    "            extract_adjectives (bool): 형용사 추출 여부\n",
    "        \"\"\"\n",
    "        if not self.use_morphology:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # 형태소 분석\n",
    "            morphemes = self.morphology_analyzer.pos(text, stem=True)\n",
    "            \n",
    "            extracted_words = []\n",
    "            for word, pos in morphemes:\n",
    "                # 2글자 이상만 추출\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                    \n",
    "                # 품사별 추출\n",
    "                if extract_nouns and pos.startswith('N'):  # 명사\n",
    "                    extracted_words.append(word)\n",
    "                elif extract_verbs and pos.startswith('V'):  # 동사\n",
    "                    extracted_words.append(word)\n",
    "                elif extract_adjectives and pos.startswith('A'):  # 형용사\n",
    "                    extracted_words.append(word)\n",
    "            \n",
    "            return extracted_words\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"형태소 분석 오류: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _get_filtered_words_advanced(self, text, use_morphology=True):\n",
    "        \"\"\"\n",
    "        개선된 단어 필터링 (형태소 분석 + 기존 방식 + 욕설 필터링)\n",
    "        \"\"\"\n",
    "        # 한국어 형태소 분석 사용\n",
    "        if use_morphology and self.use_morphology:\n",
    "            korean_words = self.extract_korean_morphemes(\n",
    "                text, \n",
    "                extract_nouns=True, \n",
    "                extract_verbs=False,  # 동사는 제외 (의미가 모호할 수 있음)\n",
    "                extract_adjectives=False  # 형용사도 제외\n",
    "            )\n",
    "        else:\n",
    "            # 기존 방식: 정규식으로 한글 단어 추출\n",
    "            korean_words = re.findall(r'[가-힣]{2,}', text)\n",
    "        \n",
    "        # 영어/숫자 단어 추출\n",
    "        english_words = re.findall(r'[a-zA-Z0-9]{2,}', text.lower())\n",
    "        \n",
    "        # 모든 단어 합치기\n",
    "        all_words = korean_words + english_words\n",
    "        \n",
    "        # 확장된 불용어 목록 (욕설 포함)\n",
    "        stop_words = {\n",
    "            # 기존 불용어들\n",
    "            '이거', '이건', '저거', '저건', '그거', '그건', '여기', '저기', '거기',\n",
    "            '이게', '저게', '그게', '이야', '저야', '그야', '이런', '저런', '그런',\n",
    "            '뭐야', '뭔가', '진짜', '정말', '완전', '아니', '그냥', '좀', '더', \n",
    "            '너무', '되게', '엄청', '완전히', '정말로', '진짜로', '되면', '하지만',\n",
    "            '근데', '그런데', '그리고', '또한', '그래서', '따라서', '그러나', \n",
    "            '그렇지만', '그러므로', '말고', '해서', '되고', '하고', '있고', '없고',\n",
    "            '이제', '지금', '오늘', '어제', '내일', '요즘', '최근', '언제', '바로',\n",
    "            '내용', '없음', '경우', '때문', '되는', '하는', '있는', '없는',\n",
    "            '이렇게', '저렇게', '그렇게', '어떻게', '왜냐', '때문에', '이라고',\n",
    "            '내가', '나는', '너는', '너가', '걔는', '걔가', '쟤는', '쟤가',\n",
    "            '우리는', '우리가', '저는', '저가', '그가', '그는', '그녀는', '그녀가',\n",
    "            \n",
    "            # 커뮤니티 특화 불용어\n",
    "            '게시글', '댓글', '조회수', '추천', '비추천', '신고', '수정', '삭제',\n",
    "            '작성자', '닉네임', '아이디', '회원', '등급', '포인트', '게시판',\n",
    "            'dc', 'official', 'app', '다시', '계속', '여기서', '많이', '제발', 'name',\n",
    "            'txt','으후루꾸꾸루후으','루꾸꾸루','운지','노무현','일베','https',\n",
    "            \n",
    "            # 형태소 분석 결과 자주 나오는 불용어\n",
    "            '것', '수', '때', '곳', '점', '번', '개', '명', '년', '월', '일',\n",
    "            '시간', '분', '초', '정도', '만큼', '이상', '이하', '사이', '중',\n",
    "            '안', '밖', '위', '아래', '앞', '뒤', '옆', '다음', '이전', '마지막',\n",
    "            '처음', '끝', '시작', '종료', '완료', '시도', '노력', '생각', '의견',\n",
    "            '문제', '해결', '상황', '상태', '결과', '과정', '방법', '방식',\n",
    "            \n",
    "            # 감정 표현 (너무 일반적인 것들)\n",
    "            '좋다', '나쁘다', '싫다', '좋아', '싫어', '재미', '재밌', 'boring',\n",
    "            '웃음', '슬픔', '기쁨', '화남', '놀람', '걱정', '불안', '안심',\n",
    "            \n",
    "            # === 욕설 및 비속어 필터링 ===\n",
    "            # 일반적인 욕설\n",
    "            '시발', '씨발', 'ㅅㅂ', 'ㅆㅂ', '시팔', '씨팔', '시바', '씨바',\n",
    "            '개새끼', '개색끼', '개새키', '개색키', '개놈', '개년', '개썅', \n",
    "            '개쓰레기', '개돼지', '개병신', '개바보', '개멍청이',\n",
    "            '병신', '븅신', 'ㅂㅅ', '바보', '멍청이', '등신', '천치',\n",
    "            '미친놈', '미친년', '미친새끼', '미친것', '미친개', '미쳤나',\n",
    "            '또라이', '또라잇', '돌아이', '돌았나', '정신병자', '정신나간',\n",
    "            '죽어', '뒈져', '뒤져', '죽어라', '디져라', '디져', '디진다',\n",
    "            '꺼져', '꺼지라', '꺼저', '꺼쪄', '사라져', '없어져',\n",
    "            '지랄', '지럴', 'ㅈㄹ', '헛소리', '개소리', '똥싸다', '똥',\n",
    "            '엿먹어', '엿이나', '좆', 'ㅈ', '자지', '좆까', '좆나',\n",
    "            '니미', '니애미', '느금마', '느금', '니엄마', '너희엄마',\n",
    "            '호로', '창녀', '걸레', '썅년', '썅', '쌍년', '쌍놈',\n",
    "            '빡대가리', '빡종', '빡쳐', '화나', '개빡', '열받아',\n",
    "            '패고싶다', '패버린다', '때리고싶다', '죽이고싶다', '조지고싶다',\n",
    "            '새끼','존나',\n",
    "            \n",
    "            # 줄임말/은어 욕설\n",
    "            'ㅄ', 'ㅂㅅ', 'ㅅㅂ', 'ㅆㅂ', 'ㅈㄹ', 'ㅆㄹ', 'ㅂㄱ',\n",
    "            'ㅅㄲ', 'ㄱㅅㄲ', 'ㄷㅊ', 'ㅍㅌㅊ', 'ㅗㅜㅑ', 'ㅂㅅㄴ',\n",
    "        }\n",
    "        \n",
    "        # 불용어 제거 및 추가 필터링\n",
    "        filtered_words = []\n",
    "        for word in all_words:\n",
    "            # 불용어 제거 (욕설 포함)\n",
    "            if word.lower() in stop_words:\n",
    "                continue\n",
    "            \n",
    "            # 숫자만 있는 단어 제거\n",
    "            if word.isdigit():\n",
    "                continue\n",
    "            \n",
    "            # 반복 문자 제거 (ㅋㅋㅋ, ㅎㅎㅎ 등)\n",
    "            if len(set(word)) == 1 and len(word) > 2:\n",
    "                continue\n",
    "            \n",
    "            # 의성어/의태어 패턴 제거\n",
    "            if re.match(r'^(.{1,2})\\1+$', word):\n",
    "                continue\n",
    "    \n",
    "            # 특수 패턴 제거 (URL 조각, 이메일 조각 등)\n",
    "            if re.match(r'^(www|http|com|net|org)$', word):\n",
    "                continue\n",
    "            \n",
    "            # 욕설 패턴 추가 검사 (정규식 기반)\n",
    "            if self._contains_profanity_pattern(word):\n",
    "                continue\n",
    "                \n",
    "            filtered_words.append(word)\n",
    "        \n",
    "        return filtered_words\n",
    "    \n",
    "    def analyze_text_frequency_advanced(self, top_n=30, use_filtered_data=True, use_morphology=True, save_to_csv=True):\n",
    "        \"\"\"\n",
    "        개선된 텍스트 빈도 분석 (CSV 저장 옵션)\n",
    "        \n",
    "        Args:\n",
    "            top_n (int): 상위 N개 단어\n",
    "            use_filtered_data (bool): 도배 필터링된 데이터 사용 여부\n",
    "            use_morphology (bool): 형태소 분석 사용 여부\n",
    "            save_to_csv (bool): CSV 파일 저장 여부\n",
    "        \"\"\"\n",
    "        # 사용할 데이터 선택\n",
    "        if use_filtered_data and self.filtered_df is not None:\n",
    "            data_to_use = self.filtered_df\n",
    "            data_desc = \"도배 필터링 후\"\n",
    "        else:\n",
    "            data_to_use = self.processed_df\n",
    "            data_desc = \"전체\"\n",
    "        \n",
    "        if data_to_use is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        # 모든 텍스트 합치기\n",
    "        all_text = (data_to_use['제목'] + ' ' + data_to_use['내용']).str.cat(sep=' ')\n",
    "        \n",
    "        # 개선된 단어 추출\n",
    "        final_words = self._get_filtered_words_advanced(all_text, use_morphology)\n",
    "        \n",
    "        # 빈도 계산\n",
    "        word_freq = Counter(final_words)\n",
    "        top_words = word_freq.most_common(top_n)\n",
    "        \n",
    "        print(f\"=== {data_desc} 데이터 단어 빈도 TOP {top_n} ===\")\n",
    "        for i, (word, count) in enumerate(top_words, 1):\n",
    "            print(f\"{i:2d}. {word}: {count:,}회\")\n",
    "        \n",
    "        # CSV 저장\n",
    "        if save_to_csv and top_words:\n",
    "            import pandas as pd\n",
    "            word_freq_data = []\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                word_freq_data.append({\n",
    "                    '순위': i,\n",
    "                    '단어': word,\n",
    "                    '빈도수': count,\n",
    "                    '데이터타입': data_desc\n",
    "                })\n",
    "            \n",
    "            word_freq_df = pd.DataFrame(word_freq_data)\n",
    "            filename = f'word_frequency_{data_desc}.csv'\n",
    "            filename = filename.replace(' ', '_').replace('(', '').replace(')', '')\n",
    "            word_freq_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"\\n✅ 단어 빈도 분석 결과가 '{filename}'에 저장되었습니다.\")\n",
    "        \n",
    "        return top_words\n",
    "    \n",
    "    def analyze_weekly_trends(self):\n",
    "        \"\"\"주간별 트렌드 분석 (전체 기간)\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        # 주간별 통계 계산 (전체 기간)\n",
    "        weekly_stats = self.processed_df.groupby('주차').agg({\n",
    "            '번호': 'count',\n",
    "            '조회수': ['sum', 'mean'],\n",
    "            '댓글갯수': ['sum', 'mean']\n",
    "        }).round(2)\n",
    "        \n",
    "        # 컬럼명 정리\n",
    "        weekly_stats.columns = ['게시글수', '총조회수', '평균조회수', '총댓글수', '평균댓글수']\n",
    "        weekly_stats = weekly_stats.reset_index()\n",
    "        weekly_stats['주차_str'] = weekly_stats['주차'].astype(str)\n",
    "        \n",
    "        print(f\"=== 주간별 통계 (전체 {len(weekly_stats)}주) ===\")\n",
    "        print(f\"첫 주차: {weekly_stats['주차_str'].iloc[0]}\")\n",
    "        print(f\"마지막 주차: {weekly_stats['주차_str'].iloc[-1]}\")\n",
    "        print(f\"주간 평균 게시글수: {weekly_stats['게시글수'].mean():.1f}개\")\n",
    "        print(f\"주간 최대 게시글수: {weekly_stats['게시글수'].max()}개\")\n",
    "        print(f\"주간 평균 총조회수: {weekly_stats['총조회수'].mean():.0f}\")\n",
    "        \n",
    "        # CSV 저장\n",
    "        filename = 'weekly_trends_analysis.csv'\n",
    "        weekly_stats.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\n✅ 주간별 분석 결과가 '{filename}'에 저장되었습니다.\")\n",
    "        \n",
    "        return weekly_stats\n",
    "    \n",
    "    def analyze_monthly_word_frequency(self, top_n=20):\n",
    "        \"\"\"월별 단어 빈도 분석\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        monthly_word_freq = {}\n",
    "        all_monthly_data = []  # CSV 저장용 데이터\n",
    "        \n",
    "        for month in sorted(self.processed_df['연월'].unique()):\n",
    "            month_data = self.processed_df[self.processed_df['연월'] == month]\n",
    "            \n",
    "            # 해당 월의 모든 텍스트 합치기\n",
    "            month_text = (month_data['제목'] + ' ' + month_data['내용']).str.cat(sep=' ')\n",
    "            \n",
    "            # 필터링된 단어 가져오기\n",
    "            month_words = self._get_filtered_words_advanced(month_text, use_morphology=self.use_morphology)\n",
    "            \n",
    "            # 빈도 계산\n",
    "            word_freq = Counter(month_words)\n",
    "            top_words = word_freq.most_common(top_n)\n",
    "            monthly_word_freq[month] = top_words\n",
    "            \n",
    "            print(f\"=== {month} 가장 많이 사용된 단어 TOP {top_n} ===\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"{i:2d}. {word}: {count:,}회\")\n",
    "                # CSV 저장용 데이터 추가\n",
    "                all_monthly_data.append({\n",
    "                    '월': str(month),\n",
    "                    '게시글수': len(month_data),\n",
    "                    '순위': i,\n",
    "                    '단어': word,\n",
    "                    '빈도수': count\n",
    "                })\n",
    "            print()\n",
    "        \n",
    "        # CSV 저장\n",
    "        if all_monthly_data:\n",
    "            import pandas as pd\n",
    "            monthly_word_df = pd.DataFrame(all_monthly_data)\n",
    "            filename = 'monthly_word_frequency.csv'\n",
    "            monthly_word_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"✅ 월별 단어 빈도 분석 결과가 '{filename}'에 저장되었습니다.\")\n",
    "        \n",
    "        return monthly_word_freq\n",
    "    \n",
    "    def analyze_weekly_word_frequency(self, top_n=20, save_to_csv=True):\n",
    "        \"\"\"주간별 단어 빈도 분석 (전체 주차)\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        weekly_word_freq = {}\n",
    "        all_weekly_data = []  # CSV 저장용 데이터\n",
    "        \n",
    "        # 전체 주차 분석\n",
    "        all_weeks = sorted(self.processed_df['주차'].unique())\n",
    "        \n",
    "        print(f\"=== 주간별 단어 빈도 분석 (전체 {len(all_weeks)}주) ===\")\n",
    "        print(f\"분석 기간: {all_weeks[0]} ~ {all_weeks[-1]}\")\n",
    "        print()\n",
    "        \n",
    "        for week in all_weeks:\n",
    "            week_data = self.processed_df[self.processed_df['주차'] == week]\n",
    "            \n",
    "            if len(week_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            # 해당 주의 모든 텍스트 합치기\n",
    "            week_text = (week_data['제목'] + ' ' + week_data['내용']).str.cat(sep=' ')\n",
    "            \n",
    "            # 필터링된 단어 가져오기\n",
    "            week_words = self._get_filtered_words_advanced(week_text, use_morphology=self.use_morphology)\n",
    "            \n",
    "            # 빈도 계산\n",
    "            word_freq = Counter(week_words)\n",
    "            top_words = word_freq.most_common(top_n)\n",
    "            weekly_word_freq[week] = top_words\n",
    "            \n",
    "            print(f\"=== {week} (게시글 {len(week_data)}개) - TOP {top_n} ===\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"{i:2d}. {word}: {count:,}회\")\n",
    "            \n",
    "            # CSV 저장용 데이터 추가\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                all_weekly_data.append({\n",
    "                    '주차': str(week),\n",
    "                    '게시글수': len(week_data),\n",
    "                    '순위': i,\n",
    "                    '단어': word,\n",
    "                    '빈도수': count\n",
    "                })\n",
    "            print()\n",
    "        \n",
    "        # CSV 저장\n",
    "        if save_to_csv and all_weekly_data:\n",
    "            import pandas as pd\n",
    "            weekly_word_df = pd.DataFrame(all_weekly_data)\n",
    "            filename = 'weekly_word_frequency_all.csv'\n",
    "            weekly_word_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"✅ 주간별 단어 빈도 분석 결과가 '{filename}'에 저장되었습니다.\")\n",
    "        \n",
    "        return weekly_word_freq\n",
    "    \n",
    "    def plot_yearly_trends(self, yearly_stats):\n",
    "        \"\"\"연도별 트렌드 차트 그리기\"\"\"\n",
    "        if yearly_stats is None:\n",
    "            print(\"연도별 통계를 먼저 계산해주세요.\")\n",
    "            return\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        \n",
    "        # 연도별 게시글 수\n",
    "        ax1.bar(yearly_stats['연도'], yearly_stats['게시글수'], \n",
    "                color='#58D68D', alpha=0.8, width=0.6)\n",
    "        ax1.set_title('연도별 게시글 수', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('연도', fontsize=12)\n",
    "        ax1.set_ylabel('게시글 수', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 값 표시\n",
    "        for i, v in enumerate(yearly_stats['게시글수']):\n",
    "            ax1.annotate(f'{v:,.0f}', (yearly_stats['연도'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # 연도별 총 조회수\n",
    "        ax2.plot(yearly_stats['연도'], yearly_stats['총조회수'], \n",
    "                marker='o', linewidth=3, markersize=8, color='#2E86C1')\n",
    "        ax2.set_title('연도별 총 조회수', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('연도', fontsize=12)\n",
    "        ax2.set_ylabel('총 조회수', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 값 표시\n",
    "        for i, v in enumerate(yearly_stats['총조회수']):\n",
    "            ax2.annotate(f'{v:,.0f}', (yearly_stats['연도'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # 연도별 평균 조회수\n",
    "        ax3.plot(yearly_stats['연도'], yearly_stats['평균조회수'], \n",
    "                marker='s', linewidth=3, markersize=8, color='#E74C3C')\n",
    "        ax3.set_title('연도별 평균 조회수', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('연도', fontsize=12)\n",
    "        ax3.set_ylabel('평균 조회수', fontsize=12)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 값 표시\n",
    "        for i, v in enumerate(yearly_stats['평균조회수']):\n",
    "            ax3.annotate(f'{v:.1f}', (yearly_stats['연도'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # 연도별 댓글 수\n",
    "        ax4.bar(yearly_stats['연도'], yearly_stats['총댓글수'], \n",
    "                color='#F39C12', alpha=0.8, width=0.6)\n",
    "        ax4.set_title('연도별 총 댓글 수', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('연도', fontsize=12)\n",
    "        ax4.set_ylabel('총 댓글 수', fontsize=12)\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 값 표시\n",
    "        for i, v in enumerate(yearly_stats['총댓글수']):\n",
    "            ax4.annotate(f'{v:,.0f}', (yearly_stats['연도'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_monthly_views(self, monthly_stats):\n",
    "        \"\"\"월별 조회수 차트 그리기 - 개선된 버전\"\"\"\n",
    "        if monthly_stats is None:\n",
    "            print(\"월별 통계를 먼저 계산해주세요.\")\n",
    "            return\n",
    "        \n",
    "        # 그래프 크기를 더 크게 설정\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12))\n",
    "        \n",
    "        # 총 조회수 추이\n",
    "        ax1.plot(monthly_stats['연월_str'], monthly_stats['총조회수'], \n",
    "                marker='o', linewidth=3, markersize=8, color='#2E86C1')\n",
    "        ax1.set_title('월별 총 조회수 추이', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax1.set_xlabel('월', fontsize=12)\n",
    "        ax1.set_ylabel('총 조회수', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # X축 라벨 회전각 조정하고 간격 늘리기\n",
    "        ax1.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax1.tick_params(axis='y', labelsize=10)\n",
    "        \n",
    "        # 평균 조회수 추이\n",
    "        ax2.plot(monthly_stats['연월_str'], monthly_stats['평균조회수'], \n",
    "                marker='s', linewidth=3, markersize=8, color='#E74C3C')\n",
    "        ax2.set_title('월별 평균 조회수 추이', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax2.set_xlabel('월', fontsize=12)\n",
    "        ax2.set_ylabel('평균 조회수', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # X축 라벨 회전각 조정\n",
    "        ax2.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax2.tick_params(axis='y', labelsize=10)\n",
    "        \n",
    "        # 여백 조정\n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "        \n",
    "        # 게시글 수 차트 - 더 큰 크기로\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        bars = plt.bar(monthly_stats['연월_str'], monthly_stats['게시글수'], \n",
    "                      color='#58D68D', alpha=0.8, width=0.6)\n",
    "        plt.title('월별 게시글 수', fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel('월', fontsize=12)\n",
    "        plt.ylabel('게시글 수', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 값 표시 - 막대 위에 표시\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.annotate(f'{height:,.0f}', \n",
    "                        xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                        xytext=(0, 8), \n",
    "                        textcoords=\"offset points\", \n",
    "                        ha='center', \n",
    "                        va='bottom',\n",
    "                        fontsize=11)\n",
    "        \n",
    "        # X축 라벨 조정\n",
    "        plt.xticks(rotation=45, fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_weekly_trends(self, weekly_stats):\n",
    "        \"\"\"주간별 트렌드 차트 그리기 (전체 기간)\"\"\"\n",
    "        if weekly_stats is None:\n",
    "            print(\"주간별 통계를 먼저 계산해주세요.\")\n",
    "            return\n",
    "        \n",
    "        # 전체 주차가 많을 경우를 대비해 x축 라벨 간격 조정\n",
    "        total_weeks = len(weekly_stats)\n",
    "        label_step = max(1, total_weeks // 20)  # 최대 20개 라벨만 표시\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20, 15))\n",
    "        \n",
    "        # 주간별 게시글 수\n",
    "        ax1.plot(range(len(weekly_stats)), weekly_stats['게시글수'], \n",
    "                linewidth=1, color='#58D68D', alpha=0.8)\n",
    "        ax1.fill_between(range(len(weekly_stats)), weekly_stats['게시글수'], \n",
    "                        alpha=0.3, color='#58D68D')\n",
    "        ax1.set_title(f'주간별 게시글 수 (전체 {total_weeks}주)', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('주차', fontsize=12)\n",
    "        ax1.set_ylabel('게시글 수', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # x축 라벨 간격 조정\n",
    "        xtick_positions = range(0, len(weekly_stats), label_step)\n",
    "        xtick_labels = [weekly_stats['주차_str'].iloc[i] for i in xtick_positions]\n",
    "        ax1.set_xticks(xtick_positions)\n",
    "        ax1.set_xticklabels(xtick_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        # 주간별 총 조회수\n",
    "        ax2.plot(range(len(weekly_stats)), weekly_stats['총조회수'], \n",
    "                linewidth=1, color='#2E86C1', alpha=0.8)\n",
    "        ax2.fill_between(range(len(weekly_stats)), weekly_stats['총조회수'], \n",
    "                        alpha=0.3, color='#2E86C1')\n",
    "        ax2.set_title(f'주간별 총 조회수 (전체 {total_weeks}주)', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('주차', fontsize=12)\n",
    "        ax2.set_ylabel('총 조회수', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xticks(xtick_positions)\n",
    "        ax2.set_xticklabels(xtick_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        # 주간별 평균 조회수\n",
    "        ax3.plot(range(len(weekly_stats)), weekly_stats['평균조회수'], \n",
    "                linewidth=1, color='#E74C3C', alpha=0.8)\n",
    "        ax3.fill_between(range(len(weekly_stats)), weekly_stats['평균조회수'], \n",
    "                        alpha=0.3, color='#E74C3C')\n",
    "        ax3.set_title(f'주간별 평균 조회수 (전체 {total_weeks}주)', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('주차', fontsize=12)\n",
    "        ax3.set_ylabel('평균 조회수', fontsize=12)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_xticks(xtick_positions)\n",
    "        ax3.set_xticklabels(xtick_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "        \n",
    "        # 주간별 통계 요약 출력\n",
    "        print(f\"\\n=== 주간별 데이터 요약 ===\")\n",
    "        print(f\"총 주차 수: {total_weeks}주\")\n",
    "        print(f\"주간 게시글 수 - 평균: {weekly_stats['게시글수'].mean():.1f}, 최소: {weekly_stats['게시글수'].min()}, 최대: {weekly_stats['게시글수'].max()}\")\n",
    "        print(f\"주간 총조회수 - 평균: {weekly_stats['총조회수'].mean():.0f}, 최소: {weekly_stats['총조회수'].min():.0f}, 최대: {weekly_stats['총조회수'].max():.0f}\")\n",
    "        print(f\"주간 평균조회수 - 평균: {weekly_stats['평균조회수'].mean():.1f}, 최소: {weekly_stats['평균조회수'].min():.1f}, 최대: {weekly_stats['평균조회수'].max():.1f}\")\n",
    "    \n",
    "    def plot_word_frequency(self, top_words, data_desc=\"전체\", use_morphology=True):\n",
    "        \"\"\"단어 빈도 그래프 그리기\"\"\"\n",
    "        if not top_words:\n",
    "            print(\"단어 빈도 데이터가 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        # 상위 25개 단어만 표시 (그래프가 너무 복잡해지지 않도록)\n",
    "        display_words = top_words[:25]\n",
    "        words, counts = zip(*display_words)\n",
    "        \n",
    "        # 그래프 크기 설정\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # 수평 막대 그래프\n",
    "        y_pos = np.arange(len(words))\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(words)))\n",
    "        \n",
    "        bars = plt.barh(y_pos, counts, color=colors, alpha=0.8, height=0.7)\n",
    "        \n",
    "        # 그래프 설정\n",
    "        analysis_method = \"형태소 분석\" if use_morphology else \"정규식\"\n",
    "        plt.title(f'{data_desc} 데이터 단어 빈도 TOP {len(display_words)} ({analysis_method})', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel('빈도수', fontsize=14)\n",
    "        plt.ylabel('단어', fontsize=14)\n",
    "        \n",
    "        # Y축 레이블 설정\n",
    "        plt.yticks(y_pos, words, fontsize=12)\n",
    "        plt.gca().invert_yaxis()  # 빈도가 높은 단어가 위에 오도록\n",
    "        \n",
    "        # 격자 표시\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # 값 표시\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            plt.annotate(f'{width:,}', \n",
    "                        xy=(width, bar.get_y() + bar.get_height()/2),\n",
    "                        xytext=(5, 0), \n",
    "                        textcoords=\"offset points\", \n",
    "                        ha='left', \n",
    "                        va='center', \n",
    "                        fontsize=11,\n",
    "                        fontweight='bold')\n",
    "        \n",
    "        # 레이아웃 조정\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        \n",
    "        # 통계 정보 표시\n",
    "        total_unique_words = len(set([word for word, count in top_words]))\n",
    "        total_word_count = sum([count for word, count in top_words])\n",
    "        \n",
    "        plt.figtext(0.02, 0.02, \n",
    "                   f'총 고유 단어 수: {total_unique_words:,}개 | 총 단어 출현 횟수: {total_word_count:,}회', \n",
    "                   fontsize=10, ha='left')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # 간단한 통계 출력\n",
    "        print(f\"\\n=== 단어 빈도 분석 요약 ({analysis_method}) ===\")\n",
    "        print(f\"총 고유 단어 수: {total_unique_words:,}개\")\n",
    "        print(f\"총 단어 출현 횟수: {total_word_count:,}회\")\n",
    "        print(f\"가장 빈번한 단어: '{words[0]}' ({counts[0]:,}회)\")\n",
    "        print(f\"평균 빈도: {total_word_count/total_unique_words:.1f}회\")\n",
    "        \"\"\"연도별 단어 빈도 분석 (CSV 저장 옵션)\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        yearly_word_freq = {}\n",
    "        all_yearly_data = []  # CSV 저장용 데이터\n",
    "        \n",
    "        for year in sorted(self.processed_df['연도'].unique()):\n",
    "            year_data = self.processed_df[self.processed_df['연도'] == year]\n",
    "            \n",
    "            # 해당 연도의 모든 텍스트 합치기\n",
    "            year_text = (year_data['제목'] + ' ' + year_data['내용']).str.cat(sep=' ')\n",
    "            \n",
    "            # 필터링된 단어 가져오기\n",
    "            year_words = self._get_filtered_words_advanced(year_text, use_morphology=self.use_morphology)\n",
    "            \n",
    "            # 빈도 계산\n",
    "            word_freq = Counter(year_words)\n",
    "            top_words = word_freq.most_common(top_n)\n",
    "            yearly_word_freq[year] = top_words\n",
    "            \n",
    "            print(f\"=== {year}년 가장 많이 사용된 단어 TOP {top_n} ===\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"{i:2d}. {word}: {count:,}회\")\n",
    "                # CSV 저장용 데이터 추가\n",
    "                all_yearly_data.append({\n",
    "                    '연도': year,\n",
    "                    '순위': i,\n",
    "                    '단어': word,\n",
    "                    '빈도수': count\n",
    "                })\n",
    "            print()\n",
    "        \n",
    "        # CSV 저장\n",
    "        if save_to_csv and all_yearly_data:\n",
    "            import pandas as pd\n",
    "            yearly_word_df = pd.DataFrame(all_yearly_data)\n",
    "            filename = 'yearly_word_frequency.csv'\n",
    "            yearly_word_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            print(f\"✅ 연도별 단어 빈도 분석 결과가 '{filename}'에 저장되었습니다.\")\n",
    "        \n",
    "        return yearly_word_freq\n",
    "\n",
    "\n",
    "def main_menu():\n",
    "    \"\"\"메인 메뉴 표시 및 사용자 선택 처리\"\"\"\n",
    "    # CSV 파일 경로 설정\n",
    "    csv_file_path = 'community/ChartAnalysis.csv'  # 실제 파일 경로로 수정하세요\n",
    "    analyzer = AdvancedCommunityDataAnalyzer(csv_file_path)\n",
    "    \n",
    "    # 자동으로 데이터 로드 및 전처리 수행\n",
    "    print(\"🔄 데이터 로드 및 전처리 시작...\")\n",
    "    if not analyzer.load_data():\n",
    "        print(\"❌ 데이터 로드 실패. 프로그램을 종료합니다.\")\n",
    "        return\n",
    "    \n",
    "    if not analyzer.preprocess_data():\n",
    "        print(\"❌ 데이터 전처리 실패. 프로그램을 종료합니다.\")\n",
    "        return\n",
    "    \n",
    "    print(\"✅ 데이터 로드 및 전처리 완료!\")\n",
    "    \n",
    "    # 자동으로 중복 필터링 수행\n",
    "    print(\"\\n🧹 중복 게시글 필터링 시작...\")\n",
    "    spam_indices = analyzer.detect_spam_posts_exact_only(use_hashing=True)\n",
    "    \n",
    "    # 필터링된 데이터를 CSV로 저장\n",
    "    if analyzer.filtered_df is not None:\n",
    "        output_filename = 'filtered_community_data.csv'\n",
    "        analyzer.filtered_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "        print(f\"✅ 필터링된 데이터가 '{output_filename}'로 저장되었습니다.\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"🚀 커뮤니티 데이터 분석기 - 분석 메뉴\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"【시간별 트렌드 분석】\")\n",
    "        print(\"1️⃣  연도별 트렌드 분석 (게시글수, 조회수, 댓글수)\")\n",
    "        print(\"2️⃣  월별 트렌드 분석 (게시글수, 조회수, 댓글수)\")\n",
    "        print(\"3️⃣  주간별 트렌드 분석 (게시글수, 조회수, 댓글수)\")\n",
    "        print()\n",
    "        print(\"【시간별 단어 빈도 분석】\")\n",
    "        print(\"4️⃣  연도별 단어 빈도 분석\")\n",
    "        print(\"5️⃣  월별 단어 빈도 분석\")\n",
    "        print(\"6️⃣  주간별 단어 빈도 분석 (전체 주차)\")\n",
    "        print()\n",
    "        print(\"【전체 데이터 분석】\")\n",
    "        print(\"7️⃣  전체 단어 빈도 분석\")\n",
    "        print(\"8️⃣  정규식 기반 단어 빈도 (형태소 분석 없음)\")\n",
    "        print()\n",
    "        print(\"【그래프 시각화】\")\n",
    "        print(\"11  연도별 트렌드 그래프\")\n",
    "        print(\"12  월별 트렌드 그래프\")\n",
    "        print(\"13  주간별 트렌드 그래프\")\n",
    "        print(\"14  전체 단어 빈도 그래프\")\n",
    "        print()\n",
    "        print(\"【결과 저장 및 리포트】\")\n",
    "        print(\"9️⃣  모든 분석 결과 Excel 저장\")\n",
    "        print(\"🔟  종합 리포트 출력\")\n",
    "        print(\"0️⃣  프로그램 종료\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            choice = input(\"\\n📌 원하는 분석의 번호를 입력하세요 (0-14): \").strip()\n",
    "            \n",
    "            if choice == '0':\n",
    "                print(\"👋 프로그램을 종료합니다.\")\n",
    "                break\n",
    "                \n",
    "            elif choice == '1':\n",
    "                print(\"\\n📅 연도별 트렌드 분석 시작...\")\n",
    "                yearly_stats = analyzer.analyze_yearly_trends(save_to_csv=True)\n",
    "                \n",
    "            elif choice == '2':\n",
    "                print(\"\\n📊 월별 트렌드 분석 시작...\")\n",
    "                monthly_stats = analyzer.analyze_monthly_trends()\n",
    "                \n",
    "            elif choice == '3':\n",
    "                print(\"\\n📈 주간별 트렌드 분석 시작...\")\n",
    "                weekly_stats = analyzer.analyze_weekly_trends()\n",
    "                \n",
    "            elif choice == '4':\n",
    "                print(\"\\n📅 연도별 단어 빈도 분석 시작...\")\n",
    "                if not analyzer.use_morphology:\n",
    "                    print(\"⚠️ 형태소 분석 라이브러리가 없어서 정규식 기반으로 분석합니다.\")\n",
    "                else:\n",
    "                    print(\"🔤 형태소 분석 기반으로 분석합니다.\")\n",
    "                yearly_word_freq = analyzer.analyze_yearly_word_frequency(top_n=20, save_to_csv=True)\n",
    "                \n",
    "            elif choice == '5':\n",
    "                print(\"\\n📊 월별 단어 빈도 분석 시작...\")\n",
    "                if not analyzer.use_morphology:\n",
    "                    print(\"⚠️ 형태소 분석 라이브러리가 없어서 정규식 기반으로 분석합니다.\")\n",
    "                else:\n",
    "                    print(\"🔤 형태소 분석 기반으로 분석합니다.\")\n",
    "                monthly_word_freq = analyzer.analyze_monthly_word_frequency(top_n=20)\n",
    "                \n",
    "            elif choice == '6':\n",
    "                print(\"\\n📈 주간별 단어 빈도 분석 시작...\")\n",
    "                if not analyzer.use_morphology:\n",
    "                    print(\"⚠️ 형태소 분석 라이브러리가 없어서 정규식 기반으로 분석합니다.\")\n",
    "                else:\n",
    "                    print(\"🔤 형태소 분석 기반으로 분석합니다.\")\n",
    "                weekly_word_freq = analyzer.analyze_weekly_word_frequency(top_n=15, save_to_csv=True)\n",
    "                \n",
    "            elif choice == '7':\n",
    "                print(\"\\n🔤 전체 단어 빈도 분석 시작...\")\n",
    "                \n",
    "                if not analyzer.use_morphology:\n",
    "                    print(\"⚠️ 형태소 분석 라이브러리가 없어서 정규식 기반으로 분석합니다.\")\n",
    "                else:\n",
    "                    print(\"🔤 형태소 분석 기반으로 분석합니다.\")\n",
    "                \n",
    "                # 필터링된 데이터가 있으면 사용, 없으면 전체 데이터 사용\n",
    "                use_filtered = analyzer.filtered_df is not None\n",
    "                if use_filtered:\n",
    "                    print(\"🧹 필터링된 데이터를 사용합니다.\")\n",
    "                else:\n",
    "                    print(\"📊 전체 데이터를 사용합니다.\")\n",
    "                    \n",
    "                word_freq = analyzer.analyze_text_frequency_advanced(\n",
    "                    top_n=30, \n",
    "                    use_filtered_data=use_filtered, \n",
    "                    use_morphology=analyzer.use_morphology,  # 형태소 분석 우선 사용\n",
    "                    save_to_csv=True\n",
    "                )\n",
    "                \n",
    "            elif choice == '8':\n",
    "                print(\"\\n📝 정규식 기반 단어 빈도 분석 시작...\")\n",
    "                print(\"🔍 정규식으로만 단어를 추출합니다 (형태소 분석 없음)\")\n",
    "                \n",
    "                # 필터링된 데이터가 있으면 사용, 없으면 전체 데이터 사용\n",
    "                use_filtered = analyzer.filtered_df is not None\n",
    "                if use_filtered:\n",
    "                    print(\"🧹 필터링된 데이터를 사용합니다.\")\n",
    "                else:\n",
    "                    print(\"📊 전체 데이터를 사용합니다.\")\n",
    "                    \n",
    "                word_freq = analyzer.analyze_text_frequency_advanced(\n",
    "                    top_n=30, \n",
    "                    use_filtered_data=use_filtered, \n",
    "                    use_morphology=False,  # 정규식만 사용\n",
    "                    save_to_csv=True\n",
    "                )\n",
    "                \n",
    "            elif choice == '9':\n",
    "                print(\"\\n💾 모든 분석 결과 Excel 저장 시작...\")\n",
    "                analyzer.save_all_analysis_to_excel()\n",
    "                \n",
    "            elif choice == '10':\n",
    "                print(\"\\n📋 종합 리포트 출력...\")\n",
    "                analyzer.generate_advanced_summary_report()\n",
    "                \n",
    "            elif choice == '11':\n",
    "                print(\"\\n📊 연도별 트렌드 그래프 그리기...\")\n",
    "                # 연도별 통계 다시 계산\n",
    "                yearly_stats = analyzer.processed_df.groupby('연도').agg({\n",
    "                    '번호': 'count',\n",
    "                    '조회수': ['sum', 'mean', 'max'],\n",
    "                    '댓글갯수': ['sum', 'mean']\n",
    "                }).round(2)\n",
    "                yearly_stats.columns = ['게시글수', '총조회수', '평균조회수', '최대조회수', '총댓글수', '평균댓글수']\n",
    "                yearly_stats = yearly_stats.reset_index()\n",
    "                analyzer.plot_yearly_trends(yearly_stats)\n",
    "                \n",
    "            elif choice == '12':\n",
    "                print(\"\\n📊 월별 트렌드 그래프 그리기...\")\n",
    "                # 월별 통계 다시 계산\n",
    "                monthly_stats = analyzer.processed_df.groupby('연월').agg({\n",
    "                    '번호': 'count',\n",
    "                    '조회수': ['sum', 'mean', 'max'],\n",
    "                    '댓글갯수': ['sum', 'mean']\n",
    "                }).round(2)\n",
    "                monthly_stats.columns = ['게시글수', '총조회수', '평균조회수', '최대조회수', '총댓글수', '평균댓글수']\n",
    "                monthly_stats = monthly_stats.reset_index()\n",
    "                monthly_stats['연월_str'] = monthly_stats['연월'].astype(str)\n",
    "                analyzer.plot_monthly_views(monthly_stats)\n",
    "                \n",
    "            elif choice == '13':\n",
    "                print(\"\\n📊 주간별 트렌드 그래프 그리기...\")\n",
    "                # 주간별 통계 다시 계산\n",
    "                weekly_stats = analyzer.processed_df.groupby('주차').agg({\n",
    "                    '번호': 'count',\n",
    "                    '조회수': ['sum', 'mean'],\n",
    "                    '댓글갯수': ['sum', 'mean']\n",
    "                }).round(2)\n",
    "                weekly_stats.columns = ['게시글수', '총조회수', '평균조회수', '총댓글수', '평균댓글수']\n",
    "                weekly_stats = weekly_stats.reset_index()\n",
    "                weekly_stats['주차_str'] = weekly_stats['주차'].astype(str)\n",
    "                analyzer.plot_weekly_trends(weekly_stats)\n",
    "                \n",
    "            elif choice == '14':\n",
    "                print(\"\\n📊 전체 단어 빈도 그래프 그리기...\")\n",
    "                \n",
    "                # 필터링된 데이터가 있으면 사용, 없으면 전체 데이터 사용\n",
    "                use_filtered = analyzer.filtered_df is not None\n",
    "                data_to_use = analyzer.filtered_df if use_filtered else analyzer.processed_df\n",
    "                data_desc = \"필터링 후\" if use_filtered else \"전체\"\n",
    "                \n",
    "                if not analyzer.use_morphology:\n",
    "                    print(\"⚠️ 형태소 분석 라이브러리가 없어서 정규식 기반으로 분석합니다.\")\n",
    "                    use_morphology = False\n",
    "                else:\n",
    "                    print(\"🔤 형태소 분석 기반으로 분석합니다.\")\n",
    "                    use_morphology = True\n",
    "                \n",
    "                print(f\"📊 {data_desc} 데이터를 사용합니다.\")\n",
    "                \n",
    "                # 단어 빈도 계산\n",
    "                all_text = (data_to_use['제목'] + ' ' + data_to_use['내용']).str.cat(sep=' ')\n",
    "                final_words = analyzer._get_filtered_words_advanced(all_text, use_morphology)\n",
    "                word_freq = Counter(final_words)\n",
    "                top_words = word_freq.most_common(30)\n",
    "                \n",
    "                analyzer.plot_word_frequency(top_words, data_desc, use_morphology)\n",
    "                \n",
    "            else:\n",
    "                print(\"❌ 잘못된 선택입니다. 0-14 사이의 숫자를 입력해주세요.\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n👋 사용자가 프로그램을 중단했습니다.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 오류가 발생했습니다: {e}\")\n",
    "\n",
    "\n",
    "def save_all_analysis_to_excel(self):\n",
    "    \"\"\"모든 분석 결과를 하나의 Excel 파일에 저장\"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        from openpyxl import Workbook\n",
    "        from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "        from openpyxl.styles import Font, PatternFill, Alignment\n",
    "        \n",
    "        print(\"📊 모든 분석 결과를 Excel 파일로 저장 중...\")\n",
    "        \n",
    "        # Excel 파일 생성\n",
    "        wb = Workbook()\n",
    "        wb.remove(wb.active)  # 기본 시트 제거\n",
    "        \n",
    "        # 1. 연도별 트렌드 분석\n",
    "        if self.processed_df is not None:\n",
    "            yearly_stats = self.processed_df.groupby('연도').agg({\n",
    "                '번호': 'count',\n",
    "                '조회수': ['sum', 'mean', 'max'],\n",
    "                '댓글갯수': ['sum', 'mean']\n",
    "            }).round(2)\n",
    "            yearly_stats.columns = ['게시글수', '총조회수', '평균조회수', '최대조회수', '총댓글수', '평균댓글수']\n",
    "            yearly_stats = yearly_stats.reset_index()\n",
    "            \n",
    "            ws1 = wb.create_sheet(\"연도별_트렌드\")\n",
    "            for row in dataframe_to_rows(yearly_stats, index=False, header=True):\n",
    "                ws1.append(row)\n",
    "            \n",
    "            # 헤더 스타일링\n",
    "            for cell in ws1[1]:\n",
    "                cell.font = Font(bold=True)\n",
    "                cell.fill = PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n",
    "        \n",
    "        # 2. 전체 단어 빈도 분석\n",
    "        all_text = (self.processed_df['제목'] + ' ' + self.processed_df['내용']).str.cat(sep=' ')\n",
    "        final_words = self._get_filtered_words_advanced(all_text, use_morphology=self.use_morphology)\n",
    "        word_freq = Counter(final_words)\n",
    "        top_words = word_freq.most_common(50)\n",
    "        \n",
    "        word_freq_data = []\n",
    "        for i, (word, count) in enumerate(top_words, 1):\n",
    "            word_freq_data.append({'순위': i, '단어': word, '빈도수': count})\n",
    "        \n",
    "        word_freq_df = pd.DataFrame(word_freq_data)\n",
    "        ws2 = wb.create_sheet(\"전체_단어빈도\")\n",
    "        for row in dataframe_to_rows(word_freq_df, index=False, header=True):\n",
    "            ws2.append(row)\n",
    "        \n",
    "        # 헤더 스타일링\n",
    "        for cell in ws2[1]:\n",
    "            cell.font = Font(bold=True)\n",
    "            cell.fill = PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n",
    "        \n",
    "        # 3. 연도별 단어 빈도 분석\n",
    "        yearly_word_data = []\n",
    "        for year in sorted(self.processed_df['연도'].unique()):\n",
    "            year_data = self.processed_df[self.processed_df['연도'] == year]\n",
    "            year_text = (year_data['제목'] + ' ' + year_data['내용']).str.cat(sep=' ')\n",
    "            year_words = self._get_filtered_words_advanced(year_text, use_morphology=self.use_morphology)\n",
    "            year_word_freq = Counter(year_words)\n",
    "            top_year_words = year_word_freq.most_common(20)\n",
    "            \n",
    "            for i, (word, count) in enumerate(top_year_words, 1):\n",
    "                yearly_word_data.append({\n",
    "                    '연도': year,\n",
    "                    '순위': i,\n",
    "                    '단어': word,\n",
    "                    '빈도수': count\n",
    "                })\n",
    "        \n",
    "        yearly_word_df = pd.DataFrame(yearly_word_data)\n",
    "        ws3 = wb.create_sheet(\"연도별_단어빈도\")\n",
    "        for row in dataframe_to_rows(yearly_word_df, index=False, header=True):\n",
    "            ws3.append(row)\n",
    "        \n",
    "        # 헤더 스타일링\n",
    "        for cell in ws3[1]:\n",
    "            cell.font = Font(bold=True)\n",
    "            cell.fill = PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n",
    "        \n",
    "        # 4. 중복 제거 결과 (있다면)\n",
    "        if self.filtered_df is not None:\n",
    "            duplicate_stats = pd.DataFrame({\n",
    "                '항목': ['원본 게시글 수', '중복 제거 후', '제거된 중복 수', '중복 비율(%)'],\n",
    "                '값': [\n",
    "                    len(self.processed_df),\n",
    "                    len(self.filtered_df),\n",
    "                    len(self.processed_df) - len(self.filtered_df),\n",
    "                    round((len(self.processed_df) - len(self.filtered_df)) / len(self.processed_df) * 100, 2)\n",
    "                ]\n",
    "            })\n",
    "            \n",
    "            ws4 = wb.create_sheet(\"중복제거_통계\")\n",
    "            for row in dataframe_to_rows(duplicate_stats, index=False, header=True):\n",
    "                ws4.append(row)\n",
    "            \n",
    "            # 헤더 스타일링\n",
    "            for cell in ws4[1]:\n",
    "                cell.font = Font(bold=True)\n",
    "                cell.fill = PatternFill(start_color=\"CCCCCC\", end_color=\"CCCCCC\", fill_type=\"solid\")\n",
    "        \n",
    "        # 파일 저장\n",
    "        filename = 'community_analysis_complete.xlsx'\n",
    "        wb.save(filename)\n",
    "        print(f\"✅ 모든 분석 결과가 '{filename}'에 저장되었습니다!\")\n",
    "        print(f\"   📋 포함된 시트: {', '.join([ws.title for ws in wb.worksheets])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Excel 저장 중 오류 발생: {e}\")\n",
    "        print(\"📥 openpyxl 라이브러리가 필요합니다: pip install openpyxl\")\n",
    "\n",
    "\n",
    "def generate_advanced_summary_report(self):\n",
    "    \"\"\"개선된 종합 분석 리포트\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"     개선된 커뮤니티 데이터 분석 리포트 (욕설 필터링 포함)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if self.processed_df is not None:\n",
    "        print(\"📊 원본 데이터 통계\")\n",
    "        total_posts = len(self.processed_df)\n",
    "        print(f\"   총 게시글 수: {total_posts:,}개\")\n",
    "        print(f\"   총 조회수: {self.processed_df['조회수'].sum():,}회\")\n",
    "        print(f\"   평균 조회수: {self.processed_df['조회수'].mean():.1f}회\")\n",
    "    \n",
    "    if self.filtered_df is not None:\n",
    "        print(\"\\n🧹 필터링 후 데이터 통계\")\n",
    "        filtered_posts = len(self.filtered_df)\n",
    "        removed_posts = total_posts - filtered_posts\n",
    "        print(f\"   필터링 후 게시글 수: {filtered_posts:,}개\")\n",
    "        print(f\"   제거된 게시글 수: {removed_posts:,}개 ({removed_posts/total_posts*100:.1f}%)\")\n",
    "        print(f\"   필터링 후 총 조회수: {self.filtered_df['조회수'].sum():,}회\")\n",
    "        print(f\"   필터링 후 평균 조회수: {self.filtered_df['조회수'].mean():.1f}회\")\n",
    "    \n",
    "    if self.use_morphology:\n",
    "        print(f\"\\n🔤 형태소 분석: ✅ 사용 가능 (KoNLPy)\")\n",
    "    else:\n",
    "        print(f\"\\n🔤 형태소 분석: ❌ 사용 불가 (정규식 사용)\")\n",
    "    \n",
    "    print(f\"\\n🚫 욕설 필터링: ✅ 적용됨 (패턴 매칭 + 사전 기반)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# 클래스에 메서드 추가\n",
    "AdvancedCommunityDataAnalyzer.save_all_analysis_to_excel = save_all_analysis_to_excel\n",
    "AdvancedCommunityDataAnalyzer.generate_advanced_summary_report = generate_advanced_summary_report\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 개선된 커뮤니티 데이터 분석기\")\n",
    "    print(\"💡 팁: 먼저 메뉴 1번으로 데이터를 로드하고 전처리한 후 다른 기능을 사용하세요!\")\n",
    "    main_menu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
