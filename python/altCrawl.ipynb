{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c03f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.2 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/109.0',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 16_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.2 Mobile/15E148 Safari/604.1'\n",
    "]\n",
    "\n",
    "BASE_FILENAME = 'alt'\n",
    "BASE_URL = 'https://gall.dcinside.com/mgallery/board/lists/?id=coin'\n",
    "VIEW_URL = 'https://gall.dcinside.com/mgallery/board/view/'\n",
    "TOTAL_POSTS = 100000\n",
    "SAVE_INTERVAL = 5000\n",
    "post_id_cache = set()\n",
    "collected_posts = 0\n",
    "start_page = 15\n",
    "end_page = 570\n",
    "MAX_WORKERS = 10\n",
    "MAX_RETRIES = 3\n",
    "MIN_DELAY = 0.1\n",
    "MAX_DELAY = 0.5\n",
    "session = requests.Session()\n",
    "\n",
    "def random_delay():\n",
    "    time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))\n",
    "\n",
    "def get_headers():\n",
    "    return {\n",
    "        'User-Agent': random.choice(user_agents),\n",
    "        'Referer': BASE_URL,\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'max-age=0'\n",
    "    }\n",
    "\n",
    "def save_to_csv(dataframe, count):\n",
    "    filename = f\"{BASE_FILENAME}.csv\" \n",
    "    dataframe.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"{filename} 파일에 {len(dataframe)}개 게시물 저장 완료\")\n",
    "\n",
    "def crawl_post_detail(post_info):\n",
    "    post_url, post_id = post_info\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            if attempt > 0:\n",
    "                time.sleep(random.uniform(0.5, 1.0))\n",
    "            headers = get_headers()\n",
    "            response = session.get(post_url, headers=headers, timeout=5)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            content_div = soup.select_one('div.write_div')\n",
    "            content = content_div.text.strip() if content_div else \"내용 없음\"\n",
    "            view_count = soup.select_one('span.gall_count')\n",
    "            view_count = view_count.text.replace('조회 ', '').strip() if view_count else \"0\"\n",
    "            comment_count = soup.select_one('span.gall_reply_num')\n",
    "            comment_count = comment_count.text.replace('댓글 ', '').strip() if comment_count else \"0\"\n",
    "            view_count = re.sub(r'[^0-9]', '', view_count)\n",
    "            comment_count = re.sub(r'[^0-9]', '', comment_count)\n",
    "            return {\n",
    "                'post_id': post_id,\n",
    "                '내용': content,\n",
    "                '조회수': view_count,\n",
    "                '댓글갯수': comment_count\n",
    "            }\n",
    "        except Exception:\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "def calculate_sampling_interval():\n",
    "    return 7 \n",
    "\n",
    "def extract_post_info(page):\n",
    "    for retry in range(MAX_RETRIES):\n",
    "        try:\n",
    "            headers = get_headers()\n",
    "            page_url = f\"{BASE_URL}&page={page}&list_num=100&sort_type=N\"\n",
    "            response = session.get(page_url, headers=headers, timeout=5)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            posts = soup.select('tr.ub-content')\n",
    "            post_info_list = []\n",
    "            for post in posts:\n",
    "                if 'notice' in post.get('class', []):\n",
    "                    continue\n",
    "                post_id_elem = post.select_one('td.gall_num')\n",
    "                post_id = post_id_elem.text.strip() if post_id_elem else \"0\"\n",
    "                if post_id in post_id_cache:\n",
    "                    continue\n",
    "                post_title_elem = post.select_one('td.gall_tit a')\n",
    "                title = post_title_elem.text.strip() if post_title_elem else \"제목 없음\"\n",
    "                post_date_elem = post.select_one('td.gall_date')\n",
    "                date = post_date_elem.text.strip() if post_date_elem else \"\"\n",
    "                post_detail_url = None\n",
    "                if post_title_elem and post_title_elem.get('href'):\n",
    "                    post_detail_url = post_title_elem['href']\n",
    "                    if not post_detail_url.startswith('http'):\n",
    "                        post_detail_url = \"https://gall.dcinside.com\" + post_detail_url\n",
    "                if post_detail_url:\n",
    "                    post_info_list.append({\n",
    "                        'post_id': post_id,\n",
    "                        'title': title,\n",
    "                        'date': date,\n",
    "                        'url': post_detail_url\n",
    "                    })\n",
    "            return post_info_list\n",
    "        except Exception:\n",
    "            if retry == MAX_RETRIES - 1:\n",
    "                return []\n",
    "            time.sleep(random.uniform(1.0, 2.0))\n",
    "    return []\n",
    "\n",
    "def main():\n",
    "    global collected_posts, post_id_cache\n",
    "    all_data = []\n",
    "    existing_files = [f for f in os.listdir('.') if f.startswith(BASE_FILENAME) and f.endswith('.csv')]\n",
    "    if existing_files:\n",
    "        latest_file = sorted(existing_files)[-1]\n",
    "        temp_df = pd.read_csv(latest_file, encoding='utf-8-sig')\n",
    "        collected_posts = len(temp_df)\n",
    "        all_data = temp_df.to_dict('records')\n",
    "        for post_id in temp_df['번호']:\n",
    "            post_id_cache.add(str(post_id))\n",
    "    sampling_interval = calculate_sampling_interval()\n",
    "    print(f\"샘플링 간격: {sampling_interval}개마다 1개 수집\")\n",
    "    current_batch_size = collected_posts % SAVE_INTERVAL\n",
    "    page_ranges = list(range(start_page, end_page + 1))\n",
    "    start_time = time.time()\n",
    "    posts_at_start = collected_posts\n",
    "    try:\n",
    "        with tqdm(total=TOTAL_POSTS, initial=collected_posts, desc=\"게시물 크롤링 진행\") as pbar:\n",
    "            for page_chunk_idx in range(0, len(page_ranges), 5):\n",
    "                if collected_posts >= TOTAL_POSTS:\n",
    "                    break\n",
    "                current_pages = page_ranges[page_chunk_idx:page_chunk_idx + 5]\n",
    "                all_posts_info = []\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "                    results = list(executor.map(extract_post_info, current_pages))\n",
    "                for posts_in_page in results:\n",
    "                    all_posts_info.extend(posts_in_page)\n",
    "                selected_posts = []\n",
    "                for idx, post_info in enumerate(all_posts_info):\n",
    "                    if (collected_posts + idx) % sampling_interval == 0:\n",
    "                        selected_posts.append(post_info)\n",
    "                        post_id_cache.add(post_info['post_id'])\n",
    "                post_details_to_crawl = [(post['url'], post['post_id']) for post in selected_posts]\n",
    "                crawled_details = []\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                    future_to_post = {executor.submit(crawl_post_detail, post_detail): post_detail for post_detail in post_details_to_crawl}\n",
    "                    for future in concurrent.futures.as_completed(future_to_post):\n",
    "                        result = future.result()\n",
    "                        if result:\n",
    "                            crawled_details.append(result)\n",
    "                for detail in crawled_details:\n",
    "                    matching_post = next((post for post in selected_posts if post['post_id'] == detail['post_id']), None)\n",
    "                    if matching_post:\n",
    "                        new_row = {\n",
    "                            '번호': matching_post['post_id'],\n",
    "                            '날짜': matching_post['date'],\n",
    "                            '제목': matching_post['title'],\n",
    "                            '내용': detail['내용'],\n",
    "                            '조회수': detail['조회수'],\n",
    "                            '댓글갯수': detail['댓글갯수']\n",
    "                        }\n",
    "                        all_data.append(new_row)\n",
    "                        collected_posts += 1\n",
    "                        current_batch_size += 1\n",
    "                        pbar.update(1)\n",
    "                        if current_batch_size >= SAVE_INTERVAL:\n",
    "                            df = pd.DataFrame(all_data)\n",
    "                            save_to_csv(df, collected_posts)\n",
    "                            current_batch_size = 0\n",
    "                current_time = time.time()\n",
    "                elapsed_time = current_time - start_time\n",
    "                if elapsed_time > 0:\n",
    "                    posts_collected = collected_posts - posts_at_start\n",
    "                    posts_per_minute = (posts_collected / elapsed_time) * 60\n",
    "                    print(f\"현재 크롤링 속도: {posts_per_minute:.2f} 게시물/분 (총 {posts_collected}개 수집, {elapsed_time:.1f}초 소요)\")\n",
    "                time.sleep(0.5)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"사용자에 의해 크롤링이 중단되었습니다.\")\n",
    "    finally:\n",
    "        if all_data and current_batch_size > 0:\n",
    "            df = pd.DataFrame(all_data)\n",
    "            save_to_csv(df, collected_posts)\n",
    "        total_time = time.tㅁime() - start_time\n",
    "        posts_collected = collected_posts - posts_at_start\n",
    "        if total_time > 0:\n",
    "            posts_per_minute = (posts_collected / total_time) * 60\n",
    "            print(f\"평균 크롤링 속도: {posts_per_minute:.2f} 게시물/분\")\n",
    "            print(f\"총 소요 시간: {total_time/60:.2f}분 ({total_time:.1f}초)\")\n",
    "        print(f\"크롤링 완료: 총 {collected_posts}개 게시물 수집\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
