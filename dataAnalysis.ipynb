{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8706ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\chan5\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\chan5\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\chan5\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: koreanize-matplotlib in c:\\users\\chan5\\anaconda3\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: konlpy in c:\\users\\chan5\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from konlpy) (5.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib koreanize-matplotlib konlpy JPype1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f081ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… KoNLPy ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
      "ğŸš€ ê°œì„ ëœ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ë¶„ì„ê¸°\n",
      "==================================================\n",
      "1. run_advanced_analysis() - ì „ì²´ ë¶„ì„\n",
      "2. quick_exact_duplicate_filter() - ì™„ì „ ë™ì¼ ì¤‘ë³µë§Œ í•„í„°ë§\n",
      "3. morphology_analysis_only() - í˜•íƒœì†Œ ë¶„ì„ë§Œ\n",
      "==================================================\n",
      "ğŸ’¡ ì™„ì „íˆ ë˜‘ê°™ì€ ê¸€ë§Œ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ê°€ì¥ ë³´ìˆ˜ì )\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# 1ë‹¨ê³„: ë¼ì´ë¸ŒëŸ¬ë¦¬ import ë° ì„¤ì •\n",
    "# ===================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import koreanize_matplotlib\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„¤ì¹˜ í•„ìš”)\n",
    "try:\n",
    "    from konlpy.tag import Okt, Mecab, Kkma\n",
    "    KONLPY_AVAILABLE = True\n",
    "    print(\"âœ… KoNLPy ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "except ImportError:\n",
    "    KONLPY_AVAILABLE = False\n",
    "    print(\"âš ï¸ KoNLPy ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "    print(\"ì„¤ì¹˜ ë°©ë²•: !pip install konlpy\")\n",
    "\n",
    "# ===================================\n",
    "# 2ë‹¨ê³„: ë¶„ì„ê¸° í´ë˜ìŠ¤ ì •ì˜\n",
    "# ===================================\n",
    "\n",
    "class AdvancedCommunityDataAnalyzer:\n",
    "    def __init__(self, csv_file_path):\n",
    "        \"\"\"\n",
    "        ê°œì„ ëœ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "        \n",
    "        Args:\n",
    "            csv_file_path (str): CSV íŒŒì¼ ê²½ë¡œ\n",
    "        \"\"\"\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.df = None\n",
    "        self.processed_df = None\n",
    "        self.filtered_df = None  # ë„ë°° í•„í„°ë§ëœ ë°ì´í„°\n",
    "        \n",
    "        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "        if KONLPY_AVAILABLE:\n",
    "            try:\n",
    "                self.okt = Okt()\n",
    "                self.morphology_analyzer = self.okt\n",
    "                self.use_morphology = True\n",
    "                print(\"âœ… Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "            except:\n",
    "                self.use_morphology = False\n",
    "                print(\"âš ï¸ í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨. ê¸°ë³¸ ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            self.use_morphology = False\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê¸°ë³¸ ì •ë³´ë¥¼ ì¶œë ¥\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.csv_file_path, encoding='utf-8')\n",
    "            print(\"=== ë°ì´í„° ë¡œë“œ ì™„ë£Œ ===\")\n",
    "            print(f\"ì´ ë°ì´í„° ìˆ˜: {len(self.df):,}ê°œ\")\n",
    "            print(f\"ì»¬ëŸ¼: {list(self.df.columns)}\")\n",
    "            print(\"\\n=== ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ===\")\n",
    "            print(self.df.head())\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
    "        if self.df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•´ì£¼ì„¸ìš”.\")\n",
    "            return False\n",
    "        \n",
    "        # ë°ì´í„° ë³µì‚¬\n",
    "        self.processed_df = self.df.copy()\n",
    "        \n",
    "        # ë‚ ì§œ ë³€í™˜ í•¨ìˆ˜\n",
    "        def convert_date_pandas(date_str):\n",
    "            if pd.isna(date_str):\n",
    "                return pd.NaT\n",
    "            \n",
    "            date_str = str(date_str)\n",
    "            try:\n",
    "                if '.' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%y.%m.%d')\n",
    "                elif '/' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%y/%m/%d')\n",
    "                else:\n",
    "                    return pd.NaT\n",
    "            except:\n",
    "                return pd.NaT\n",
    "        \n",
    "        # ë‚ ì§œ ë³€í™˜\n",
    "        self.processed_df['ë‚ ì§œ_ë³€í™˜'] = self.processed_df['ë‚ ì§œ'].apply(convert_date_pandas)\n",
    "        self.processed_df = self.processed_df.dropna(subset=['ë‚ ì§œ_ë³€í™˜'])\n",
    "        \n",
    "        # ì—°ë„, ì—°ì›”, ì£¼ì°¨ ì»¬ëŸ¼ ì¶”ê°€\n",
    "        self.processed_df['ì—°ë„'] = self.processed_df['ë‚ ì§œ_ë³€í™˜'].dt.year\n",
    "        self.processed_df['ì—°ì›”'] = self.processed_df['ë‚ ì§œ_ë³€í™˜'].dt.to_period('M')\n",
    "        self.processed_df['ì£¼ì°¨'] = self.processed_df['ë‚ ì§œ_ë³€í™˜'].dt.to_period('W')\n",
    "        \n",
    "        # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜\n",
    "        def safe_convert_to_numeric(x):\n",
    "            try:\n",
    "                if pd.isna(x) or x == '' or x == 'NaN':\n",
    "                    return 0\n",
    "                if isinstance(x, str):\n",
    "                    clean_num = ''.join(filter(str.isdigit, str(x)))\n",
    "                    return int(clean_num) if clean_num else 0\n",
    "                return int(x)\n",
    "            except:\n",
    "                return 0\n",
    "        \n",
    "        self.processed_df['ì¡°íšŒìˆ˜'] = self.processed_df['ì¡°íšŒìˆ˜'].apply(safe_convert_to_numeric)\n",
    "        self.processed_df['ëŒ“ê¸€ê°¯ìˆ˜'] = self.processed_df['ëŒ“ê¸€ê°¯ìˆ˜'].apply(safe_convert_to_numeric)\n",
    "        \n",
    "        # ê²°ì¸¡ê°’ ì²˜ë¦¬\n",
    "        self.processed_df['ì œëª©'] = self.processed_df['ì œëª©'].fillna(\"\")\n",
    "        self.processed_df['ë‚´ìš©'] = self.processed_df['ë‚´ìš©'].fillna(\"\")\n",
    "        \n",
    "        print(\"=== ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ ===\")\n",
    "        print(f\"ì²˜ë¦¬ëœ ë°ì´í„° ìˆ˜: {len(self.processed_df):,}ê°œ\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def calculate_text_similarity(self, text1, text2):\n",
    "        \"\"\"ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0~1)\"\"\"\n",
    "        return SequenceMatcher(None, text1, text2).ratio()\n",
    "    \n",
    "    def detect_spam_posts_exact_only(self, use_hashing=True):\n",
    "        \"\"\"\n",
    "        ì™„ì „íˆ ë˜‘ê°™ì€ ê¸€ë§Œ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬ (ê°€ì¥ ë³´ìˆ˜ì )\n",
    "        \n",
    "        Args:\n",
    "            use_hashing (bool): í•´ì‹œ ê¸°ë°˜ ë¹ ë¥¸ í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=== ì™„ì „ ë™ì¼í•œ ì¤‘ë³µ ê²Œì‹œê¸€ë§Œ íƒì§€ ===\")\n",
    "        print(f\"ğŸ“Š ì´ {len(self.processed_df):,}ê°œ ê²Œì‹œê¸€ ë¶„ì„\")\n",
    "        print(\"ğŸ” ê¸€ì í•˜ë‚˜ë¼ë„ ë‹¤ë¥´ë©´ ë‹¤ë¥¸ ê¸€ë¡œ ì¸ì •í•©ë‹ˆë‹¤\")\n",
    "        \n",
    "        spam_indices = set()\n",
    "        \n",
    "        if use_hashing:\n",
    "            # í•´ì‹œ ê¸°ë°˜ ì™„ì „ ì¤‘ë³µ íƒì§€ë§Œ ì‚¬ìš©\n",
    "            print(\"ğŸš€ í•´ì‹œ ê¸°ë°˜ ì™„ì „ ì¤‘ë³µ íƒì§€...\")\n",
    "            title_hashes = {}\n",
    "            content_hashes = {}\n",
    "            combined_hashes = {}  # ì œëª©+ë‚´ìš© í†µí•© í•´ì‹œ\n",
    "            \n",
    "            for idx, row in self.processed_df.iterrows():\n",
    "                # ê³µë°± ì •ê·œí™” (ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ)\n",
    "                title_clean = re.sub(r'\\s+', ' ', str(row['ì œëª©']).strip())\n",
    "                content_clean = re.sub(r'\\s+', ' ', str(row['ë‚´ìš©']).strip())\n",
    "                combined_clean = title_clean + \" | \" + content_clean\n",
    "                \n",
    "                # 1. ì œëª© ì™„ì „ ì¤‘ë³µ ì²´í¬\n",
    "                title_hash = hash(title_clean.lower())\n",
    "                if title_hash in title_hashes:\n",
    "                    spam_indices.add(idx)\n",
    "                else:\n",
    "                    title_hashes[title_hash] = idx\n",
    "                \n",
    "                # 2. ë‚´ìš© ì™„ì „ ì¤‘ë³µ ì²´í¬ (3ê¸€ì ì´ìƒì¸ ê²½ìš°ë§Œ)\n",
    "                if len(content_clean) >= 3:\n",
    "                    content_hash = hash(content_clean.lower())\n",
    "                    if content_hash in content_hashes:\n",
    "                        spam_indices.add(idx)\n",
    "                    else:\n",
    "                        content_hashes[content_hash] = idx\n",
    "                \n",
    "                # 3. ì œëª©+ë‚´ìš© í†µí•© ì™„ì „ ì¤‘ë³µ ì²´í¬\n",
    "                combined_hash = hash(combined_clean.lower())\n",
    "                if combined_hash in combined_hashes:\n",
    "                    spam_indices.add(idx)\n",
    "                else:\n",
    "                    combined_hashes[combined_hash] = idx\n",
    "            \n",
    "            print(f\"   ì´ ì¤‘ë³µ ë°œê²¬: {len(spam_indices):,}ê°œ\")\n",
    "        \n",
    "        # ì¤‘ë³µì´ ì•„ë‹Œ ê²Œì‹œê¸€ë§Œ í•„í„°ë§\n",
    "        self.filtered_df = self.processed_df[~self.processed_df.index.isin(spam_indices)].copy()\n",
    "        \n",
    "        print(f\"\\n=== ì™„ì „ ë™ì¼ ì¤‘ë³µ í•„í„°ë§ ê²°ê³¼ ===\")\n",
    "        print(f\"ì›ë³¸ ê²Œì‹œê¸€ ìˆ˜: {len(self.processed_df):,}ê°œ\")\n",
    "        print(f\"ì™„ì „ ë™ì¼ ì¤‘ë³µ ìˆ˜: {len(spam_indices):,}ê°œ ({len(spam_indices)/len(self.processed_df)*100:.1f}%)\")\n",
    "        print(f\"í•„í„°ë§ í›„ ê²Œì‹œê¸€ ìˆ˜: {len(self.filtered_df):,}ê°œ\")\n",
    "        print(f\"âœ… ê¸€ì í•˜ë‚˜ë¼ë„ ë‹¤ë¥´ë©´ ë³„ê°œ ê²Œì‹œê¸€ë¡œ ìœ ì§€ë©ë‹ˆë‹¤!\")\n",
    "        \n",
    "        return spam_indices\n",
    "    \n",
    "    def analyze_yearly_trends(self):\n",
    "        \"\"\"ì—°ë„ë³„ íŠ¸ë Œë“œ ë¶„ì„\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        # ì—°ë„ë³„ í†µê³„ ê³„ì‚°\n",
    "        yearly_stats = self.processed_df.groupby('ì—°ë„').agg({\n",
    "            'ë²ˆí˜¸': 'count',\n",
    "            'ì¡°íšŒìˆ˜': ['sum', 'mean', 'max'],\n",
    "            'ëŒ“ê¸€ê°¯ìˆ˜': ['sum', 'mean']\n",
    "        }).round(2)\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… ì •ë¦¬\n",
    "        yearly_stats.columns = ['ê²Œì‹œê¸€ìˆ˜', 'ì´ì¡°íšŒìˆ˜', 'í‰ê· ì¡°íšŒìˆ˜', 'ìµœëŒ€ì¡°íšŒìˆ˜', 'ì´ëŒ“ê¸€ìˆ˜', 'í‰ê· ëŒ“ê¸€ìˆ˜']\n",
    "        yearly_stats = yearly_stats.reset_index()\n",
    "        \n",
    "        print(\"=== ì—°ë„ë³„ í†µê³„ ===\")\n",
    "        for _, row in yearly_stats.iterrows():\n",
    "            print(f\"{int(row['ì—°ë„'])}: ê²Œì‹œê¸€ {row['ê²Œì‹œê¸€ìˆ˜']:,}ê°œ, \"\n",
    "                  f\"ì´ì¡°íšŒìˆ˜ {row['ì´ì¡°íšŒìˆ˜']:,.0f}, í‰ê· ì¡°íšŒìˆ˜ {row['í‰ê· ì¡°íšŒìˆ˜']:.1f}\")\n",
    "        \n",
    "        return yearly_stats\n",
    "    \n",
    "    def analyze_monthly_trends(self):\n",
    "        \"\"\"ì›”ë³„ íŠ¸ë Œë“œ ë¶„ì„\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        # ì›”ë³„ í†µê³„ ê³„ì‚°\n",
    "        monthly_stats = self.processed_df.groupby('ì—°ì›”').agg({\n",
    "            'ë²ˆí˜¸': 'count',\n",
    "            'ì¡°íšŒìˆ˜': ['sum', 'mean', 'max'],\n",
    "            'ëŒ“ê¸€ê°¯ìˆ˜': ['sum', 'mean']\n",
    "        }).round(2)\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… ì •ë¦¬\n",
    "        monthly_stats.columns = ['ê²Œì‹œê¸€ìˆ˜', 'ì´ì¡°íšŒìˆ˜', 'í‰ê· ì¡°íšŒìˆ˜', 'ìµœëŒ€ì¡°íšŒìˆ˜', 'ì´ëŒ“ê¸€ìˆ˜', 'í‰ê· ëŒ“ê¸€ìˆ˜']\n",
    "        monthly_stats = monthly_stats.reset_index()\n",
    "        monthly_stats['ì—°ì›”_str'] = monthly_stats['ì—°ì›”'].astype(str)\n",
    "        \n",
    "        print(\"=== ì›”ë³„ í†µê³„ ===\")\n",
    "        for _, row in monthly_stats.iterrows():\n",
    "            print(f\"{row['ì—°ì›”_str']}: ê²Œì‹œê¸€ {row['ê²Œì‹œê¸€ìˆ˜']:,}ê°œ, \"\n",
    "                  f\"ì´ì¡°íšŒìˆ˜ {row['ì´ì¡°íšŒìˆ˜']:,.0f}, í‰ê· ì¡°íšŒìˆ˜ {row['í‰ê· ì¡°íšŒìˆ˜']:.1f}\")\n",
    "        \n",
    "        return monthly_stats\n",
    "    \n",
    "    def analyze_weekly_trends(self):\n",
    "        \"\"\"ì£¼ê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„ (ì „ì²´ ê¸°ê°„)\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        # ì£¼ê°„ë³„ í†µê³„ ê³„ì‚° (ì „ì²´ ê¸°ê°„)\n",
    "        weekly_stats = self.processed_df.groupby('ì£¼ì°¨').agg({\n",
    "            'ë²ˆí˜¸': 'count',\n",
    "            'ì¡°íšŒìˆ˜': ['sum', 'mean'],\n",
    "            'ëŒ“ê¸€ê°¯ìˆ˜': ['sum', 'mean']\n",
    "        }).round(2)\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… ì •ë¦¬\n",
    "        weekly_stats.columns = ['ê²Œì‹œê¸€ìˆ˜', 'ì´ì¡°íšŒìˆ˜', 'í‰ê· ì¡°íšŒìˆ˜', 'ì´ëŒ“ê¸€ìˆ˜', 'í‰ê· ëŒ“ê¸€ìˆ˜']\n",
    "        weekly_stats = weekly_stats.reset_index()\n",
    "        weekly_stats['ì£¼ì°¨_str'] = weekly_stats['ì£¼ì°¨'].astype(str)\n",
    "        \n",
    "        print(f\"=== ì£¼ê°„ë³„ í†µê³„ (ì „ì²´ {len(weekly_stats)}ì£¼) ===\")\n",
    "        print(f\"ì²« ì£¼ì°¨: {weekly_stats['ì£¼ì°¨_str'].iloc[0]}\")\n",
    "        print(f\"ë§ˆì§€ë§‰ ì£¼ì°¨: {weekly_stats['ì£¼ì°¨_str'].iloc[-1]}\")\n",
    "        print(f\"ì£¼ê°„ í‰ê·  ê²Œì‹œê¸€ìˆ˜: {weekly_stats['ê²Œì‹œê¸€ìˆ˜'].mean():.1f}ê°œ\")\n",
    "        print(f\"ì£¼ê°„ ìµœëŒ€ ê²Œì‹œê¸€ìˆ˜: {weekly_stats['ê²Œì‹œê¸€ìˆ˜'].max()}ê°œ\")\n",
    "        print(f\"ì£¼ê°„ í‰ê·  ì´ì¡°íšŒìˆ˜: {weekly_stats['ì´ì¡°íšŒìˆ˜'].mean():.0f}\")\n",
    "        \n",
    "        return weekly_stats\n",
    "    \n",
    "    def plot_yearly_trends(self, yearly_stats):\n",
    "        \"\"\"ì—°ë„ë³„ íŠ¸ë Œë“œ ì°¨íŠ¸ ê·¸ë¦¬ê¸°\"\"\"\n",
    "        if yearly_stats is None:\n",
    "            print(\"ì—°ë„ë³„ í†µê³„ë¥¼ ë¨¼ì € ê³„ì‚°í•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        \n",
    "        # ì—°ë„ë³„ ê²Œì‹œê¸€ ìˆ˜\n",
    "        ax1.bar(yearly_stats['ì—°ë„'], yearly_stats['ê²Œì‹œê¸€ìˆ˜'], \n",
    "                color='#58D68D', alpha=0.8, width=0.6)\n",
    "        ax1.set_title('ì—°ë„ë³„ ê²Œì‹œê¸€ ìˆ˜', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('ì—°ë„', fontsize=12)\n",
    "        ax1.set_ylabel('ê²Œì‹œê¸€ ìˆ˜', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # ê°’ í‘œì‹œ\n",
    "        for i, v in enumerate(yearly_stats['ê²Œì‹œê¸€ìˆ˜']):\n",
    "            ax1.annotate(f'{v:,.0f}', (yearly_stats['ì—°ë„'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # ì—°ë„ë³„ ì´ ì¡°íšŒìˆ˜\n",
    "        ax2.plot(yearly_stats['ì—°ë„'], yearly_stats['ì´ì¡°íšŒìˆ˜'], \n",
    "                marker='o', linewidth=3, markersize=8, color='#2E86C1')\n",
    "        ax2.set_title('ì—°ë„ë³„ ì´ ì¡°íšŒìˆ˜', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('ì—°ë„', fontsize=12)\n",
    "        ax2.set_ylabel('ì´ ì¡°íšŒìˆ˜', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ê°’ í‘œì‹œ\n",
    "        for i, v in enumerate(yearly_stats['ì´ì¡°íšŒìˆ˜']):\n",
    "            ax2.annotate(f'{v:,.0f}', (yearly_stats['ì—°ë„'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # ì—°ë„ë³„ í‰ê·  ì¡°íšŒìˆ˜\n",
    "        ax3.plot(yearly_stats['ì—°ë„'], yearly_stats['í‰ê· ì¡°íšŒìˆ˜'], \n",
    "                marker='s', linewidth=3, markersize=8, color='#E74C3C')\n",
    "        ax3.set_title('ì—°ë„ë³„ í‰ê·  ì¡°íšŒìˆ˜', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('ì—°ë„', fontsize=12)\n",
    "        ax3.set_ylabel('í‰ê·  ì¡°íšŒìˆ˜', fontsize=12)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ê°’ í‘œì‹œ\n",
    "        for i, v in enumerate(yearly_stats['í‰ê· ì¡°íšŒìˆ˜']):\n",
    "            ax3.annotate(f'{v:.1f}', (yearly_stats['ì—°ë„'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # ì—°ë„ë³„ ëŒ“ê¸€ ìˆ˜\n",
    "        ax4.bar(yearly_stats['ì—°ë„'], yearly_stats['ì´ëŒ“ê¸€ìˆ˜'], \n",
    "                color='#F39C12', alpha=0.8, width=0.6)\n",
    "        ax4.set_title('ì—°ë„ë³„ ì´ ëŒ“ê¸€ ìˆ˜', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('ì—°ë„', fontsize=12)\n",
    "        ax4.set_ylabel('ì´ ëŒ“ê¸€ ìˆ˜', fontsize=12)\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # ê°’ í‘œì‹œ\n",
    "        for i, v in enumerate(yearly_stats['ì´ëŒ“ê¸€ìˆ˜']):\n",
    "            ax4.annotate(f'{v:,.0f}', (yearly_stats['ì—°ë„'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_monthly_views(self, monthly_stats):\n",
    "        \"\"\"ì›”ë³„ ì¡°íšŒìˆ˜ ì°¨íŠ¸ ê·¸ë¦¬ê¸° - ê°œì„ ëœ ë²„ì „\"\"\"\n",
    "        if monthly_stats is None:\n",
    "            print(\"ì›”ë³„ í†µê³„ë¥¼ ë¨¼ì € ê³„ì‚°í•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "        \n",
    "        # ê·¸ë˜í”„ í¬ê¸°ë¥¼ ë” í¬ê²Œ ì„¤ì •\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12))\n",
    "        \n",
    "        # ì´ ì¡°íšŒìˆ˜ ì¶”ì´\n",
    "        ax1.plot(monthly_stats['ì—°ì›”_str'], monthly_stats['ì´ì¡°íšŒìˆ˜'], \n",
    "                marker='o', linewidth=3, markersize=8, color='#2E86C1')\n",
    "        ax1.set_title('ì›”ë³„ ì´ ì¡°íšŒìˆ˜ ì¶”ì´', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax1.set_xlabel('ì›”', fontsize=12)\n",
    "        ax1.set_ylabel('ì´ ì¡°íšŒìˆ˜', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Xì¶• ë¼ë²¨ íšŒì „ê° ì¡°ì •í•˜ê³  ê°„ê²© ëŠ˜ë¦¬ê¸°\n",
    "        ax1.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax1.tick_params(axis='y', labelsize=10)\n",
    "        \n",
    "        # í‰ê·  ì¡°íšŒìˆ˜ ì¶”ì´\n",
    "        ax2.plot(monthly_stats['ì—°ì›”_str'], monthly_stats['í‰ê· ì¡°íšŒìˆ˜'], \n",
    "                marker='s', linewidth=3, markersize=8, color='#E74C3C')\n",
    "        ax2.set_title('ì›”ë³„ í‰ê·  ì¡°íšŒìˆ˜ ì¶”ì´', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax2.set_xlabel('ì›”', fontsize=12)\n",
    "        ax2.set_ylabel('í‰ê·  ì¡°íšŒìˆ˜', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Xì¶• ë¼ë²¨ íšŒì „ê° ì¡°ì •\n",
    "        ax2.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax2.tick_params(axis='y', labelsize=10)\n",
    "        \n",
    "        # ì—¬ë°± ì¡°ì •\n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "        \n",
    "        # ê²Œì‹œê¸€ ìˆ˜ ì°¨íŠ¸ - ë” í° í¬ê¸°ë¡œ\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        bars = plt.bar(monthly_stats['ì—°ì›”_str'], monthly_stats['ê²Œì‹œê¸€ìˆ˜'], \n",
    "                      color='#58D68D', alpha=0.8, width=0.6)\n",
    "        plt.title('ì›”ë³„ ê²Œì‹œê¸€ ìˆ˜', fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel('ì›”', fontsize=12)\n",
    "        plt.ylabel('ê²Œì‹œê¸€ ìˆ˜', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # ê°’ í‘œì‹œ - ë§‰ëŒ€ ìœ„ì— í‘œì‹œ\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.annotate(f'{height:,.0f}', \n",
    "                        xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                        xytext=(0, 8), \n",
    "                        textcoords=\"offset points\", \n",
    "                        ha='center', \n",
    "                        va='bottom',\n",
    "                        fontsize=11)\n",
    "        \n",
    "        # Xì¶• ë¼ë²¨ ì¡°ì •\n",
    "        plt.xticks(rotation=45, fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_weekly_trends(self, weekly_stats):\n",
    "        \"\"\"ì£¼ê°„ë³„ íŠ¸ë Œë“œ ì°¨íŠ¸ ê·¸ë¦¬ê¸° (ì „ì²´ ê¸°ê°„)\"\"\"\n",
    "        if weekly_stats is None:\n",
    "            print(\"ì£¼ê°„ë³„ í†µê³„ë¥¼ ë¨¼ì € ê³„ì‚°í•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "        \n",
    "        # ì „ì²´ ì£¼ì°¨ê°€ ë§ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ xì¶• ë¼ë²¨ ê°„ê²© ì¡°ì •\n",
    "        total_weeks = len(weekly_stats)\n",
    "        label_step = max(1, total_weeks // 20)  # ìµœëŒ€ 20ê°œ ë¼ë²¨ë§Œ í‘œì‹œ\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20, 15))\n",
    "        \n",
    "        # ì£¼ê°„ë³„ ê²Œì‹œê¸€ ìˆ˜\n",
    "        ax1.plot(range(len(weekly_stats)), weekly_stats['ê²Œì‹œê¸€ìˆ˜'], \n",
    "                linewidth=1, color='#58D68D', alpha=0.8)\n",
    "        ax1.fill_between(range(len(weekly_stats)), weekly_stats['ê²Œì‹œê¸€ìˆ˜'], \n",
    "                        alpha=0.3, color='#58D68D')\n",
    "        ax1.set_title(f'ì£¼ê°„ë³„ ê²Œì‹œê¸€ ìˆ˜ (ì „ì²´ {total_weeks}ì£¼)', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('ì£¼ì°¨', fontsize=12)\n",
    "        ax1.set_ylabel('ê²Œì‹œê¸€ ìˆ˜', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # xì¶• ë¼ë²¨ ê°„ê²© ì¡°ì •\n",
    "        xtick_positions = range(0, len(weekly_stats), label_step)\n",
    "        xtick_labels = [weekly_stats['ì£¼ì°¨_str'].iloc[i] for i in xtick_positions]\n",
    "        ax1.set_xticks(xtick_positions)\n",
    "        ax1.set_xticklabels(xtick_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        # ì£¼ê°„ë³„ ì´ ì¡°íšŒìˆ˜\n",
    "        ax2.plot(range(len(weekly_stats)), weekly_stats['ì´ì¡°íšŒìˆ˜'], \n",
    "                linewidth=1, color='#2E86C1', alpha=0.8)\n",
    "        ax2.fill_between(range(len(weekly_stats)), weekly_stats['ì´ì¡°íšŒìˆ˜'], \n",
    "                        alpha=0.3, color='#2E86C1')\n",
    "        ax2.set_title(f'ì£¼ê°„ë³„ ì´ ì¡°íšŒìˆ˜ (ì „ì²´ {total_weeks}ì£¼)', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('ì£¼ì°¨', fontsize=12)\n",
    "        ax2.set_ylabel('ì´ ì¡°íšŒìˆ˜', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xticks(xtick_positions)\n",
    "        ax2.set_xticklabels(xtick_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        # ì£¼ê°„ë³„ í‰ê·  ì¡°íšŒìˆ˜\n",
    "        ax3.plot(range(len(weekly_stats)), weekly_stats['í‰ê· ì¡°íšŒìˆ˜'], \n",
    "                linewidth=1, color='#E74C3C', alpha=0.8)\n",
    "        ax3.fill_between(range(len(weekly_stats)), weekly_stats['í‰ê· ì¡°íšŒìˆ˜'], \n",
    "                        alpha=0.3, color='#E74C3C')\n",
    "        ax3.set_title(f'ì£¼ê°„ë³„ í‰ê·  ì¡°íšŒìˆ˜ (ì „ì²´ {total_weeks}ì£¼)', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('ì£¼ì°¨', fontsize=12)\n",
    "        ax3.set_ylabel('í‰ê·  ì¡°íšŒìˆ˜', fontsize=12)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_xticks(xtick_positions)\n",
    "        ax3.set_xticklabels(xtick_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "        \n",
    "        # ì£¼ê°„ë³„ í†µê³„ ìš”ì•½ ì¶œë ¥\n",
    "        print(f\"\\n=== ì£¼ê°„ë³„ ë°ì´í„° ìš”ì•½ ===\")\n",
    "        print(f\"ì´ ì£¼ì°¨ ìˆ˜: {total_weeks}ì£¼\")\n",
    "        print(f\"ì£¼ê°„ ê²Œì‹œê¸€ ìˆ˜ - í‰ê· : {weekly_stats['ê²Œì‹œê¸€ìˆ˜'].mean():.1f}, ìµœì†Œ: {weekly_stats['ê²Œì‹œê¸€ìˆ˜'].min()}, ìµœëŒ€: {weekly_stats['ê²Œì‹œê¸€ìˆ˜'].max()}\")\n",
    "        print(f\"ì£¼ê°„ ì´ì¡°íšŒìˆ˜ - í‰ê· : {weekly_stats['ì´ì¡°íšŒìˆ˜'].mean():.0f}, ìµœì†Œ: {weekly_stats['ì´ì¡°íšŒìˆ˜'].min():.0f}, ìµœëŒ€: {weekly_stats['ì´ì¡°íšŒìˆ˜'].max():.0f}\")\n",
    "        print(f\"ì£¼ê°„ í‰ê· ì¡°íšŒìˆ˜ - í‰ê· : {weekly_stats['í‰ê· ì¡°íšŒìˆ˜'].mean():.1f}, ìµœì†Œ: {weekly_stats['í‰ê· ì¡°íšŒìˆ˜'].min():.1f}, ìµœëŒ€: {weekly_stats['í‰ê· ì¡°íšŒìˆ˜'].max():.1f}\")\n",
    "    \n",
    "    def extract_korean_morphemes(self, text, extract_nouns=True, extract_verbs=False, extract_adjectives=False):\n",
    "        \"\"\"\n",
    "        í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ ë‹¨ì–´ ì¶”ì¶œ\n",
    "        \n",
    "        Args:\n",
    "            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸\n",
    "            extract_nouns (bool): ëª…ì‚¬ ì¶”ì¶œ ì—¬ë¶€\n",
    "            extract_verbs (bool): ë™ì‚¬ ì¶”ì¶œ ì—¬ë¶€  \n",
    "            extract_adjectives (bool): í˜•ìš©ì‚¬ ì¶”ì¶œ ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        if not self.use_morphology:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # í˜•íƒœì†Œ ë¶„ì„\n",
    "            morphemes = self.morphology_analyzer.pos(text, stem=True)\n",
    "            \n",
    "            extracted_words = []\n",
    "            for word, pos in morphemes:\n",
    "                # 2ê¸€ì ì´ìƒë§Œ ì¶”ì¶œ\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                    \n",
    "                # í’ˆì‚¬ë³„ ì¶”ì¶œ\n",
    "                if extract_nouns and pos.startswith('N'):  # ëª…ì‚¬\n",
    "                    extracted_words.append(word)\n",
    "                elif extract_verbs and pos.startswith('V'):  # ë™ì‚¬\n",
    "                    extracted_words.append(word)\n",
    "                elif extract_adjectives and pos.startswith('A'):  # í˜•ìš©ì‚¬\n",
    "                    extracted_words.append(word)\n",
    "            \n",
    "            return extracted_words\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"í˜•íƒœì†Œ ë¶„ì„ ì˜¤ë¥˜: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _get_filtered_words_advanced(self, text, use_morphology=True):\n",
    "        \"\"\"\n",
    "        ê°œì„ ëœ ë‹¨ì–´ í•„í„°ë§ (í˜•íƒœì†Œ ë¶„ì„ + ê¸°ì¡´ ë°©ì‹)\n",
    "        \"\"\"\n",
    "        # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ì‚¬ìš©\n",
    "        if use_morphology and self.use_morphology:\n",
    "            korean_words = self.extract_korean_morphemes(\n",
    "                text, \n",
    "                extract_nouns=True, \n",
    "                extract_verbs=False,  # ë™ì‚¬ëŠ” ì œì™¸ (ì˜ë¯¸ê°€ ëª¨í˜¸í•  ìˆ˜ ìˆìŒ)\n",
    "                extract_adjectives=False  # í˜•ìš©ì‚¬ë„ ì œì™¸\n",
    "            )\n",
    "        else:\n",
    "            # ê¸°ì¡´ ë°©ì‹: ì •ê·œì‹ìœ¼ë¡œ í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ\n",
    "            korean_words = re.findall(r'[ê°€-í£]{2,}', text)\n",
    "        \n",
    "        # ì˜ì–´/ìˆ«ì ë‹¨ì–´ ì¶”ì¶œ\n",
    "        english_words = re.findall(r'[a-zA-Z0-9]{2,}', text.lower())\n",
    "        \n",
    "        # ëª¨ë“  ë‹¨ì–´ í•©ì¹˜ê¸°\n",
    "        all_words = korean_words + english_words\n",
    "        \n",
    "        # í™•ì¥ëœ ë¶ˆìš©ì–´ ëª©ë¡\n",
    "        stop_words = {\n",
    "            # ê¸°ì¡´ ë¶ˆìš©ì–´ë“¤...\n",
    "            'ì´ê±°', 'ì´ê±´', 'ì €ê±°', 'ì €ê±´', 'ê·¸ê±°', 'ê·¸ê±´', 'ì—¬ê¸°', 'ì €ê¸°', 'ê±°ê¸°',\n",
    "            'ì´ê²Œ', 'ì €ê²Œ', 'ê·¸ê²Œ', 'ì´ì•¼', 'ì €ì•¼', 'ê·¸ì•¼', 'ì´ëŸ°', 'ì €ëŸ°', 'ê·¸ëŸ°',\n",
    "            'ë­ì•¼', 'ë­”ê°€', 'ì§„ì§œ', 'ì •ë§', 'ì™„ì „', 'ì•„ë‹ˆ', 'ê·¸ëƒ¥', 'ì¢€', 'ë”', \n",
    "            'ë„ˆë¬´', 'ë˜ê²Œ', 'ì—„ì²­', 'ì™„ì „íˆ', 'ì •ë§ë¡œ', 'ì§„ì§œë¡œ', 'ë˜ë©´', 'í•˜ì§€ë§Œ',\n",
    "            'ê·¼ë°', 'ê·¸ëŸ°ë°', 'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë‚˜', \n",
    "            'ê·¸ë ‡ì§€ë§Œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë§ê³ ', 'í•´ì„œ', 'ë˜ê³ ', 'í•˜ê³ ', 'ìˆê³ ', 'ì—†ê³ ',\n",
    "            'ì´ì œ', 'ì§€ê¸ˆ', 'ì˜¤ëŠ˜', 'ì–´ì œ', 'ë‚´ì¼', 'ìš”ì¦˜', 'ìµœê·¼', 'ì–¸ì œ', 'ë°”ë¡œ',\n",
    "            'ë‚´ìš©', 'ì—†ìŒ', 'ê²½ìš°', 'ë•Œë¬¸', 'ë˜ëŠ”', 'í•˜ëŠ”', 'ìˆëŠ”', 'ì—†ëŠ”',\n",
    "            'ì´ë ‡ê²Œ', 'ì €ë ‡ê²Œ', 'ê·¸ë ‡ê²Œ', 'ì–´ë–»ê²Œ', 'ì™œëƒ', 'ë•Œë¬¸ì—', 'ì´ë¼ê³ ',\n",
    "            'ë‚´ê°€', 'ë‚˜ëŠ”', 'ë„ˆëŠ”', 'ë„ˆê°€', 'ê±”ëŠ”', 'ê±”ê°€', 'ìŸ¤ëŠ”', 'ìŸ¤ê°€',\n",
    "            'ìš°ë¦¬ëŠ”', 'ìš°ë¦¬ê°€', 'ì €ëŠ”', 'ì €ê°€', 'ê·¸ê°€', 'ê·¸ëŠ”', 'ê·¸ë…€ëŠ”', 'ê·¸ë…€ê°€',\n",
    "            \n",
    "            # ì»¤ë®¤ë‹ˆí‹° íŠ¹í™” ë¶ˆìš©ì–´\n",
    "            'ê²Œì‹œê¸€', 'ëŒ“ê¸€', 'ì¡°íšŒìˆ˜', 'ì¶”ì²œ', 'ë¹„ì¶”ì²œ', 'ì‹ ê³ ', 'ìˆ˜ì •', 'ì‚­ì œ',\n",
    "            'ì‘ì„±ì', 'ë‹‰ë„¤ì„', 'ì•„ì´ë””', 'íšŒì›', 'ë“±ê¸‰', 'í¬ì¸íŠ¸', 'ê²Œì‹œíŒ',\n",
    "            'dc', 'official', 'app', 'ë‹¤ì‹œ', 'ê³„ì†', 'ì—¬ê¸°ì„œ', 'ë§ì´', 'ì œë°œ', 'name',\n",
    "            \n",
    "            # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ ìì£¼ ë‚˜ì˜¤ëŠ” ë¶ˆìš©ì–´\n",
    "            'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ê³³', 'ì ', 'ë²ˆ', 'ê°œ', 'ëª…', 'ë…„', 'ì›”', 'ì¼',\n",
    "            'ì‹œê°„', 'ë¶„', 'ì´ˆ', 'ì •ë„', 'ë§Œí¼', 'ì´ìƒ', 'ì´í•˜', 'ì‚¬ì´', 'ì¤‘',\n",
    "            'ì•ˆ', 'ë°–', 'ìœ„', 'ì•„ë˜', 'ì•', 'ë’¤', 'ì˜†', 'ë‹¤ìŒ', 'ì´ì „', 'ë§ˆì§€ë§‰',\n",
    "            'ì²˜ìŒ', 'ë', 'ì‹œì‘', 'ì¢…ë£Œ', 'ì™„ë£Œ', 'ì‹œë„', 'ë…¸ë ¥', 'ìƒê°', 'ì˜ê²¬',\n",
    "            'ë¬¸ì œ', 'í•´ê²°', 'ìƒí™©', 'ìƒíƒœ', 'ê²°ê³¼', 'ê³¼ì •', 'ë°©ë²•', 'ë°©ì‹',\n",
    "            \n",
    "            # ê°ì • í‘œí˜„ (ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²ƒë“¤)\n",
    "            'ì¢‹ë‹¤', 'ë‚˜ì˜ë‹¤', 'ì‹«ë‹¤', 'ì¢‹ì•„', 'ì‹«ì–´', 'ì¬ë¯¸', 'ì¬ë°Œ', 'boring',\n",
    "            'ì›ƒìŒ', 'ìŠ¬í””', 'ê¸°ì¨', 'í™”ë‚¨', 'ë†€ëŒ', 'ê±±ì •', 'ë¶ˆì•ˆ', 'ì•ˆì‹¬'\n",
    "        }\n",
    "        \n",
    "        # ë¶ˆìš©ì–´ ì œê±° ë° ì¶”ê°€ í•„í„°ë§\n",
    "        filtered_words = []\n",
    "        for word in all_words:\n",
    "            # ë¶ˆìš©ì–´ ì œê±°\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            \n",
    "            # ìˆ«ìë§Œ ìˆëŠ” ë‹¨ì–´ ì œê±°\n",
    "            if word.isdigit():\n",
    "                continue\n",
    "            \n",
    "            # ë°˜ë³µ ë¬¸ì ì œê±° (ã…‹ã…‹ã…‹, ã…ã…ã… ë“±)\n",
    "            if len(set(word)) == 1 and len(word) > 2:\n",
    "                continue\n",
    "            \n",
    "            # ì˜ì„±ì–´/ì˜íƒœì–´ íŒ¨í„´ ì œê±°\n",
    "            if re.match(r'^(.{1,2})\\1+$', word):\n",
    "                continue\n",
    "            \n",
    "            # íŠ¹ìˆ˜ íŒ¨í„´ ì œê±° (URL ì¡°ê°, ì´ë©”ì¼ ì¡°ê° ë“±)\n",
    "            if re.match(r'^(www|http|com|net|org)$', word):\n",
    "                continue\n",
    "                \n",
    "            filtered_words.append(word)\n",
    "        \n",
    "        return filtered_words\n",
    "    \n",
    "    def analyze_text_frequency_advanced(self, top_n=30, use_filtered_data=True, use_morphology=True):\n",
    "        \"\"\"\n",
    "        ê°œì„ ëœ í…ìŠ¤íŠ¸ ë¹ˆë„ ë¶„ì„\n",
    "        \n",
    "        Args:\n",
    "            top_n (int): ìƒìœ„ Nê°œ ë‹¨ì–´\n",
    "            use_filtered_data (bool): ë„ë°° í•„í„°ë§ëœ ë°ì´í„° ì‚¬ìš© ì—¬ë¶€\n",
    "            use_morphology (bool): í˜•íƒœì†Œ ë¶„ì„ ì‚¬ìš© ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        # ì‚¬ìš©í•  ë°ì´í„° ì„ íƒ\n",
    "        if use_filtered_data and self.filtered_df is not None:\n",
    "            data_to_use = self.filtered_df\n",
    "            data_desc = \"ë„ë°° í•„í„°ë§ í›„\"\n",
    "        else:\n",
    "            data_to_use = self.processed_df\n",
    "            data_desc = \"ì „ì²´\"\n",
    "        \n",
    "        if data_to_use is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        # ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°\n",
    "        all_text = (data_to_use['ì œëª©'] + ' ' + data_to_use['ë‚´ìš©']).str.cat(sep=' ')\n",
    "        \n",
    "        # ê°œì„ ëœ ë‹¨ì–´ ì¶”ì¶œ\n",
    "        final_words = self._get_filtered_words_advanced(all_text, use_morphology)\n",
    "        \n",
    "        # ë¹ˆë„ ê³„ì‚°\n",
    "        word_freq = Counter(final_words)\n",
    "        top_words = word_freq.most_common(top_n)\n",
    "        \n",
    "        morphology_desc = \"í˜•íƒœì†Œ ë¶„ì„\" if (use_morphology and self.use_morphology) else \"ì •ê·œì‹\"\n",
    "        print(f\"=== {data_desc} ë°ì´í„° ë‹¨ì–´ ë¹ˆë„ TOP {top_n} ({morphology_desc}) ===\")\n",
    "        for i, (word, count) in enumerate(top_words, 1):\n",
    "            print(f\"{i:2d}. {word}: {count:,}íšŒ\")\n",
    "        \n",
    "        return top_words\n",
    "    \n",
    "    def analyze_yearly_word_frequency(self, top_n=20):\n",
    "        \"\"\"ì—°ë„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        yearly_word_freq = {}\n",
    "        \n",
    "        for year in sorted(self.processed_df['ì—°ë„'].unique()):\n",
    "            year_data = self.processed_df[self.processed_df['ì—°ë„'] == year]\n",
    "            \n",
    "            # í•´ë‹¹ ì—°ë„ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°\n",
    "            year_text = (year_data['ì œëª©'] + ' ' + year_data['ë‚´ìš©']).str.cat(sep=' ')\n",
    "            \n",
    "            # í•„í„°ë§ëœ ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸°\n",
    "            year_words = self._get_filtered_words_advanced(year_text, use_morphology=self.use_morphology)\n",
    "            \n",
    "            # ë¹ˆë„ ê³„ì‚°\n",
    "            word_freq = Counter(year_words)\n",
    "            top_words = word_freq.most_common(top_n)\n",
    "            yearly_word_freq[year] = top_words\n",
    "            \n",
    "            print(f\"=== {year}ë…„ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë‹¨ì–´ TOP {top_n} ===\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"{i:2d}. {word}: {count:,}íšŒ\")\n",
    "            print()\n",
    "        \n",
    "        return yearly_word_freq\n",
    "    \n",
    "    def analyze_monthly_word_frequency(self, top_n=20):\n",
    "        \"\"\"ì›”ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        monthly_word_freq = {}\n",
    "        \n",
    "        for month in sorted(self.processed_df['ì—°ì›”'].unique()):\n",
    "            month_data = self.processed_df[self.processed_df['ì—°ì›”'] == month]\n",
    "            \n",
    "            # í•´ë‹¹ ì›”ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°\n",
    "            month_text = (month_data['ì œëª©'] + ' ' + month_data['ë‚´ìš©']).str.cat(sep=' ')\n",
    "            \n",
    "            # í•„í„°ë§ëœ ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸°\n",
    "            month_words = self._get_filtered_words_advanced(month_text, use_morphology=self.use_morphology)\n",
    "            \n",
    "            # ë¹ˆë„ ê³„ì‚°\n",
    "            word_freq = Counter(month_words)\n",
    "            top_words = word_freq.most_common(top_n)\n",
    "            monthly_word_freq[month] = top_words\n",
    "            \n",
    "            print(f\"=== {month} ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë‹¨ì–´ TOP {top_n} ===\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"{i:2d}. {word}: {count:,}íšŒ\")\n",
    "            print()\n",
    "        \n",
    "        return monthly_word_freq\n",
    "    \n",
    "    def analyze_weekly_word_frequency(self, top_n=20):\n",
    "        \"\"\"ì£¼ê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ìµœê·¼ 20ì£¼ë§Œ)\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"ë°ì´í„°ë¥¼ ë¨¼ì € ì „ì²˜ë¦¬í•´ì£¼ì„¸ìš”.\")\n",
    "            return None\n",
    "        \n",
    "        weekly_word_freq = {}\n",
    "        \n",
    "        # ìµœê·¼ 20ì£¼ë§Œ ë¶„ì„ (ì „ì²´ ì£¼ì°¨ê°€ ë„ˆë¬´ ë§ì„ ìˆ˜ ìˆìŒ)\n",
    "        recent_weeks = sorted(self.processed_df['ì£¼ì°¨'].unique())[-20:]\n",
    "        \n",
    "        for week in recent_weeks:\n",
    "            week_data = self.processed_df[self.processed_df['ì£¼ì°¨'] == week]\n",
    "            \n",
    "            # í•´ë‹¹ ì£¼ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°\n",
    "            week_text = (week_data['ì œëª©'] + ' ' + week_data['ë‚´ìš©']).str.cat(sep=' ')\n",
    "            \n",
    "            # í•„í„°ë§ëœ ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸°\n",
    "            week_words = self._get_filtered_words_advanced(week_text, use_morphology=self.use_morphology)\n",
    "            \n",
    "            # ë¹ˆë„ ê³„ì‚°\n",
    "            word_freq = Counter(week_words)\n",
    "            top_words = word_freq.most_common(top_n)\n",
    "            weekly_word_freq[week] = top_words\n",
    "            \n",
    "            print(f\"=== {week} ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë‹¨ì–´ TOP {top_n} ===\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"{i:2d}. {word}: {count:,}íšŒ\")\n",
    "            print()\n",
    "        \n",
    "        return weekly_word_freq\n",
    "    \n",
    "    def plot_yearly_word_frequency(self, yearly_word_freq, top_n=10):\n",
    "        \"\"\"ì—°ë„ë³„ ë‹¨ì–´ ë¹ˆë„ ì°¨íŠ¸ ê·¸ë¦¬ê¸°\"\"\"\n",
    "        if not yearly_word_freq:\n",
    "            print(\"ì—°ë„ë³„ ë‹¨ì–´ ë¹ˆë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        years = sorted(yearly_word_freq.keys())\n",
    "        num_years = len(years)\n",
    "        \n",
    "        # ì„œë¸Œí”Œë¡¯ êµ¬ì„± (2ì—´ë¡œ ë°°ì¹˜)\n",
    "        cols = 2\n",
    "        rows = (num_years + 1) // 2\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(20, 6*rows))\n",
    "        if rows == 1:\n",
    "            axes = [axes] if num_years == 1 else axes\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for i, year in enumerate(years):\n",
    "            words, counts = zip(*yearly_word_freq[year][:top_n])\n",
    "            \n",
    "            ax = axes[i]\n",
    "            y_pos = np.arange(len(words))\n",
    "            \n",
    "            bars = ax.barh(y_pos, counts, color='#3498DB', alpha=0.8, height=0.7)\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(words, fontsize=10)\n",
    "            ax.set_xlabel('ë¹ˆë„ìˆ˜', fontsize=11)\n",
    "            ax.set_title(f'{year}ë…„ ë‹¨ì–´ ë¹ˆë„ TOP {top_n}', fontsize=14, fontweight='bold')\n",
    "            ax.invert_yaxis()\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # ê°’ í‘œì‹œ\n",
    "            for j, bar in enumerate(bars):\n",
    "                width = bar.get_width()\n",
    "                ax.annotate(f'{width:,}', \n",
    "                           xy=(width, bar.get_y() + bar.get_height()/2),\n",
    "                           xytext=(5, 0), \n",
    "                           textcoords=\"offset points\", \n",
    "                           ha='left', \n",
    "                           va='center', \n",
    "                           fontsize=9)\n",
    "        \n",
    "        # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì„œë¸Œí”Œë¡¯ ìˆ¨ê¸°ê¸°\n",
    "        for i in range(num_years, len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_monthly_word_frequency(self, monthly_word_freq, top_n=10, months_per_page=6):\n",
    "        \"\"\"ì›”ë³„ ë‹¨ì–´ ë¹ˆë„ ì°¨íŠ¸ ê·¸ë¦¬ê¸° (í˜ì´ì§€ ë‚˜ëˆ„ê¸°)\"\"\"\n",
    "        if not monthly_word_freq:\n",
    "            print(\"ì›”ë³„ ë‹¨ì–´ ë¹ˆë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        months = sorted(monthly_word_freq.keys())\n",
    "        total_months = len(months)\n",
    "        \n",
    "        # í˜ì´ì§€ë³„ë¡œ ë‚˜ëˆ„ì–´ ê·¸ë¦¬ê¸°\n",
    "        for page_start in range(0, total_months, months_per_page):\n",
    "            page_end = min(page_start + months_per_page, total_months)\n",
    "            page_months = months[page_start:page_end]\n",
    "            \n",
    "            # ì„œë¸Œí”Œë¡¯ êµ¬ì„±\n",
    "            cols = 3\n",
    "            rows = (len(page_months) + 2) // 3\n",
    "            \n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(18, 6*rows))\n",
    "            if rows == 1:\n",
    "                axes = [axes] if len(page_months) == 1 else axes\n",
    "            else:\n",
    "                axes = axes.flatten()\n",
    "            \n",
    "            for i, month in enumerate(page_months):\n",
    "                words, counts = zip(*monthly_word_freq[month][:top_n])\n",
    "                \n",
    "                ax = axes[i]\n",
    "                y_pos = np.arange(len(words))\n",
    "                \n",
    "                bars = ax.barh(y_pos, counts, color='#E74C3C', alpha=0.8, height=0.7)\n",
    "                ax.set_yticks(y_pos)\n",
    "                ax.set_yticklabels(words, fontsize=9)\n",
    "                ax.set_xlabel('ë¹ˆë„ìˆ˜', fontsize=10)\n",
    "                ax.set_title(f'{month} ë‹¨ì–´ ë¹ˆë„ TOP {top_n}', fontsize=12, fontweight='bold')\n",
    "                ax.invert_yaxis()\n",
    "                ax.grid(True, alpha=0.3, axis='x')\n",
    "                \n",
    "                # ê°’ í‘œì‹œ\n",
    "                for j, bar in enumerate(bars):\n",
    "                    width = bar.get_width()\n",
    "                    ax.annotate(f'{width:,}', \n",
    "                               xy=(width, bar.get_y() + bar.get_height()/2),\n",
    "                               xytext=(3, 0), \n",
    "                               textcoords=\"offset points\", \n",
    "                               ha='left', \n",
    "                               va='center', \n",
    "                               fontsize=8)\n",
    "            \n",
    "            # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì„œë¸Œí”Œë¡¯ ìˆ¨ê¸°ê¸°\n",
    "            for i in range(len(page_months), len(axes)):\n",
    "                axes[i].set_visible(False)\n",
    "            \n",
    "            plt.suptitle(f'ì›”ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ ({page_start//months_per_page + 1}í˜ì´ì§€)', \n",
    "                        fontsize=16, fontweight='bold')\n",
    "            plt.tight_layout(pad=2.0)\n",
    "            plt.show()\n",
    "    \n",
    "    def generate_advanced_summary_report(self):\n",
    "        \"\"\"ê°œì„ ëœ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"           ê°œì„ ëœ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if self.processed_df is not None:\n",
    "            print(\"ğŸ“Š ì›ë³¸ ë°ì´í„° í†µê³„\")\n",
    "            total_posts = len(self.processed_df)\n",
    "            print(f\"   ì´ ê²Œì‹œê¸€ ìˆ˜: {total_posts:,}ê°œ\")\n",
    "            print(f\"   ì´ ì¡°íšŒìˆ˜: {self.processed_df['ì¡°íšŒìˆ˜'].sum():,}íšŒ\")\n",
    "            print(f\"   í‰ê·  ì¡°íšŒìˆ˜: {self.processed_df['ì¡°íšŒìˆ˜'].mean():.1f}íšŒ\")\n",
    "        \n",
    "        if self.filtered_df is not None:\n",
    "            print(\"\\nğŸ§¹ í•„í„°ë§ í›„ ë°ì´í„° í†µê³„\")\n",
    "            filtered_posts = len(self.filtered_df)\n",
    "            removed_posts = total_posts - filtered_posts\n",
    "            print(f\"   í•„í„°ë§ í›„ ê²Œì‹œê¸€ ìˆ˜: {filtered_posts:,}ê°œ\")\n",
    "            print(f\"   ì œê±°ëœ ê²Œì‹œê¸€ ìˆ˜: {removed_posts:,}ê°œ ({removed_posts/total_posts*100:.1f}%)\")\n",
    "            print(f\"   í•„í„°ë§ í›„ ì´ ì¡°íšŒìˆ˜: {self.filtered_df['ì¡°íšŒìˆ˜'].sum():,}íšŒ\")\n",
    "            print(f\"   í•„í„°ë§ í›„ í‰ê·  ì¡°íšŒìˆ˜: {self.filtered_df['ì¡°íšŒìˆ˜'].mean():.1f}íšŒ\")\n",
    "        \n",
    "        if self.use_morphology:\n",
    "            print(f\"\\nğŸ”¤ í˜•íƒœì†Œ ë¶„ì„: âœ… ì‚¬ìš© ê°€ëŠ¥ (KoNLPy)\")\n",
    "        else:\n",
    "            print(f\"\\nğŸ”¤ í˜•íƒœì†Œ ë¶„ì„: âŒ ì‚¬ìš© ë¶ˆê°€ (ì •ê·œì‹ ì‚¬ìš©)\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# ===================================\n",
    "# 3ë‹¨ê³„: ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜ë“¤\n",
    "# ===================================\n",
    "\n",
    "def run_advanced_analysis():\n",
    "    \"\"\"ê°œì„ ëœ ë¶„ì„ ì‹¤í–‰\"\"\"\n",
    "    # ë¶„ì„ê¸° ì´ˆê¸°í™” (íŒŒì¼ ê²½ë¡œ ìˆ˜ì • í•„ìš”)\n",
    "    analyzer = AdvancedCommunityDataAnalyzer('community/ChartAnalysis.csv')  # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”\n",
    "    \n",
    "    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "    if not analyzer.load_data():\n",
    "        return\n",
    "    if not analyzer.preprocess_data():\n",
    "        return\n",
    "    \n",
    "    # 2. ì™„ì „ ë™ì¼í•œ ì¤‘ë³µ ê²Œì‹œê¸€ë§Œ í•„í„°ë§\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸš« ì™„ì „íˆ ë˜‘ê°™ì€ ì¤‘ë³µ ê²Œì‹œê¸€ë§Œ í•„í„°ë§\")\n",
    "    print(\"=\"*50)\n",
    "    spam_indices = analyzer.detect_spam_posts_exact_only(use_hashing=True)\n",
    "    \n",
    "    # 3. ì‹œê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„ (ì›ë˜ ê¸°ëŠ¥ë“¤)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“… ì‹œê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 3-1. ì—°ë„ë³„ íŠ¸ë Œë“œ ë¶„ì„ ë° ì°¨íŠ¸\n",
    "    print(\"ğŸ—“ï¸ ì—°ë„ë³„ ë¶„ì„...\")\n",
    "    yearly_stats = analyzer.analyze_yearly_trends()\n",
    "    if yearly_stats is not None:\n",
    "        analyzer.plot_yearly_trends(yearly_stats)\n",
    "    \n",
    "    # 3-2. ì›”ë³„ íŠ¸ë Œë“œ ë¶„ì„ ë° ì°¨íŠ¸  \n",
    "    print(\"\\nğŸ“… ì›”ë³„ ë¶„ì„...\")\n",
    "    monthly_stats = analyzer.analyze_monthly_trends()\n",
    "    if monthly_stats is not None:\n",
    "        analyzer.plot_monthly_views(monthly_stats)\n",
    "    \n",
    "    # 3-3. ì£¼ê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„ ë° ì°¨íŠ¸\n",
    "    print(\"\\nğŸ“ˆ ì£¼ê°„ë³„ ë¶„ì„...\")\n",
    "    weekly_stats = analyzer.analyze_weekly_trends()\n",
    "    if weekly_stats is not None:\n",
    "        analyzer.plot_weekly_trends(weekly_stats)\n",
    "    \n",
    "    # 4. ì‹œê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ì›ë˜ ê¸°ëŠ¥ë“¤)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ”¤ ì‹œê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 4-1. ì—°ë„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\n",
    "    print(\"ğŸ“… ì—°ë„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„...\")\n",
    "    yearly_word_freq = analyzer.analyze_yearly_word_frequency(top_n=20)\n",
    "    if yearly_word_freq:\n",
    "        analyzer.plot_yearly_word_frequency(yearly_word_freq, top_n=15)\n",
    "    \n",
    "    # 4-2. ì›”ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\n",
    "    print(\"\\nğŸ“… ì›”ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„...\")\n",
    "    monthly_word_freq = analyzer.analyze_monthly_word_frequency(top_n=20)\n",
    "    if monthly_word_freq:\n",
    "        analyzer.plot_monthly_word_frequency(monthly_word_freq, top_n=10, months_per_page=6)\n",
    "    \n",
    "    # 4-3. ì£¼ê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ìµœê·¼ 20ì£¼)\n",
    "    print(\"\\nğŸ“ˆ ì£¼ê°„ë³„ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ìµœê·¼ 20ì£¼)...\")\n",
    "    weekly_word_freq = analyzer.analyze_weekly_word_frequency(top_n=15)\n",
    "    \n",
    "    # 5. ì „ì²´ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ğŸ“ ì „ì²´ ë°ì´í„° ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # í•„í„°ë§ëœ ë°ì´í„° ë¶„ì„ (í˜•íƒœì†Œ ë¶„ì„)\n",
    "    if analyzer.use_morphology:\n",
    "        print(\"\\n[í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜]\")\n",
    "        filtered_words_morph = analyzer.analyze_text_frequency_advanced(\n",
    "            top_n=30, use_filtered_data=True, use_morphology=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n[ì •ê·œì‹ ê¸°ë°˜]\")\n",
    "        filtered_words_regex = analyzer.analyze_text_frequency_advanced(\n",
    "            top_n=30, use_filtered_data=True, use_morphology=False\n",
    "        )\n",
    "    \n",
    "    # 6. ì¢…í•© ë¦¬í¬íŠ¸\n",
    "    analyzer.generate_advanced_summary_report()\n",
    "\n",
    "\n",
    "def quick_exact_duplicate_filter():\n",
    "    \"\"\"ì™„ì „íˆ ë˜‘ê°™ì€ ì¤‘ë³µ ê²Œì‹œê¸€ë§Œ í•„í„°ë§\"\"\"\n",
    "    analyzer = AdvancedCommunityDataAnalyzer('community/ChartAnalysis.csv')  # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”\n",
    "    \n",
    "    if not analyzer.load_data() or not analyzer.preprocess_data():\n",
    "        return\n",
    "    \n",
    "    # ì™„ì „ ë™ì¼ ì¤‘ë³µë§Œ í•„í„°ë§\n",
    "    spam_indices = analyzer.detect_spam_posts_exact_only(use_hashing=True)\n",
    "    \n",
    "    # í•„í„°ë§ëœ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥\n",
    "    if analyzer.filtered_df is not None:\n",
    "        output_filename = 'filtered_community_data.csv'\n",
    "        analyzer.filtered_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "        print(f\"\\nâœ… í•„í„°ë§ëœ ë°ì´í„°ê°€ '{output_filename}'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "def morphology_analysis_only():\n",
    "    \"\"\"í˜•íƒœì†Œ ë¶„ì„ë§Œ ìˆ˜í–‰\"\"\"\n",
    "    analyzer = AdvancedCommunityDataAnalyzer('community/ChartAnalysis.csv')  # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”\n",
    "    \n",
    "    if not analyzer.load_data() or not analyzer.preprocess_data():\n",
    "        return\n",
    "    \n",
    "    if not analyzer.use_morphology:\n",
    "        print(\"âŒ í˜•íƒœì†Œ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        print(\"ì„¤ì¹˜ ë°©ë²•: !pip install konlpy\")\n",
    "        return\n",
    "    \n",
    "    print(\"ğŸ”¤ í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„\")\n",
    "    \n",
    "    # ëª…ì‚¬ë§Œ ì¶”ì¶œ\n",
    "    morpheme_words = analyzer.analyze_text_frequency_advanced(\n",
    "        top_n=30, use_filtered_data=False, use_morphology=True\n",
    "    )\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    if morpheme_words:\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        words, counts = zip(*morpheme_words[:25])\n",
    "        y_pos = np.arange(len(words))\n",
    "        \n",
    "        bars = plt.barh(y_pos, counts, color='#9B59B6', alpha=0.8)\n",
    "        plt.yticks(y_pos, words, fontsize=11)\n",
    "        plt.xlabel('ë¹ˆë„ìˆ˜', fontsize=12)\n",
    "        plt.title('í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ ë‹¨ì–´ ë¹ˆë„ (ëª…ì‚¬)', fontsize=16, fontweight='bold')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            plt.annotate(f'{width:,}', xy=(width, bar.get_y() + bar.get_height()/2),\n",
    "                       xytext=(5, 0), textcoords=\"offset points\", \n",
    "                       ha='left', va='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ===================================\n",
    "# 4ë‹¨ê³„: ì‹¤í–‰\n",
    "# ===================================\n",
    "\n",
    "print(\"ğŸš€ ê°œì„ ëœ ì»¤ë®¤ë‹ˆí‹° ë°ì´í„° ë¶„ì„ê¸°\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. run_advanced_analysis() - ì „ì²´ ë¶„ì„\")\n",
    "print(\"2. quick_exact_duplicate_filter() - ì™„ì „ ë™ì¼ ì¤‘ë³µë§Œ í•„í„°ë§\")  \n",
    "print(\"3. morphology_analysis_only() - í˜•íƒœì†Œ ë¶„ì„ë§Œ\")\n",
    "print(\"=\"*50)\n",
    "print(\"ğŸ’¡ ì™„ì „íˆ ë˜‘ê°™ì€ ê¸€ë§Œ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ê°€ì¥ ë³´ìˆ˜ì )\")\n",
    "\n",
    "# íŒŒì¼ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •í•œ í›„ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‹¤í–‰í•˜ì„¸ìš”\n",
    "run_advanced_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
