{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8706ac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\chan5\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\chan5\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\chan5\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: koreanize-matplotlib in c:\\users\\chan5\\anaconda3\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: konlpy in c:\\users\\chan5\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from konlpy) (5.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chan5\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib koreanize-matplotlib konlpy JPype1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f081ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ KoNLPy 라이브러리가 사용 가능합니다.\n",
      "🚀 개선된 커뮤니티 데이터 분석기\n",
      "==================================================\n",
      "1. run_advanced_analysis() - 전체 분석\n",
      "2. quick_exact_duplicate_filter() - 완전 동일 중복만 필터링\n",
      "3. morphology_analysis_only() - 형태소 분석만\n",
      "==================================================\n",
      "💡 완전히 똑같은 글만 중복으로 처리합니다 (가장 보수적)\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# 1단계: 라이브러리 import 및 설정\n",
    "# ===================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import koreanize_matplotlib\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# 한국어 형태소 분석을 위한 라이브러리 (설치 필요)\n",
    "try:\n",
    "    from konlpy.tag import Okt, Mecab, Kkma\n",
    "    KONLPY_AVAILABLE = True\n",
    "    print(\"✅ KoNLPy 라이브러리가 사용 가능합니다.\")\n",
    "except ImportError:\n",
    "    KONLPY_AVAILABLE = False\n",
    "    print(\"⚠️ KoNLPy 라이브러리가 설치되지 않았습니다. 기본 분석을 사용합니다.\")\n",
    "    print(\"설치 방법: !pip install konlpy\")\n",
    "\n",
    "# ===================================\n",
    "# 2단계: 분석기 클래스 정의\n",
    "# ===================================\n",
    "\n",
    "class AdvancedCommunityDataAnalyzer:\n",
    "    def __init__(self, csv_file_path):\n",
    "        \"\"\"\n",
    "        개선된 커뮤니티 데이터 분석기 초기화\n",
    "        \n",
    "        Args:\n",
    "            csv_file_path (str): CSV 파일 경로\n",
    "        \"\"\"\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.df = None\n",
    "        self.processed_df = None\n",
    "        self.filtered_df = None  # 도배 필터링된 데이터\n",
    "        \n",
    "        # 한국어 형태소 분석기 초기화\n",
    "        if KONLPY_AVAILABLE:\n",
    "            try:\n",
    "                self.okt = Okt()\n",
    "                self.morphology_analyzer = self.okt\n",
    "                self.use_morphology = True\n",
    "                print(\"✅ Okt 형태소 분석기를 사용합니다.\")\n",
    "            except:\n",
    "                self.use_morphology = False\n",
    "                print(\"⚠️ 형태소 분석기 초기화 실패. 기본 분석을 사용합니다.\")\n",
    "        else:\n",
    "            self.use_morphology = False\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"CSV 파일을 로드하고 기본 정보를 출력\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.csv_file_path, encoding='utf-8')\n",
    "            print(\"=== 데이터 로드 완료 ===\")\n",
    "            print(f\"총 데이터 수: {len(self.df):,}개\")\n",
    "            print(f\"컬럼: {list(self.df.columns)}\")\n",
    "            print(\"\\n=== 데이터 미리보기 ===\")\n",
    "            print(self.df.head())\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"데이터 로드 오류: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"데이터 전처리\"\"\"\n",
    "        if self.df is None:\n",
    "            print(\"데이터를 먼저 로드해주세요.\")\n",
    "            return False\n",
    "        \n",
    "        # 데이터 복사\n",
    "        self.processed_df = self.df.copy()\n",
    "        \n",
    "        # 날짜 변환 함수\n",
    "        def convert_date_pandas(date_str):\n",
    "            if pd.isna(date_str):\n",
    "                return pd.NaT\n",
    "            \n",
    "            date_str = str(date_str)\n",
    "            try:\n",
    "                if '.' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%y.%m.%d')\n",
    "                elif '/' in date_str:\n",
    "                    return pd.to_datetime(date_str, format='%y/%m/%d')\n",
    "                else:\n",
    "                    return pd.NaT\n",
    "            except:\n",
    "                return pd.NaT\n",
    "        \n",
    "        # 날짜 변환\n",
    "        self.processed_df['날짜_변환'] = self.processed_df['날짜'].apply(convert_date_pandas)\n",
    "        self.processed_df = self.processed_df.dropna(subset=['날짜_변환'])\n",
    "        \n",
    "        # 연도, 연월, 주차 컬럼 추가\n",
    "        self.processed_df['연도'] = self.processed_df['날짜_변환'].dt.year\n",
    "        self.processed_df['연월'] = self.processed_df['날짜_변환'].dt.to_period('M')\n",
    "        self.processed_df['주차'] = self.processed_df['날짜_변환'].dt.to_period('W')\n",
    "        \n",
    "        # 숫자 컬럼 변환\n",
    "        def safe_convert_to_numeric(x):\n",
    "            try:\n",
    "                if pd.isna(x) or x == '' or x == 'NaN':\n",
    "                    return 0\n",
    "                if isinstance(x, str):\n",
    "                    clean_num = ''.join(filter(str.isdigit, str(x)))\n",
    "                    return int(clean_num) if clean_num else 0\n",
    "                return int(x)\n",
    "            except:\n",
    "                return 0\n",
    "        \n",
    "        self.processed_df['조회수'] = self.processed_df['조회수'].apply(safe_convert_to_numeric)\n",
    "        self.processed_df['댓글갯수'] = self.processed_df['댓글갯수'].apply(safe_convert_to_numeric)\n",
    "        \n",
    "        # 결측값 처리\n",
    "        self.processed_df['제목'] = self.processed_df['제목'].fillna(\"\")\n",
    "        self.processed_df['내용'] = self.processed_df['내용'].fillna(\"\")\n",
    "        \n",
    "        print(\"=== 데이터 전처리 완료 ===\")\n",
    "        print(f\"처리된 데이터 수: {len(self.processed_df):,}개\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def calculate_text_similarity(self, text1, text2):\n",
    "        \"\"\"두 텍스트 간의 유사도 계산 (0~1)\"\"\"\n",
    "        return SequenceMatcher(None, text1, text2).ratio()\n",
    "    \n",
    "    def detect_spam_posts_exact_only(self, use_hashing=True):\n",
    "        \"\"\"\n",
    "        완전히 똑같은 글만 중복으로 처리 (가장 보수적)\n",
    "        \n",
    "        Args:\n",
    "            use_hashing (bool): 해시 기반 빠른 필터링 사용 여부\n",
    "        \"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=== 완전 동일한 중복 게시글만 탐지 ===\")\n",
    "        print(f\"📊 총 {len(self.processed_df):,}개 게시글 분석\")\n",
    "        print(\"🔍 글자 하나라도 다르면 다른 글로 인정합니다\")\n",
    "        \n",
    "        spam_indices = set()\n",
    "        \n",
    "        if use_hashing:\n",
    "            # 해시 기반 완전 중복 탐지만 사용\n",
    "            print(\"🚀 해시 기반 완전 중복 탐지...\")\n",
    "            title_hashes = {}\n",
    "            content_hashes = {}\n",
    "            combined_hashes = {}  # 제목+내용 통합 해시\n",
    "            \n",
    "            for idx, row in self.processed_df.iterrows():\n",
    "                # 공백 정규화 (연속된 공백을 하나로)\n",
    "                title_clean = re.sub(r'\\s+', ' ', str(row['제목']).strip())\n",
    "                content_clean = re.sub(r'\\s+', ' ', str(row['내용']).strip())\n",
    "                combined_clean = title_clean + \" | \" + content_clean\n",
    "                \n",
    "                # 1. 제목 완전 중복 체크\n",
    "                title_hash = hash(title_clean.lower())\n",
    "                if title_hash in title_hashes:\n",
    "                    spam_indices.add(idx)\n",
    "                else:\n",
    "                    title_hashes[title_hash] = idx\n",
    "                \n",
    "                # 2. 내용 완전 중복 체크 (3글자 이상인 경우만)\n",
    "                if len(content_clean) >= 3:\n",
    "                    content_hash = hash(content_clean.lower())\n",
    "                    if content_hash in content_hashes:\n",
    "                        spam_indices.add(idx)\n",
    "                    else:\n",
    "                        content_hashes[content_hash] = idx\n",
    "                \n",
    "                # 3. 제목+내용 통합 완전 중복 체크\n",
    "                combined_hash = hash(combined_clean.lower())\n",
    "                if combined_hash in combined_hashes:\n",
    "                    spam_indices.add(idx)\n",
    "                else:\n",
    "                    combined_hashes[combined_hash] = idx\n",
    "            \n",
    "            print(f\"   총 중복 발견: {len(spam_indices):,}개\")\n",
    "        \n",
    "        # 중복이 아닌 게시글만 필터링\n",
    "        self.filtered_df = self.processed_df[~self.processed_df.index.isin(spam_indices)].copy()\n",
    "        \n",
    "        print(f\"\\n=== 완전 동일 중복 필터링 결과 ===\")\n",
    "        print(f\"원본 게시글 수: {len(self.processed_df):,}개\")\n",
    "        print(f\"완전 동일 중복 수: {len(spam_indices):,}개 ({len(spam_indices)/len(self.processed_df)*100:.1f}%)\")\n",
    "        print(f\"필터링 후 게시글 수: {len(self.filtered_df):,}개\")\n",
    "        print(f\"✅ 글자 하나라도 다르면 별개 게시글로 유지됩니다!\")\n",
    "        \n",
    "        return spam_indices\n",
    "    \n",
    "    def analyze_yearly_trends(self):\n",
    "        \"\"\"연도별 트렌드 분석\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        # 연도별 통계 계산\n",
    "        yearly_stats = self.processed_df.groupby('연도').agg({\n",
    "            '번호': 'count',\n",
    "            '조회수': ['sum', 'mean', 'max'],\n",
    "            '댓글갯수': ['sum', 'mean']\n",
    "        }).round(2)\n",
    "        \n",
    "        # 컬럼명 정리\n",
    "        yearly_stats.columns = ['게시글수', '총조회수', '평균조회수', '최대조회수', '총댓글수', '평균댓글수']\n",
    "        yearly_stats = yearly_stats.reset_index()\n",
    "        \n",
    "        print(\"=== 연도별 통계 ===\")\n",
    "        for _, row in yearly_stats.iterrows():\n",
    "            print(f\"{int(row['연도'])}: 게시글 {row['게시글수']:,}개, \"\n",
    "                  f\"총조회수 {row['총조회수']:,.0f}, 평균조회수 {row['평균조회수']:.1f}\")\n",
    "        \n",
    "        return yearly_stats\n",
    "    \n",
    "    def analyze_monthly_trends(self):\n",
    "        \"\"\"월별 트렌드 분석\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        # 월별 통계 계산\n",
    "        monthly_stats = self.processed_df.groupby('연월').agg({\n",
    "            '번호': 'count',\n",
    "            '조회수': ['sum', 'mean', 'max'],\n",
    "            '댓글갯수': ['sum', 'mean']\n",
    "        }).round(2)\n",
    "        \n",
    "        # 컬럼명 정리\n",
    "        monthly_stats.columns = ['게시글수', '총조회수', '평균조회수', '최대조회수', '총댓글수', '평균댓글수']\n",
    "        monthly_stats = monthly_stats.reset_index()\n",
    "        monthly_stats['연월_str'] = monthly_stats['연월'].astype(str)\n",
    "        \n",
    "        print(\"=== 월별 통계 ===\")\n",
    "        for _, row in monthly_stats.iterrows():\n",
    "            print(f\"{row['연월_str']}: 게시글 {row['게시글수']:,}개, \"\n",
    "                  f\"총조회수 {row['총조회수']:,.0f}, 평균조회수 {row['평균조회수']:.1f}\")\n",
    "        \n",
    "        return monthly_stats\n",
    "    \n",
    "    def analyze_weekly_trends(self):\n",
    "        \"\"\"주간별 트렌드 분석 (전체 기간)\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        # 주간별 통계 계산 (전체 기간)\n",
    "        weekly_stats = self.processed_df.groupby('주차').agg({\n",
    "            '번호': 'count',\n",
    "            '조회수': ['sum', 'mean'],\n",
    "            '댓글갯수': ['sum', 'mean']\n",
    "        }).round(2)\n",
    "        \n",
    "        # 컬럼명 정리\n",
    "        weekly_stats.columns = ['게시글수', '총조회수', '평균조회수', '총댓글수', '평균댓글수']\n",
    "        weekly_stats = weekly_stats.reset_index()\n",
    "        weekly_stats['주차_str'] = weekly_stats['주차'].astype(str)\n",
    "        \n",
    "        print(f\"=== 주간별 통계 (전체 {len(weekly_stats)}주) ===\")\n",
    "        print(f\"첫 주차: {weekly_stats['주차_str'].iloc[0]}\")\n",
    "        print(f\"마지막 주차: {weekly_stats['주차_str'].iloc[-1]}\")\n",
    "        print(f\"주간 평균 게시글수: {weekly_stats['게시글수'].mean():.1f}개\")\n",
    "        print(f\"주간 최대 게시글수: {weekly_stats['게시글수'].max()}개\")\n",
    "        print(f\"주간 평균 총조회수: {weekly_stats['총조회수'].mean():.0f}\")\n",
    "        \n",
    "        return weekly_stats\n",
    "    \n",
    "    def plot_yearly_trends(self, yearly_stats):\n",
    "        \"\"\"연도별 트렌드 차트 그리기\"\"\"\n",
    "        if yearly_stats is None:\n",
    "            print(\"연도별 통계를 먼저 계산해주세요.\")\n",
    "            return\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        \n",
    "        # 연도별 게시글 수\n",
    "        ax1.bar(yearly_stats['연도'], yearly_stats['게시글수'], \n",
    "                color='#58D68D', alpha=0.8, width=0.6)\n",
    "        ax1.set_title('연도별 게시글 수', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('연도', fontsize=12)\n",
    "        ax1.set_ylabel('게시글 수', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 값 표시\n",
    "        for i, v in enumerate(yearly_stats['게시글수']):\n",
    "            ax1.annotate(f'{v:,.0f}', (yearly_stats['연도'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # 연도별 총 조회수\n",
    "        ax2.plot(yearly_stats['연도'], yearly_stats['총조회수'], \n",
    "                marker='o', linewidth=3, markersize=8, color='#2E86C1')\n",
    "        ax2.set_title('연도별 총 조회수', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('연도', fontsize=12)\n",
    "        ax2.set_ylabel('총 조회수', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 값 표시\n",
    "        for i, v in enumerate(yearly_stats['총조회수']):\n",
    "            ax2.annotate(f'{v:,.0f}', (yearly_stats['연도'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # 연도별 평균 조회수\n",
    "        ax3.plot(yearly_stats['연도'], yearly_stats['평균조회수'], \n",
    "                marker='s', linewidth=3, markersize=8, color='#E74C3C')\n",
    "        ax3.set_title('연도별 평균 조회수', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('연도', fontsize=12)\n",
    "        ax3.set_ylabel('평균 조회수', fontsize=12)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 값 표시\n",
    "        for i, v in enumerate(yearly_stats['평균조회수']):\n",
    "            ax3.annotate(f'{v:.1f}', (yearly_stats['연도'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        # 연도별 댓글 수\n",
    "        ax4.bar(yearly_stats['연도'], yearly_stats['총댓글수'], \n",
    "                color='#F39C12', alpha=0.8, width=0.6)\n",
    "        ax4.set_title('연도별 총 댓글 수', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('연도', fontsize=12)\n",
    "        ax4.set_ylabel('총 댓글 수', fontsize=12)\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 값 표시\n",
    "        for i, v in enumerate(yearly_stats['총댓글수']):\n",
    "            ax4.annotate(f'{v:,.0f}', (yearly_stats['연도'].iloc[i], v), \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_monthly_views(self, monthly_stats):\n",
    "        \"\"\"월별 조회수 차트 그리기 - 개선된 버전\"\"\"\n",
    "        if monthly_stats is None:\n",
    "            print(\"월별 통계를 먼저 계산해주세요.\")\n",
    "            return\n",
    "        \n",
    "        # 그래프 크기를 더 크게 설정\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12))\n",
    "        \n",
    "        # 총 조회수 추이\n",
    "        ax1.plot(monthly_stats['연월_str'], monthly_stats['총조회수'], \n",
    "                marker='o', linewidth=3, markersize=8, color='#2E86C1')\n",
    "        ax1.set_title('월별 총 조회수 추이', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax1.set_xlabel('월', fontsize=12)\n",
    "        ax1.set_ylabel('총 조회수', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # X축 라벨 회전각 조정하고 간격 늘리기\n",
    "        ax1.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax1.tick_params(axis='y', labelsize=10)\n",
    "        \n",
    "        # 평균 조회수 추이\n",
    "        ax2.plot(monthly_stats['연월_str'], monthly_stats['평균조회수'], \n",
    "                marker='s', linewidth=3, markersize=8, color='#E74C3C')\n",
    "        ax2.set_title('월별 평균 조회수 추이', fontsize=16, fontweight='bold', pad=20)\n",
    "        ax2.set_xlabel('월', fontsize=12)\n",
    "        ax2.set_ylabel('평균 조회수', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # X축 라벨 회전각 조정\n",
    "        ax2.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "        ax2.tick_params(axis='y', labelsize=10)\n",
    "        \n",
    "        # 여백 조정\n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "        \n",
    "        # 게시글 수 차트 - 더 큰 크기로\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        bars = plt.bar(monthly_stats['연월_str'], monthly_stats['게시글수'], \n",
    "                      color='#58D68D', alpha=0.8, width=0.6)\n",
    "        plt.title('월별 게시글 수', fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.xlabel('월', fontsize=12)\n",
    "        plt.ylabel('게시글 수', fontsize=12)\n",
    "        plt.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # 값 표시 - 막대 위에 표시\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.annotate(f'{height:,.0f}', \n",
    "                        xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                        xytext=(0, 8), \n",
    "                        textcoords=\"offset points\", \n",
    "                        ha='center', \n",
    "                        va='bottom',\n",
    "                        fontsize=11)\n",
    "        \n",
    "        # X축 라벨 조정\n",
    "        plt.xticks(rotation=45, fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.tight_layout(pad=2.0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_weekly_trends(self, weekly_stats):\n",
    "        \"\"\"주간별 트렌드 차트 그리기 (전체 기간)\"\"\"\n",
    "        if weekly_stats is None:\n",
    "            print(\"주간별 통계를 먼저 계산해주세요.\")\n",
    "            return\n",
    "        \n",
    "        # 전체 주차가 많을 경우를 대비해 x축 라벨 간격 조정\n",
    "        total_weeks = len(weekly_stats)\n",
    "        label_step = max(1, total_weeks // 20)  # 최대 20개 라벨만 표시\n",
    "        \n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20, 15))\n",
    "        \n",
    "        # 주간별 게시글 수\n",
    "        ax1.plot(range(len(weekly_stats)), weekly_stats['게시글수'], \n",
    "                linewidth=1, color='#58D68D', alpha=0.8)\n",
    "        ax1.fill_between(range(len(weekly_stats)), weekly_stats['게시글수'], \n",
    "                        alpha=0.3, color='#58D68D')\n",
    "        ax1.set_title(f'주간별 게시글 수 (전체 {total_weeks}주)', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('주차', fontsize=12)\n",
    "        ax1.set_ylabel('게시글 수', fontsize=12)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # x축 라벨 간격 조정\n",
    "        xtick_positions = range(0, len(weekly_stats), label_step)\n",
    "        xtick_labels = [weekly_stats['주차_str'].iloc[i] for i in xtick_positions]\n",
    "        ax1.set_xticks(xtick_positions)\n",
    "        ax1.set_xticklabels(xtick_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        # 주간별 총 조회수\n",
    "        ax2.plot(range(len(weekly_stats)), weekly_stats['총조회수'], \n",
    "                linewidth=1, color='#2E86C1', alpha=0.8)\n",
    "        ax2.fill_between(range(len(weekly_stats)), weekly_stats['총조회수'], \n",
    "                        alpha=0.3, color='#2E86C1')\n",
    "        ax2.set_title(f'주간별 총 조회수 (전체 {total_weeks}주)', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('주차', fontsize=12)\n",
    "        ax2.set_ylabel('총 조회수', fontsize=12)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xticks(xtick_positions)\n",
    "        ax2.set_xticklabels(xtick_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        # 주간별 평균 조회수\n",
    "        ax3.plot(range(len(weekly_stats)), weekly_stats['평균조회수'], \n",
    "                linewidth=1, color='#E74C3C', alpha=0.8)\n",
    "        ax3.fill_between(range(len(weekly_stats)), weekly_stats['평균조회수'], \n",
    "                        alpha=0.3, color='#E74C3C')\n",
    "        ax3.set_title(f'주간별 평균 조회수 (전체 {total_weeks}주)', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('주차', fontsize=12)\n",
    "        ax3.set_ylabel('평균 조회수', fontsize=12)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_xticks(xtick_positions)\n",
    "        ax3.set_xticklabels(xtick_labels, rotation=45, fontsize=8)\n",
    "        \n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "        \n",
    "        # 주간별 통계 요약 출력\n",
    "        print(f\"\\n=== 주간별 데이터 요약 ===\")\n",
    "        print(f\"총 주차 수: {total_weeks}주\")\n",
    "        print(f\"주간 게시글 수 - 평균: {weekly_stats['게시글수'].mean():.1f}, 최소: {weekly_stats['게시글수'].min()}, 최대: {weekly_stats['게시글수'].max()}\")\n",
    "        print(f\"주간 총조회수 - 평균: {weekly_stats['총조회수'].mean():.0f}, 최소: {weekly_stats['총조회수'].min():.0f}, 최대: {weekly_stats['총조회수'].max():.0f}\")\n",
    "        print(f\"주간 평균조회수 - 평균: {weekly_stats['평균조회수'].mean():.1f}, 최소: {weekly_stats['평균조회수'].min():.1f}, 최대: {weekly_stats['평균조회수'].max():.1f}\")\n",
    "    \n",
    "    def extract_korean_morphemes(self, text, extract_nouns=True, extract_verbs=False, extract_adjectives=False):\n",
    "        \"\"\"\n",
    "        한국어 형태소 분석을 통한 단어 추출\n",
    "        \n",
    "        Args:\n",
    "            text (str): 분석할 텍스트\n",
    "            extract_nouns (bool): 명사 추출 여부\n",
    "            extract_verbs (bool): 동사 추출 여부  \n",
    "            extract_adjectives (bool): 형용사 추출 여부\n",
    "        \"\"\"\n",
    "        if not self.use_morphology:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # 형태소 분석\n",
    "            morphemes = self.morphology_analyzer.pos(text, stem=True)\n",
    "            \n",
    "            extracted_words = []\n",
    "            for word, pos in morphemes:\n",
    "                # 2글자 이상만 추출\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                    \n",
    "                # 품사별 추출\n",
    "                if extract_nouns and pos.startswith('N'):  # 명사\n",
    "                    extracted_words.append(word)\n",
    "                elif extract_verbs and pos.startswith('V'):  # 동사\n",
    "                    extracted_words.append(word)\n",
    "                elif extract_adjectives and pos.startswith('A'):  # 형용사\n",
    "                    extracted_words.append(word)\n",
    "            \n",
    "            return extracted_words\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"형태소 분석 오류: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _get_filtered_words_advanced(self, text, use_morphology=True):\n",
    "        \"\"\"\n",
    "        개선된 단어 필터링 (형태소 분석 + 기존 방식)\n",
    "        \"\"\"\n",
    "        # 한국어 형태소 분석 사용\n",
    "        if use_morphology and self.use_morphology:\n",
    "            korean_words = self.extract_korean_morphemes(\n",
    "                text, \n",
    "                extract_nouns=True, \n",
    "                extract_verbs=False,  # 동사는 제외 (의미가 모호할 수 있음)\n",
    "                extract_adjectives=False  # 형용사도 제외\n",
    "            )\n",
    "        else:\n",
    "            # 기존 방식: 정규식으로 한글 단어 추출\n",
    "            korean_words = re.findall(r'[가-힣]{2,}', text)\n",
    "        \n",
    "        # 영어/숫자 단어 추출\n",
    "        english_words = re.findall(r'[a-zA-Z0-9]{2,}', text.lower())\n",
    "        \n",
    "        # 모든 단어 합치기\n",
    "        all_words = korean_words + english_words\n",
    "        \n",
    "        # 확장된 불용어 목록\n",
    "        stop_words = {\n",
    "            # 기존 불용어들...\n",
    "            '이거', '이건', '저거', '저건', '그거', '그건', '여기', '저기', '거기',\n",
    "            '이게', '저게', '그게', '이야', '저야', '그야', '이런', '저런', '그런',\n",
    "            '뭐야', '뭔가', '진짜', '정말', '완전', '아니', '그냥', '좀', '더', \n",
    "            '너무', '되게', '엄청', '완전히', '정말로', '진짜로', '되면', '하지만',\n",
    "            '근데', '그런데', '그리고', '또한', '그래서', '따라서', '그러나', \n",
    "            '그렇지만', '그러므로', '말고', '해서', '되고', '하고', '있고', '없고',\n",
    "            '이제', '지금', '오늘', '어제', '내일', '요즘', '최근', '언제', '바로',\n",
    "            '내용', '없음', '경우', '때문', '되는', '하는', '있는', '없는',\n",
    "            '이렇게', '저렇게', '그렇게', '어떻게', '왜냐', '때문에', '이라고',\n",
    "            '내가', '나는', '너는', '너가', '걔는', '걔가', '쟤는', '쟤가',\n",
    "            '우리는', '우리가', '저는', '저가', '그가', '그는', '그녀는', '그녀가',\n",
    "            \n",
    "            # 커뮤니티 특화 불용어\n",
    "            '게시글', '댓글', '조회수', '추천', '비추천', '신고', '수정', '삭제',\n",
    "            '작성자', '닉네임', '아이디', '회원', '등급', '포인트', '게시판',\n",
    "            'dc', 'official', 'app', '다시', '계속', '여기서', '많이', '제발', 'name',\n",
    "            \n",
    "            # 형태소 분석 결과 자주 나오는 불용어\n",
    "            '것', '수', '때', '곳', '점', '번', '개', '명', '년', '월', '일',\n",
    "            '시간', '분', '초', '정도', '만큼', '이상', '이하', '사이', '중',\n",
    "            '안', '밖', '위', '아래', '앞', '뒤', '옆', '다음', '이전', '마지막',\n",
    "            '처음', '끝', '시작', '종료', '완료', '시도', '노력', '생각', '의견',\n",
    "            '문제', '해결', '상황', '상태', '결과', '과정', '방법', '방식',\n",
    "            \n",
    "            # 감정 표현 (너무 일반적인 것들)\n",
    "            '좋다', '나쁘다', '싫다', '좋아', '싫어', '재미', '재밌', 'boring',\n",
    "            '웃음', '슬픔', '기쁨', '화남', '놀람', '걱정', '불안', '안심'\n",
    "        }\n",
    "        \n",
    "        # 불용어 제거 및 추가 필터링\n",
    "        filtered_words = []\n",
    "        for word in all_words:\n",
    "            # 불용어 제거\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            \n",
    "            # 숫자만 있는 단어 제거\n",
    "            if word.isdigit():\n",
    "                continue\n",
    "            \n",
    "            # 반복 문자 제거 (ㅋㅋㅋ, ㅎㅎㅎ 등)\n",
    "            if len(set(word)) == 1 and len(word) > 2:\n",
    "                continue\n",
    "            \n",
    "            # 의성어/의태어 패턴 제거\n",
    "            if re.match(r'^(.{1,2})\\1+$', word):\n",
    "                continue\n",
    "            \n",
    "            # 특수 패턴 제거 (URL 조각, 이메일 조각 등)\n",
    "            if re.match(r'^(www|http|com|net|org)$', word):\n",
    "                continue\n",
    "                \n",
    "            filtered_words.append(word)\n",
    "        \n",
    "        return filtered_words\n",
    "    \n",
    "    def analyze_text_frequency_advanced(self, top_n=30, use_filtered_data=True, use_morphology=True):\n",
    "        \"\"\"\n",
    "        개선된 텍스트 빈도 분석\n",
    "        \n",
    "        Args:\n",
    "            top_n (int): 상위 N개 단어\n",
    "            use_filtered_data (bool): 도배 필터링된 데이터 사용 여부\n",
    "            use_morphology (bool): 형태소 분석 사용 여부\n",
    "        \"\"\"\n",
    "        # 사용할 데이터 선택\n",
    "        if use_filtered_data and self.filtered_df is not None:\n",
    "            data_to_use = self.filtered_df\n",
    "            data_desc = \"도배 필터링 후\"\n",
    "        else:\n",
    "            data_to_use = self.processed_df\n",
    "            data_desc = \"전체\"\n",
    "        \n",
    "        if data_to_use is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        # 모든 텍스트 합치기\n",
    "        all_text = (data_to_use['제목'] + ' ' + data_to_use['내용']).str.cat(sep=' ')\n",
    "        \n",
    "        # 개선된 단어 추출\n",
    "        final_words = self._get_filtered_words_advanced(all_text, use_morphology)\n",
    "        \n",
    "        # 빈도 계산\n",
    "        word_freq = Counter(final_words)\n",
    "        top_words = word_freq.most_common(top_n)\n",
    "        \n",
    "        morphology_desc = \"형태소 분석\" if (use_morphology and self.use_morphology) else \"정규식\"\n",
    "        print(f\"=== {data_desc} 데이터 단어 빈도 TOP {top_n} ({morphology_desc}) ===\")\n",
    "        for i, (word, count) in enumerate(top_words, 1):\n",
    "            print(f\"{i:2d}. {word}: {count:,}회\")\n",
    "        \n",
    "        return top_words\n",
    "    \n",
    "    def analyze_yearly_word_frequency(self, top_n=20):\n",
    "        \"\"\"연도별 단어 빈도 분석\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        yearly_word_freq = {}\n",
    "        \n",
    "        for year in sorted(self.processed_df['연도'].unique()):\n",
    "            year_data = self.processed_df[self.processed_df['연도'] == year]\n",
    "            \n",
    "            # 해당 연도의 모든 텍스트 합치기\n",
    "            year_text = (year_data['제목'] + ' ' + year_data['내용']).str.cat(sep=' ')\n",
    "            \n",
    "            # 필터링된 단어 가져오기\n",
    "            year_words = self._get_filtered_words_advanced(year_text, use_morphology=self.use_morphology)\n",
    "            \n",
    "            # 빈도 계산\n",
    "            word_freq = Counter(year_words)\n",
    "            top_words = word_freq.most_common(top_n)\n",
    "            yearly_word_freq[year] = top_words\n",
    "            \n",
    "            print(f\"=== {year}년 가장 많이 사용된 단어 TOP {top_n} ===\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"{i:2d}. {word}: {count:,}회\")\n",
    "            print()\n",
    "        \n",
    "        return yearly_word_freq\n",
    "    \n",
    "    def analyze_monthly_word_frequency(self, top_n=20):\n",
    "        \"\"\"월별 단어 빈도 분석\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        monthly_word_freq = {}\n",
    "        \n",
    "        for month in sorted(self.processed_df['연월'].unique()):\n",
    "            month_data = self.processed_df[self.processed_df['연월'] == month]\n",
    "            \n",
    "            # 해당 월의 모든 텍스트 합치기\n",
    "            month_text = (month_data['제목'] + ' ' + month_data['내용']).str.cat(sep=' ')\n",
    "            \n",
    "            # 필터링된 단어 가져오기\n",
    "            month_words = self._get_filtered_words_advanced(month_text, use_morphology=self.use_morphology)\n",
    "            \n",
    "            # 빈도 계산\n",
    "            word_freq = Counter(month_words)\n",
    "            top_words = word_freq.most_common(top_n)\n",
    "            monthly_word_freq[month] = top_words\n",
    "            \n",
    "            print(f\"=== {month} 가장 많이 사용된 단어 TOP {top_n} ===\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"{i:2d}. {word}: {count:,}회\")\n",
    "            print()\n",
    "        \n",
    "        return monthly_word_freq\n",
    "    \n",
    "    def analyze_weekly_word_frequency(self, top_n=20):\n",
    "        \"\"\"주간별 단어 빈도 분석 (최근 20주만)\"\"\"\n",
    "        if self.processed_df is None:\n",
    "            print(\"데이터를 먼저 전처리해주세요.\")\n",
    "            return None\n",
    "        \n",
    "        weekly_word_freq = {}\n",
    "        \n",
    "        # 최근 20주만 분석 (전체 주차가 너무 많을 수 있음)\n",
    "        recent_weeks = sorted(self.processed_df['주차'].unique())[-20:]\n",
    "        \n",
    "        for week in recent_weeks:\n",
    "            week_data = self.processed_df[self.processed_df['주차'] == week]\n",
    "            \n",
    "            # 해당 주의 모든 텍스트 합치기\n",
    "            week_text = (week_data['제목'] + ' ' + week_data['내용']).str.cat(sep=' ')\n",
    "            \n",
    "            # 필터링된 단어 가져오기\n",
    "            week_words = self._get_filtered_words_advanced(week_text, use_morphology=self.use_morphology)\n",
    "            \n",
    "            # 빈도 계산\n",
    "            word_freq = Counter(week_words)\n",
    "            top_words = word_freq.most_common(top_n)\n",
    "            weekly_word_freq[week] = top_words\n",
    "            \n",
    "            print(f\"=== {week} 가장 많이 사용된 단어 TOP {top_n} ===\")\n",
    "            for i, (word, count) in enumerate(top_words, 1):\n",
    "                print(f\"{i:2d}. {word}: {count:,}회\")\n",
    "            print()\n",
    "        \n",
    "        return weekly_word_freq\n",
    "    \n",
    "    def plot_yearly_word_frequency(self, yearly_word_freq, top_n=10):\n",
    "        \"\"\"연도별 단어 빈도 차트 그리기\"\"\"\n",
    "        if not yearly_word_freq:\n",
    "            print(\"연도별 단어 빈도 데이터가 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        years = sorted(yearly_word_freq.keys())\n",
    "        num_years = len(years)\n",
    "        \n",
    "        # 서브플롯 구성 (2열로 배치)\n",
    "        cols = 2\n",
    "        rows = (num_years + 1) // 2\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(20, 6*rows))\n",
    "        if rows == 1:\n",
    "            axes = [axes] if num_years == 1 else axes\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for i, year in enumerate(years):\n",
    "            words, counts = zip(*yearly_word_freq[year][:top_n])\n",
    "            \n",
    "            ax = axes[i]\n",
    "            y_pos = np.arange(len(words))\n",
    "            \n",
    "            bars = ax.barh(y_pos, counts, color='#3498DB', alpha=0.8, height=0.7)\n",
    "            ax.set_yticks(y_pos)\n",
    "            ax.set_yticklabels(words, fontsize=10)\n",
    "            ax.set_xlabel('빈도수', fontsize=11)\n",
    "            ax.set_title(f'{year}년 단어 빈도 TOP {top_n}', fontsize=14, fontweight='bold')\n",
    "            ax.invert_yaxis()\n",
    "            ax.grid(True, alpha=0.3, axis='x')\n",
    "            \n",
    "            # 값 표시\n",
    "            for j, bar in enumerate(bars):\n",
    "                width = bar.get_width()\n",
    "                ax.annotate(f'{width:,}', \n",
    "                           xy=(width, bar.get_y() + bar.get_height()/2),\n",
    "                           xytext=(5, 0), \n",
    "                           textcoords=\"offset points\", \n",
    "                           ha='left', \n",
    "                           va='center', \n",
    "                           fontsize=9)\n",
    "        \n",
    "        # 사용하지 않는 서브플롯 숨기기\n",
    "        for i in range(num_years, len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout(pad=3.0)\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_monthly_word_frequency(self, monthly_word_freq, top_n=10, months_per_page=6):\n",
    "        \"\"\"월별 단어 빈도 차트 그리기 (페이지 나누기)\"\"\"\n",
    "        if not monthly_word_freq:\n",
    "            print(\"월별 단어 빈도 데이터가 없습니다.\")\n",
    "            return\n",
    "        \n",
    "        months = sorted(monthly_word_freq.keys())\n",
    "        total_months = len(months)\n",
    "        \n",
    "        # 페이지별로 나누어 그리기\n",
    "        for page_start in range(0, total_months, months_per_page):\n",
    "            page_end = min(page_start + months_per_page, total_months)\n",
    "            page_months = months[page_start:page_end]\n",
    "            \n",
    "            # 서브플롯 구성\n",
    "            cols = 3\n",
    "            rows = (len(page_months) + 2) // 3\n",
    "            \n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(18, 6*rows))\n",
    "            if rows == 1:\n",
    "                axes = [axes] if len(page_months) == 1 else axes\n",
    "            else:\n",
    "                axes = axes.flatten()\n",
    "            \n",
    "            for i, month in enumerate(page_months):\n",
    "                words, counts = zip(*monthly_word_freq[month][:top_n])\n",
    "                \n",
    "                ax = axes[i]\n",
    "                y_pos = np.arange(len(words))\n",
    "                \n",
    "                bars = ax.barh(y_pos, counts, color='#E74C3C', alpha=0.8, height=0.7)\n",
    "                ax.set_yticks(y_pos)\n",
    "                ax.set_yticklabels(words, fontsize=9)\n",
    "                ax.set_xlabel('빈도수', fontsize=10)\n",
    "                ax.set_title(f'{month} 단어 빈도 TOP {top_n}', fontsize=12, fontweight='bold')\n",
    "                ax.invert_yaxis()\n",
    "                ax.grid(True, alpha=0.3, axis='x')\n",
    "                \n",
    "                # 값 표시\n",
    "                for j, bar in enumerate(bars):\n",
    "                    width = bar.get_width()\n",
    "                    ax.annotate(f'{width:,}', \n",
    "                               xy=(width, bar.get_y() + bar.get_height()/2),\n",
    "                               xytext=(3, 0), \n",
    "                               textcoords=\"offset points\", \n",
    "                               ha='left', \n",
    "                               va='center', \n",
    "                               fontsize=8)\n",
    "            \n",
    "            # 사용하지 않는 서브플롯 숨기기\n",
    "            for i in range(len(page_months), len(axes)):\n",
    "                axes[i].set_visible(False)\n",
    "            \n",
    "            plt.suptitle(f'월별 단어 빈도 분석 ({page_start//months_per_page + 1}페이지)', \n",
    "                        fontsize=16, fontweight='bold')\n",
    "            plt.tight_layout(pad=2.0)\n",
    "            plt.show()\n",
    "    \n",
    "    def generate_advanced_summary_report(self):\n",
    "        \"\"\"개선된 종합 분석 리포트\"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"           개선된 커뮤니티 데이터 분석 리포트\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if self.processed_df is not None:\n",
    "            print(\"📊 원본 데이터 통계\")\n",
    "            total_posts = len(self.processed_df)\n",
    "            print(f\"   총 게시글 수: {total_posts:,}개\")\n",
    "            print(f\"   총 조회수: {self.processed_df['조회수'].sum():,}회\")\n",
    "            print(f\"   평균 조회수: {self.processed_df['조회수'].mean():.1f}회\")\n",
    "        \n",
    "        if self.filtered_df is not None:\n",
    "            print(\"\\n🧹 필터링 후 데이터 통계\")\n",
    "            filtered_posts = len(self.filtered_df)\n",
    "            removed_posts = total_posts - filtered_posts\n",
    "            print(f\"   필터링 후 게시글 수: {filtered_posts:,}개\")\n",
    "            print(f\"   제거된 게시글 수: {removed_posts:,}개 ({removed_posts/total_posts*100:.1f}%)\")\n",
    "            print(f\"   필터링 후 총 조회수: {self.filtered_df['조회수'].sum():,}회\")\n",
    "            print(f\"   필터링 후 평균 조회수: {self.filtered_df['조회수'].mean():.1f}회\")\n",
    "        \n",
    "        if self.use_morphology:\n",
    "            print(f\"\\n🔤 형태소 분석: ✅ 사용 가능 (KoNLPy)\")\n",
    "        else:\n",
    "            print(f\"\\n🔤 형태소 분석: ❌ 사용 불가 (정규식 사용)\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# ===================================\n",
    "# 3단계: 분석 실행 함수들\n",
    "# ===================================\n",
    "\n",
    "def run_advanced_analysis():\n",
    "    \"\"\"개선된 분석 실행\"\"\"\n",
    "    # 분석기 초기화 (파일 경로 수정 필요)\n",
    "    analyzer = AdvancedCommunityDataAnalyzer('community/ChartAnalysis.csv')  # 실제 파일 경로로 수정하세요\n",
    "    \n",
    "    # 1. 데이터 로드 및 전처리\n",
    "    if not analyzer.load_data():\n",
    "        return\n",
    "    if not analyzer.preprocess_data():\n",
    "        return\n",
    "    \n",
    "    # 2. 완전 동일한 중복 게시글만 필터링\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🚫 완전히 똑같은 중복 게시글만 필터링\")\n",
    "    print(\"=\"*50)\n",
    "    spam_indices = analyzer.detect_spam_posts_exact_only(use_hashing=True)\n",
    "    \n",
    "    # 3. 시간별 트렌드 분석 (원래 기능들)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📅 시간별 트렌드 분석\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 3-1. 연도별 트렌드 분석 및 차트\n",
    "    print(\"🗓️ 연도별 분석...\")\n",
    "    yearly_stats = analyzer.analyze_yearly_trends()\n",
    "    if yearly_stats is not None:\n",
    "        analyzer.plot_yearly_trends(yearly_stats)\n",
    "    \n",
    "    # 3-2. 월별 트렌드 분석 및 차트  \n",
    "    print(\"\\n📅 월별 분석...\")\n",
    "    monthly_stats = analyzer.analyze_monthly_trends()\n",
    "    if monthly_stats is not None:\n",
    "        analyzer.plot_monthly_views(monthly_stats)\n",
    "    \n",
    "    # 3-3. 주간별 트렌드 분석 및 차트\n",
    "    print(\"\\n📈 주간별 분석...\")\n",
    "    weekly_stats = analyzer.analyze_weekly_trends()\n",
    "    if weekly_stats is not None:\n",
    "        analyzer.plot_weekly_trends(weekly_stats)\n",
    "    \n",
    "    # 4. 시간별 단어 빈도 분석 (원래 기능들)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🔤 시간별 단어 빈도 분석\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 4-1. 연도별 단어 빈도 분석\n",
    "    print(\"📅 연도별 단어 빈도 분석...\")\n",
    "    yearly_word_freq = analyzer.analyze_yearly_word_frequency(top_n=20)\n",
    "    if yearly_word_freq:\n",
    "        analyzer.plot_yearly_word_frequency(yearly_word_freq, top_n=15)\n",
    "    \n",
    "    # 4-2. 월별 단어 빈도 분석\n",
    "    print(\"\\n📅 월별 단어 빈도 분석...\")\n",
    "    monthly_word_freq = analyzer.analyze_monthly_word_frequency(top_n=20)\n",
    "    if monthly_word_freq:\n",
    "        analyzer.plot_monthly_word_frequency(monthly_word_freq, top_n=10, months_per_page=6)\n",
    "    \n",
    "    # 4-3. 주간별 단어 빈도 분석 (최근 20주)\n",
    "    print(\"\\n📈 주간별 단어 빈도 분석 (최근 20주)...\")\n",
    "    weekly_word_freq = analyzer.analyze_weekly_word_frequency(top_n=15)\n",
    "    \n",
    "    # 5. 전체 단어 빈도 분석\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"📝 전체 데이터 단어 빈도 분석\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 필터링된 데이터 분석 (형태소 분석)\n",
    "    if analyzer.use_morphology:\n",
    "        print(\"\\n[형태소 분석 기반]\")\n",
    "        filtered_words_morph = analyzer.analyze_text_frequency_advanced(\n",
    "            top_n=30, use_filtered_data=True, use_morphology=True\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\n[정규식 기반]\")\n",
    "        filtered_words_regex = analyzer.analyze_text_frequency_advanced(\n",
    "            top_n=30, use_filtered_data=True, use_morphology=False\n",
    "        )\n",
    "    \n",
    "    # 6. 종합 리포트\n",
    "    analyzer.generate_advanced_summary_report()\n",
    "\n",
    "\n",
    "def quick_exact_duplicate_filter():\n",
    "    \"\"\"완전히 똑같은 중복 게시글만 필터링\"\"\"\n",
    "    analyzer = AdvancedCommunityDataAnalyzer('community/ChartAnalysis.csv')  # 실제 파일 경로로 수정하세요\n",
    "    \n",
    "    if not analyzer.load_data() or not analyzer.preprocess_data():\n",
    "        return\n",
    "    \n",
    "    # 완전 동일 중복만 필터링\n",
    "    spam_indices = analyzer.detect_spam_posts_exact_only(use_hashing=True)\n",
    "    \n",
    "    # 필터링된 데이터를 CSV로 저장\n",
    "    if analyzer.filtered_df is not None:\n",
    "        output_filename = 'filtered_community_data.csv'\n",
    "        analyzer.filtered_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "        print(f\"\\n✅ 필터링된 데이터가 '{output_filename}'로 저장되었습니다.\")\n",
    "\n",
    "\n",
    "def morphology_analysis_only():\n",
    "    \"\"\"형태소 분석만 수행\"\"\"\n",
    "    analyzer = AdvancedCommunityDataAnalyzer('community/ChartAnalysis.csv')  # 실제 파일 경로로 수정하세요\n",
    "    \n",
    "    if not analyzer.load_data() or not analyzer.preprocess_data():\n",
    "        return\n",
    "    \n",
    "    if not analyzer.use_morphology:\n",
    "        print(\"❌ 형태소 분석 라이브러리가 없습니다.\")\n",
    "        print(\"설치 방법: !pip install konlpy\")\n",
    "        return\n",
    "    \n",
    "    print(\"🔤 형태소 분석 기반 단어 빈도 분석\")\n",
    "    \n",
    "    # 명사만 추출\n",
    "    morpheme_words = analyzer.analyze_text_frequency_advanced(\n",
    "        top_n=30, use_filtered_data=False, use_morphology=True\n",
    "    )\n",
    "    \n",
    "    # 시각화\n",
    "    if morpheme_words:\n",
    "        plt.figure(figsize=(14, 12))\n",
    "        words, counts = zip(*morpheme_words[:25])\n",
    "        y_pos = np.arange(len(words))\n",
    "        \n",
    "        bars = plt.barh(y_pos, counts, color='#9B59B6', alpha=0.8)\n",
    "        plt.yticks(y_pos, words, fontsize=11)\n",
    "        plt.xlabel('빈도수', fontsize=12)\n",
    "        plt.title('형태소 분석 기반 단어 빈도 (명사)', fontsize=16, fontweight='bold')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            plt.annotate(f'{width:,}', xy=(width, bar.get_y() + bar.get_height()/2),\n",
    "                       xytext=(5, 0), textcoords=\"offset points\", \n",
    "                       ha='left', va='center', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ===================================\n",
    "# 4단계: 실행\n",
    "# ===================================\n",
    "\n",
    "print(\"🚀 개선된 커뮤니티 데이터 분석기\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. run_advanced_analysis() - 전체 분석\")\n",
    "print(\"2. quick_exact_duplicate_filter() - 완전 동일 중복만 필터링\")  \n",
    "print(\"3. morphology_analysis_only() - 형태소 분석만\")\n",
    "print(\"=\"*50)\n",
    "print(\"💡 완전히 똑같은 글만 중복으로 처리합니다 (가장 보수적)\")\n",
    "\n",
    "# 파일 경로를 실제 경로로 수정한 후 아래 주석을 해제하여 실행하세요\n",
    "run_advanced_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
